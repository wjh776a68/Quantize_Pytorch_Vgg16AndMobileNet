{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train : 1000\n",
      "dataset_test : 1000\n",
      "\n",
      " Before quantization: \n",
      " VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Size of baseline model\n",
      "Size (MB): 553.438815\n",
      "..........Evaluation accuracy on 300 images, 79.33\n",
      "\n",
      " myModel.qconfig: \n",
      " QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, quant_min=0, quant_max=127){}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){})\n",
      "\n",
      "Post Training Quantization Prepare: Inserting Observers by Calibrate\n",
      "..................................Calibrate done\n",
      "Post Training Quantization: Convert done\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==========================\n",
    "    Style: static quantize\n",
    "    Model: VGG-16\n",
    "    Create by: Han_yz @ 2020/1/29\n",
    "    Email: 20125169@bjtu.edu.cn\n",
    "    Github: https://github.com/Forggtensky\n",
    "==========================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    1、Model architecture\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self,features,num_classes=1000,init_weights=False):\n",
    "        super(VGG,self).__init__()\n",
    "        self.features = features  # 提取特征部分的网络，也为Sequential格式\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(  # 分类部分的网络\n",
    "            nn.Linear(512*7*7,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,num_classes)\n",
    "        )\n",
    "        # add the quantize part\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias,0)\n",
    "            elif isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "cfgs = {\n",
    "    'vgg11':[64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg13':[64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg16':[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M'],\n",
    "    'vgg19':[64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M'],\n",
    "}\n",
    "\n",
    "def make_features(cfg:list):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2,stride=2)]  #vgg采用的池化层均为2,2参数\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels,v,kernel_size=3,padding=1)  #vgg卷积层采用的卷积核均为3,1参数\n",
    "            layers += [conv2d,nn.ReLU(True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)  #非关键字的形式输入网络的参数\n",
    "\n",
    "def vgg(model_name='vgg16',**kwargs):\n",
    "    try:\n",
    "        cfg = cfgs[model_name]\n",
    "    except:\n",
    "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
    "        exit(-1)\n",
    "    model = VGG(make_features(cfg),**kwargs)  # **kwargs为可变长度字典，保存多个输入参数\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    2、Helper functions\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def load_model(model_file):\n",
    "    model_name = \"vgg16\"\n",
    "    model = vgg(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    3. Define dataset and data loaders\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data_loaders(data_path):\n",
    "    traindir = os.path.join(data_path, 'train')\n",
    "    valdir = os.path.join(data_path, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_train : %d\" % (len(dataset)))\n",
    "\n",
    "    dataset_test = torchvision.datasets.ImageFolder(\n",
    "        valdir,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_test : %d\" % (len(dataset_test)))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "data_path = 'data/imagenet_1k'\n",
    "saved_model_dir = 'model/'\n",
    "float_model_file = 'vgg16_pretrained_float.pth'\n",
    "scripted_float_model_file = 'vgg16_quantization_scripted.pth'\n",
    "scripted_default_quantized_model_file = 'vgg16_quantization_scripted_default_quantized.pth'\n",
    "scripted_optimal_quantized_model_file = 'vgg16_quantization_scripted_optimal_quantized.pth'\n",
    "\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 30\n",
    "\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "print('\\n Before quantization: \\n',float_model)\n",
    "float_model.eval()\n",
    "\n",
    "# Note: vgg-16 has no BN layer so that not need to fuse model\n",
    "\n",
    "num_eval_batches = 10\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "# to get a “baseline” accuracy, see the accuracy of our un-quantized model\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file) # save un_quantized model\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    4. Post-training static quantization\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "num_calibration_batches = 40\n",
    "\n",
    "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "myModel.eval()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print('\\n myModel.qconfig: \\n',myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate with the training set\n",
    "print('\\nPost Training Quantization Prepare: Inserting Observers by Calibrate')\n",
    "evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "\n",
    "# print('\\n After quantization: \\n',myModel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "model_int8 conv keys: odict_keys(['features.0.weight', 'features.0.bias', 'features.0.scale', 'features.0.zero_point', 'features.2.weight', 'features.2.bias', 'features.2.scale', 'features.2.zero_point', 'features.5.weight', 'features.5.bias', 'features.5.scale', 'features.5.zero_point', 'features.7.weight', 'features.7.bias', 'features.7.scale', 'features.7.zero_point', 'features.10.weight', 'features.10.bias', 'features.10.scale', 'features.10.zero_point', 'features.12.weight', 'features.12.bias', 'features.12.scale', 'features.12.zero_point', 'features.14.weight', 'features.14.bias', 'features.14.scale', 'features.14.zero_point', 'features.17.weight', 'features.17.bias', 'features.17.scale', 'features.17.zero_point', 'features.19.weight', 'features.19.bias', 'features.19.scale', 'features.19.zero_point', 'features.21.weight', 'features.21.bias', 'features.21.scale', 'features.21.zero_point', 'features.24.weight', 'features.24.bias', 'features.24.scale', 'features.24.zero_point', 'features.26.weight', 'features.26.bias', 'features.26.scale', 'features.26.zero_point', 'features.28.weight', 'features.28.bias', 'features.28.scale', 'features.28.zero_point', 'classifier.0.scale', 'classifier.0.zero_point', 'classifier.0._packed_params.dtype', 'classifier.0._packed_params._packed_params', 'classifier.3.scale', 'classifier.3.zero_point', 'classifier.3._packed_params.dtype', 'classifier.3._packed_params._packed_params', 'classifier.6.scale', 'classifier.6.zero_point', 'classifier.6._packed_params.dtype', 'classifier.6._packed_params._packed_params', 'quant.scale', 'quant.zero_point'])\n",
      "\n",
      "\n",
      "model_int8 features.0.weight: tensor([[[[-0.5489,  0.1397,  0.5290],\n",
      "          [-0.5789,  0.3593,  0.7685],\n",
      "          [-0.6887, -0.0499,  0.4891]],\n",
      "\n",
      "         [[ 0.1797,  0.0100, -0.0798],\n",
      "          [ 0.0399, -0.0699, -0.2595],\n",
      "          [ 0.1298, -0.1697, -0.1298]],\n",
      "\n",
      "         [[ 0.3094, -0.1697, -0.4292],\n",
      "          [ 0.4791, -0.0798, -0.4891],\n",
      "          [ 0.6288,  0.0200, -0.2795]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2296,  0.1298,  0.1896],\n",
      "          [-0.4292, -0.2395,  0.2495],\n",
      "          [-0.2495,  0.1397, -0.0100]],\n",
      "\n",
      "         [[-0.1397, -0.2196,  0.1497],\n",
      "          [-0.8384, -0.3493,  0.5689],\n",
      "          [-0.2395,  0.5190,  0.5390]],\n",
      "\n",
      "         [[-0.3094, -0.3693, -0.1298],\n",
      "          [-0.4691, -0.1597,  0.3493],\n",
      "          [ 0.0499,  0.5889,  0.4990]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1797,  0.5190,  0.0100],\n",
      "          [-0.2695, -0.7186,  0.3094],\n",
      "          [-0.0798, -0.2196,  0.3394]],\n",
      "\n",
      "         [[ 0.3094,  0.6687,  0.0200],\n",
      "          [-0.4691, -1.0680,  0.3394],\n",
      "          [-0.0798, -0.3094,  0.5489]],\n",
      "\n",
      "         [[ 0.3194,  0.4192, -0.3493],\n",
      "          [ 0.0898, -0.4691,  0.0100],\n",
      "          [ 0.1098, -0.1497, -0.0200]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0798,  0.1298,  0.0299],\n",
      "          [ 0.2196,  0.2495, -0.0499],\n",
      "          [ 0.0499,  0.0299,  0.0200]],\n",
      "\n",
      "         [[-0.1797, -0.0699, -0.0100],\n",
      "          [-0.0499,  0.0100, -0.1298],\n",
      "          [-0.0599, -0.0599,  0.0399]],\n",
      "\n",
      "         [[-0.2296, -0.1198, -0.0200],\n",
      "          [-0.0998, -0.0200,  0.0000],\n",
      "          [-0.0299,  0.0000,  0.1397]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0200, -0.0299,  0.0000],\n",
      "          [-0.0699, -0.1896, -0.1397],\n",
      "          [-0.0699, -0.1797, -0.1697]],\n",
      "\n",
      "         [[ 0.0399, -0.0699, -0.0100],\n",
      "          [ 0.0100, -0.1497, -0.1198],\n",
      "          [ 0.0100, -0.0998, -0.1198]],\n",
      "\n",
      "         [[ 0.1298,  0.0898,  0.1298],\n",
      "          [ 0.1797,  0.1098,  0.1198],\n",
      "          [ 0.1497,  0.0998,  0.0998]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299, -0.1098, -0.2595],\n",
      "          [ 0.2795, -0.0399, -0.2595],\n",
      "          [ 0.3493,  0.0299, -0.0599]],\n",
      "\n",
      "         [[ 0.2495,  0.1597, -0.1697],\n",
      "          [ 0.3893,  0.0299, -0.3493],\n",
      "          [ 0.1896, -0.1996, -0.2994]],\n",
      "\n",
      "         [[ 0.4591,  0.4292,  0.2795],\n",
      "          [ 0.1597, -0.0599, -0.1896],\n",
      "          [-0.1996, -0.4591, -0.4292]]]], size=(64, 3, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.009980898350477219, zero_point=0)\n",
      "\n",
      "\n",
      "model_int8 features.0.bias: Parameter containing:\n",
      "tensor([ 0.4034,  0.3778,  0.4644, -0.3228,  0.3940, -0.3953,  0.3951, -0.5496,\n",
      "         0.2693, -0.7602, -0.3508,  0.2334, -1.3239, -0.1694,  0.3938, -0.1026,\n",
      "         0.0460, -0.6995,  0.1549,  0.5628,  0.3011,  0.3425,  0.1073,  0.4651,\n",
      "         0.1295,  0.0788, -0.0492, -0.5638,  0.1465, -0.3890, -0.0715,  0.0649,\n",
      "         0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,\n",
      "         0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,\n",
      "         0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,\n",
      "         0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "model_int8 features.0.scale: tensor(0.3098)\n",
      "\n",
      "\n",
      "model_int8 features.0.zero_point: tensor(61)\n",
      "\n",
      "\n",
      "model_int8 features.2.weight: tensor([[[[-0.0296, -0.0973, -0.1311],\n",
      "          [ 0.0085, -0.0846, -0.1650],\n",
      "          [ 0.0296, -0.0677, -0.1311]],\n",
      "\n",
      "         [[ 0.0465, -0.0296, -0.0508],\n",
      "          [ 0.0719,  0.0085, -0.0169],\n",
      "          [ 0.0719,  0.0381,  0.0169]],\n",
      "\n",
      "         [[ 0.0719,  0.0042, -0.0465],\n",
      "          [ 0.0846,  0.0508,  0.0000],\n",
      "          [ 0.0085,  0.0212,  0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0296,  0.0212, -0.0085],\n",
      "          [ 0.0254,  0.0042, -0.0338],\n",
      "          [ 0.0212,  0.0381, -0.0127]],\n",
      "\n",
      "         [[ 0.0212,  0.0423,  0.0592],\n",
      "          [ 0.0254,  0.0381,  0.0338],\n",
      "          [-0.0085,  0.0212,  0.0508]],\n",
      "\n",
      "         [[ 0.0212, -0.0212, -0.0973],\n",
      "          [-0.0592, -0.0719, -0.0761],\n",
      "          [-0.0381, -0.0254, -0.0042]]],\n",
      "\n",
      "\n",
      "        [[[-0.0127, -0.0761, -0.1354],\n",
      "          [-0.0381, -0.0804, -0.1438],\n",
      "          [-0.0423, -0.1058, -0.1607]],\n",
      "\n",
      "         [[-0.0085,  0.0465,  0.0169],\n",
      "          [ 0.0169,  0.0085, -0.0169],\n",
      "          [ 0.0000, -0.0338, -0.0338]],\n",
      "\n",
      "         [[ 0.0127, -0.0127, -0.0465],\n",
      "          [-0.0169,  0.0169, -0.0804],\n",
      "          [ 0.0169,  0.0042,  0.0085]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0423,  0.0381,  0.0254],\n",
      "          [ 0.0169,  0.0042, -0.0127],\n",
      "          [ 0.0254,  0.0169, -0.0254]],\n",
      "\n",
      "         [[-0.0212,  0.0042, -0.0212],\n",
      "          [ 0.0000, -0.0085,  0.0169],\n",
      "          [-0.0042, -0.0212, -0.0212]],\n",
      "\n",
      "         [[-0.0296, -0.0254,  0.0254],\n",
      "          [-0.0592, -0.0550,  0.0719],\n",
      "          [-0.0635, -0.0381,  0.0423]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0127,  0.0254,  0.0550],\n",
      "          [-0.0254, -0.0169,  0.0296],\n",
      "          [ 0.0127,  0.0085,  0.0296]],\n",
      "\n",
      "         [[-0.0804, -0.0888, -0.0550],\n",
      "          [-0.0465, -0.1100, -0.0127],\n",
      "          [ 0.0338,  0.0254,  0.1100]],\n",
      "\n",
      "         [[-0.0423, -0.0169, -0.0085],\n",
      "          [ 0.0296,  0.0465,  0.0169],\n",
      "          [ 0.0888,  0.1269,  0.0719]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0169, -0.0169],\n",
      "          [-0.0085, -0.0127,  0.0000],\n",
      "          [ 0.0085,  0.0000, -0.0338]],\n",
      "\n",
      "         [[-0.0085, -0.0042,  0.0000],\n",
      "          [ 0.0000, -0.0042,  0.0085],\n",
      "          [ 0.0296,  0.0169,  0.0338]],\n",
      "\n",
      "         [[ 0.0592, -0.0085,  0.0381],\n",
      "          [-0.0212, -0.0508,  0.0423],\n",
      "          [ 0.0212,  0.0085,  0.0338]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0465, -0.0296,  0.0085],\n",
      "          [-0.0465, -0.0508,  0.0169],\n",
      "          [-0.0423, -0.0254, -0.0254]],\n",
      "\n",
      "         [[-0.0212,  0.0169, -0.0635],\n",
      "          [ 0.0296,  0.0296, -0.0635],\n",
      "          [-0.0127,  0.0042, -0.0635]],\n",
      "\n",
      "         [[-0.0296, -0.0212, -0.0508],\n",
      "          [ 0.0127, -0.0508, -0.0550],\n",
      "          [-0.0381,  0.0000,  0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0085],\n",
      "          [ 0.0000,  0.0085,  0.0169],\n",
      "          [ 0.0127,  0.0085,  0.0338]],\n",
      "\n",
      "         [[ 0.0085,  0.0338,  0.0381],\n",
      "          [-0.0042,  0.0000,  0.0127],\n",
      "          [-0.0042,  0.0085,  0.0296]],\n",
      "\n",
      "         [[-0.0296, -0.0677, -0.0761],\n",
      "          [-0.0381, -0.0085,  0.0635],\n",
      "          [-0.0169,  0.0592,  0.1269]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0423,  0.0381,  0.0169],\n",
      "          [ 0.0423,  0.0296,  0.0169],\n",
      "          [-0.0042,  0.0212, -0.0042]],\n",
      "\n",
      "         [[ 0.0296, -0.0169,  0.0127],\n",
      "          [-0.0592, -0.0465, -0.0296],\n",
      "          [-0.0296,  0.0169, -0.0127]],\n",
      "\n",
      "         [[-0.0973, -0.0973, -0.1311],\n",
      "          [ 0.0719, -0.0085, -0.0761],\n",
      "          [ 0.1692,  0.1819,  0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0254,  0.0338,  0.0042],\n",
      "          [-0.0042,  0.0212,  0.0169],\n",
      "          [-0.0042, -0.0296,  0.0000]],\n",
      "\n",
      "         [[ 0.0085, -0.0042,  0.0042],\n",
      "          [ 0.0000,  0.0127,  0.0212],\n",
      "          [ 0.0042, -0.0127,  0.0042]],\n",
      "\n",
      "         [[ 0.0338,  0.0127,  0.0296],\n",
      "          [-0.0085,  0.0592,  0.0338],\n",
      "          [-0.0212,  0.0169, -0.0127]]],\n",
      "\n",
      "\n",
      "        [[[-0.0677,  0.0127,  0.0804],\n",
      "          [ 0.0085, -0.0338,  0.0508],\n",
      "          [-0.0381, -0.0296,  0.0635]],\n",
      "\n",
      "         [[ 0.0550, -0.1904,  0.1015],\n",
      "          [-0.1861,  0.0127,  0.2496],\n",
      "          [-0.0042,  0.0296, -0.0888]],\n",
      "\n",
      "         [[-0.1481, -0.1311,  0.1777],\n",
      "          [-0.1565,  0.1523,  0.0846],\n",
      "          [ 0.0931,  0.0846, -0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0127,  0.0338, -0.0085],\n",
      "          [ 0.0508, -0.0085, -0.0338],\n",
      "          [ 0.0127, -0.0296,  0.0254]],\n",
      "\n",
      "         [[ 0.0296,  0.0296,  0.0042],\n",
      "          [ 0.0042, -0.0169, -0.0212],\n",
      "          [-0.0085, -0.0381, -0.0212]],\n",
      "\n",
      "         [[ 0.0254,  0.0465,  0.0677],\n",
      "          [ 0.0127,  0.0338, -0.0592],\n",
      "          [ 0.0254, -0.0931, -0.0423]]]], size=(64, 64, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004230252001434565, zero_point=0)\n",
      "\n",
      "\n",
      "model_int8 features.2.bias: Parameter containing:\n",
      "tensor([ 0.0020, -0.0902,  0.6164, -0.0818,  0.2450, -0.0488,  0.1307, -0.0290,\n",
      "        -0.1429,  0.3068, -0.0399, -0.2524,  0.0999, -0.2326,  0.0353, -0.0904,\n",
      "         0.1138, -0.0307, -0.0108, -0.0215,  0.0554,  0.1382,  0.0362, -0.4511,\n",
      "         0.0056, -0.0246, -0.4296, -0.1458,  0.3813, -0.0359,  0.1184, -0.3527,\n",
      "        -0.0239, -0.0235,  0.6499, -0.0634, -0.0152, -0.2285,  0.0941, -0.5053,\n",
      "         0.1906,  0.0944,  0.3406, -0.0833,  0.1924, -0.1953, -0.0421, -0.1606,\n",
      "         0.3964,  0.2068,  0.1812, -0.1198, -0.0724, -0.1240,  0.1313,  0.1043,\n",
      "         0.5469,  0.5208,  0.0509, -0.8278,  0.4372, -0.3734, -0.3264, -0.1213],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "model_int8 features.2.scale: tensor(0.8419)\n",
      "\n",
      "\n",
      "model_int8 features.2.zero_point: tensor(70)\n",
      "\n",
      "\n",
      "model_int8 quant.scale: tensor([0.0375])\n",
      "\n",
      "\n",
      "model_int8 quant.zero_point: tensor([57])\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nmodel_int8 conv keys:',myModel.state_dict().keys())\n",
    "print('\\n\\nmodel_int8 features.0.weight:',myModel.state_dict()['features.0.weight'])\n",
    "print('\\n\\nmodel_int8 features.0.bias:',myModel.state_dict()['features.0.bias'])\n",
    "print('\\n\\nmodel_int8 features.0.scale:',myModel.state_dict()['features.0.scale'])\n",
    "print('\\n\\nmodel_int8 features.0.zero_point:',myModel.state_dict()['features.0.zero_point'])\n",
    "\n",
    "print('\\n\\nmodel_int8 features.2.weight:',myModel.state_dict()['features.2.weight'])\n",
    "print('\\n\\nmodel_int8 features.2.bias:',myModel.state_dict()['features.2.bias'])\n",
    "print('\\n\\nmodel_int8 features.2.scale:',myModel.state_dict()['features.2.scale'])\n",
    "print('\\n\\nmodel_int8 features.2.zero_point:',myModel.state_dict()['features.2.zero_point'])\n",
    "\n",
    "print('\\n\\nmodel_int8 quant.scale:',myModel.state_dict()['quant.scale'])\n",
    "print('\\n\\nmodel_int8 quant.zero_point:',myModel.state_dict()['quant.zero_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "conv_weight0 tensor([[[[ -55,   14,   53],\n",
      "          [ -58,   36,   77],\n",
      "          [ -69,   -5,   49]],\n",
      "\n",
      "         [[  18,    1,   -8],\n",
      "          [   4,   -7,  -26],\n",
      "          [  13,  -17,  -13]],\n",
      "\n",
      "         [[  31,  -17,  -43],\n",
      "          [  48,   -8,  -49],\n",
      "          [  63,    2,  -28]]],\n",
      "\n",
      "\n",
      "        [[[  23,   13,   19],\n",
      "          [ -43,  -24,   25],\n",
      "          [ -25,   14,   -1]],\n",
      "\n",
      "         [[ -14,  -22,   15],\n",
      "          [ -84,  -35,   57],\n",
      "          [ -24,   52,   54]],\n",
      "\n",
      "         [[ -31,  -37,  -13],\n",
      "          [ -47,  -16,   35],\n",
      "          [   5,   59,   50]]],\n",
      "\n",
      "\n",
      "        [[[  18,   52,    1],\n",
      "          [ -27,  -72,   31],\n",
      "          [  -8,  -22,   34]],\n",
      "\n",
      "         [[  31,   67,    2],\n",
      "          [ -47, -107,   34],\n",
      "          [  -8,  -31,   55]],\n",
      "\n",
      "         [[  32,   42,  -35],\n",
      "          [   9,  -47,    1],\n",
      "          [  11,  -15,   -2]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[   8,   13,    3],\n",
      "          [  22,   25,   -5],\n",
      "          [   5,    3,    2]],\n",
      "\n",
      "         [[ -18,   -7,   -1],\n",
      "          [  -5,    1,  -13],\n",
      "          [  -6,   -6,    4]],\n",
      "\n",
      "         [[ -23,  -12,   -2],\n",
      "          [ -10,   -2,    0],\n",
      "          [  -3,    0,   14]]],\n",
      "\n",
      "\n",
      "        [[[   2,   -3,    0],\n",
      "          [  -7,  -19,  -14],\n",
      "          [  -7,  -18,  -17]],\n",
      "\n",
      "         [[   4,   -7,   -1],\n",
      "          [   1,  -15,  -12],\n",
      "          [   1,  -10,  -12]],\n",
      "\n",
      "         [[  13,    9,   13],\n",
      "          [  18,   11,   12],\n",
      "          [  15,   10,   10]]],\n",
      "\n",
      "\n",
      "        [[[   3,  -11,  -26],\n",
      "          [  28,   -4,  -26],\n",
      "          [  35,    3,   -6]],\n",
      "\n",
      "         [[  25,   16,  -17],\n",
      "          [  39,    3,  -35],\n",
      "          [  19,  -20,  -30]],\n",
      "\n",
      "         [[  46,   43,   28],\n",
      "          [  16,   -6,  -19],\n",
      "          [ -20,  -46,  -43]]]], dtype=torch.int8)\n",
      "\n",
      "\n",
      "conv_weight2 tensor([[[[ -7, -23, -31],\n",
      "          [  2, -20, -39],\n",
      "          [  7, -16, -31]],\n",
      "\n",
      "         [[ 11,  -7, -12],\n",
      "          [ 17,   2,  -4],\n",
      "          [ 17,   9,   4]],\n",
      "\n",
      "         [[ 17,   1, -11],\n",
      "          [ 20,  12,   0],\n",
      "          [  2,   5,   1]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  7,   5,  -2],\n",
      "          [  6,   1,  -8],\n",
      "          [  5,   9,  -3]],\n",
      "\n",
      "         [[  5,  10,  14],\n",
      "          [  6,   9,   8],\n",
      "          [ -2,   5,  12]],\n",
      "\n",
      "         [[  5,  -5, -23],\n",
      "          [-14, -17, -18],\n",
      "          [ -9,  -6,  -1]]],\n",
      "\n",
      "\n",
      "        [[[ -3, -18, -32],\n",
      "          [ -9, -19, -34],\n",
      "          [-10, -25, -38]],\n",
      "\n",
      "         [[ -2,  11,   4],\n",
      "          [  4,   2,  -4],\n",
      "          [  0,  -8,  -8]],\n",
      "\n",
      "         [[  3,  -3, -11],\n",
      "          [ -4,   4, -19],\n",
      "          [  4,   1,   2]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 10,   9,   6],\n",
      "          [  4,   1,  -3],\n",
      "          [  6,   4,  -6]],\n",
      "\n",
      "         [[ -5,   1,  -5],\n",
      "          [  0,  -2,   4],\n",
      "          [ -1,  -5,  -5]],\n",
      "\n",
      "         [[ -7,  -6,   6],\n",
      "          [-14, -13,  17],\n",
      "          [-15,  -9,  10]]],\n",
      "\n",
      "\n",
      "        [[[  3,   6,  13],\n",
      "          [ -6,  -4,   7],\n",
      "          [  3,   2,   7]],\n",
      "\n",
      "         [[-19, -21, -13],\n",
      "          [-11, -26,  -3],\n",
      "          [  8,   6,  26]],\n",
      "\n",
      "         [[-10,  -4,  -2],\n",
      "          [  7,  11,   4],\n",
      "          [ 21,  30,  17]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,   4,  -4],\n",
      "          [ -2,  -3,   0],\n",
      "          [  2,   0,  -8]],\n",
      "\n",
      "         [[ -2,  -1,   0],\n",
      "          [  0,  -1,   2],\n",
      "          [  7,   4,   8]],\n",
      "\n",
      "         [[ 14,  -2,   9],\n",
      "          [ -5, -12,  10],\n",
      "          [  5,   2,   8]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-11,  -7,   2],\n",
      "          [-11, -12,   4],\n",
      "          [-10,  -6,  -6]],\n",
      "\n",
      "         [[ -5,   4, -15],\n",
      "          [  7,   7, -15],\n",
      "          [ -3,   1, -15]],\n",
      "\n",
      "         [[ -7,  -5, -12],\n",
      "          [  3, -12, -13],\n",
      "          [ -9,   0,   8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,   0,  -2],\n",
      "          [  0,   2,   4],\n",
      "          [  3,   2,   8]],\n",
      "\n",
      "         [[  2,   8,   9],\n",
      "          [ -1,   0,   3],\n",
      "          [ -1,   2,   7]],\n",
      "\n",
      "         [[ -7, -16, -18],\n",
      "          [ -9,  -2,  15],\n",
      "          [ -4,  14,  30]]],\n",
      "\n",
      "\n",
      "        [[[ 10,   9,   4],\n",
      "          [ 10,   7,   4],\n",
      "          [ -1,   5,  -1]],\n",
      "\n",
      "         [[  7,  -4,   3],\n",
      "          [-14, -11,  -7],\n",
      "          [ -7,   4,  -3]],\n",
      "\n",
      "         [[-23, -23, -31],\n",
      "          [ 17,  -2, -18],\n",
      "          [ 40,  43,   8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  6,   8,   1],\n",
      "          [ -1,   5,   4],\n",
      "          [ -1,  -7,   0]],\n",
      "\n",
      "         [[  2,  -1,   1],\n",
      "          [  0,   3,   5],\n",
      "          [  1,  -3,   1]],\n",
      "\n",
      "         [[  8,   3,   7],\n",
      "          [ -2,  14,   8],\n",
      "          [ -5,   4,  -3]]],\n",
      "\n",
      "\n",
      "        [[[-16,   3,  19],\n",
      "          [  2,  -8,  12],\n",
      "          [ -9,  -7,  15]],\n",
      "\n",
      "         [[ 13, -45,  24],\n",
      "          [-44,   3,  59],\n",
      "          [ -1,   7, -21]],\n",
      "\n",
      "         [[-35, -31,  42],\n",
      "          [-37,  36,  20],\n",
      "          [ 22,  20,  -8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  3,   8,  -2],\n",
      "          [ 12,  -2,  -8],\n",
      "          [  3,  -7,   6]],\n",
      "\n",
      "         [[  7,   7,   1],\n",
      "          [  1,  -4,  -5],\n",
      "          [ -2,  -9,  -5]],\n",
      "\n",
      "         [[  6,  11,  16],\n",
      "          [  3,   8, -14],\n",
      "          [  6, -22, -10]]]], dtype=torch.int8)\n",
      "Size of model after quantization\n",
      "Size (MB): 138.416915\n",
      "..........Evaluation accuracy on 300 images, 78.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "conv_weight0 = myModel.state_dict()['features.0.weight']\n",
    "conv_weight0.int_repr()\n",
    "print('\\n\\nconv_weight0',conv_weight0.int_repr())\n",
    "\n",
    "conv_weight2 = myModel.state_dict()['features.2.weight']\n",
    "conv_weight2.int_repr()\n",
    "print('\\n\\nconv_weight2',conv_weight2.int_repr())\n",
    "\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(myModel), saved_model_dir + scripted_default_quantized_model_file) # save default_quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    5. optimal\n",
    "    ·Quantizes weights on a per-channel basis\n",
    "    ·Uses a histogram observer that collects a histogram of activations and then picks quantization parameters\n",
    "    in an optimal manner.\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "# per_channel_quantized_model.fuse_model() # VGG dont need fuse\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') # set the quantize config\n",
    "print('\\n optimal quantize config: ')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True) # execute the quantize config\n",
    "evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches) # calibrate\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True) # convert to quantize model\n",
    "print('Post Training Optimal Quantization: Convert done')\n",
    "\n",
    "print(\"Size of model after optimal quantization\")\n",
    "print_size_of_model(per_channel_quantized_model)\n",
    "\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches) # test acc\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_optimal_quantized_model_file) # save quantized model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    6. compare performance\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nInference time compare: \")\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_default_quantized_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_optimal_quantized_model_file, data_loader_test)\n",
    "\n",
    "\"\"\" you can compare the model's size/accuracy/inference time.\n",
    "    ----------------------------------------------------------------------------------------\n",
    "                    | origin model | default quantized model | optimal quantized model\n",
    "    model size:     |    553 MB    |         138 MB          |        138 MB\n",
    "    test accuracy:  |    79.33     |         76.67           |        78.67\n",
    "    inference time: |    317 ms    |         254 ms          |        257 ms\n",
    "    ---------------------------------------------------------------------------------------\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
