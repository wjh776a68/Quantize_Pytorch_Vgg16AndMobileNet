{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "==========================\n",
    "    Style: static quantize\n",
    "    Model: VGG-16\n",
    "    Create by: Han_yz @ 2020/1/29\n",
    "    Email: 20125169@bjtu.edu.cn\n",
    "    Github: https://github.com/Forggtensky\n",
    "==========================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    1、Model architecture\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class VGG_fcn(nn.Module):\n",
    "    def __init__(self,num_classes=1000,init_weights=False):\n",
    "        super(VGG_fcn,self).__init__()\n",
    "        # self.features = features  # 提取特征部分的网络，也为Sequential格式\n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(  # 分类部分的网络\n",
    "            nn.Linear(512*7*7,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,num_classes)\n",
    "        )\n",
    "        # add the quantize part\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        # x = self.features(x)\n",
    "        # x = self.avgpool(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias,0)\n",
    "            elif isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self,features,num_classes=1000,init_weights=False):\n",
    "        super(VGG,self).__init__()\n",
    "        self.features = features  # 提取特征部分的网络，也为Sequential格式\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(  # 分类部分的网络\n",
    "            nn.Linear(512*7*7,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,num_classes)\n",
    "        )\n",
    "        # add the quantize part\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias,0)\n",
    "            elif isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "cfgs = {\n",
    "    'vgg11':[64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg13':[64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg16':[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M'],\n",
    "    'vgg19':[64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M'],\n",
    "}\n",
    "\n",
    "def make_features(cfg:list):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2,stride=2)]  #vgg采用的池化层均为2,2参数\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels,v,kernel_size=3,padding=1)  #vgg卷积层采用的卷积核均为3,1参数\n",
    "            layers += [conv2d,nn.ReLU(True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)  #非关键字的形式输入网络的参数\n",
    "\n",
    "def vgg(model_name='vgg16',**kwargs):\n",
    "    try:\n",
    "        cfg = cfgs[model_name]\n",
    "    except:\n",
    "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
    "        exit(-1)\n",
    "    model = VGG(make_features(cfg),**kwargs)  # **kwargs为可变长度字典，保存多个输入参数\n",
    "    return model\n",
    "\n",
    "def vgg_fcn(model_name='vgg16_fcn',**kwargs):\n",
    "    model = VGG_fcn(**kwargs)  # **kwargs为可变长度字典，保存多个输入参数\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    2、Helper functions\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def run_benchmark_fcn(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            x = torch.rand([images.shape(0), 7, 7, 512], dtype=torch.float32)\n",
    "            start = time.time()\n",
    "            output = model(x)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def load_model(model_file):\n",
    "    model_name = \"vgg16\"\n",
    "    model = vgg(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def load_model_fcn(model_file):\n",
    "    model_name = \"vgg16_fcn\"\n",
    "    model = vgg_fcn(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelfcn_name = \"vgg16_fcn\"\n",
    "modelfcn = vgg_fcn(model_name=modelfcn_name,num_classes=1000,init_weights=False)\n",
    "modelfcn.eval()\n",
    "torch.save(modelfcn.state_dict(), 'model/vgg16fcn_pretrained_float.pth')\n",
    "# state_dict = torch.load('model/vgg16fcn_pretrained_float.pth')\n",
    "# modelfcn.load_state_dict(state_dict)\n",
    "# modelfcn.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.ones([30, 7, 7, 512], dtype=torch.float32)\n",
    "# output = modelfcn(x)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "\n",
      "Post Training Quantization Prepare: Inserting Observers by Calibrate\n",
      "Calibrate done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Quantize_Pytorch_Vgg16AndMobileNet/.venv/lib/python3.8/site-packages/torch/quantization/observer.py:207: UserWarning: Must run observer before calling calculate_qparams.                           Returning default scale and zero point.\n",
      "  warnings.warn(\"Must run observer before calling calculate_qparams.\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Training Quantization: Convert done\n",
      "\n",
      " After quantization: \n",
      " VGG_fcn(\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_features=25088, out_features=4096, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "    (1): QuantizedReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): QuantizedLinear(in_features=4096, out_features=4096, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "    (4): QuantizedReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): QuantizedLinear(in_features=4096, out_features=1000, scale=1.0, zero_point=0, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([1.]), zero_point=tensor([0]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 123.673008\n"
     ]
    }
   ],
   "source": [
    "# \n",
    "# modelfcn = load_model_fcn('model/vgg16fcn_pretrained_float.pth')\n",
    "# print_size_of_model(modelfcn)\n",
    "\n",
    "\n",
    "num_calibration_batches = 10\n",
    "\n",
    "modelfcn = load_model_fcn('model/vgg16fcn_pretrained_float.pth').to('cpu')\n",
    "modelfcn.eval()\n",
    "\n",
    "torch.jit.save(torch.jit.script(modelfcn), \"model/vgg16fcn_quantization_scripted.pth\") # save un_quantized model\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "modelfcn.qconfig = torch.quantization.default_qconfig\n",
    "print(modelfcn.qconfig)\n",
    "torch.quantization.prepare(modelfcn, inplace=True)\n",
    "\n",
    "# Calibrate with the training set\n",
    "print('\\nPost Training Quantization Prepare: Inserting Observers by Calibrate')\n",
    "# evaluate(modelfcn, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "\n",
    "torch.quantization.convert(modelfcn, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "\n",
    "print('\\n After quantization: \\n',modelfcn)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(modelfcn)\n",
    "\n",
    "torch.jit.save(torch.jit.script(modelfcn), \"model/vgg16fcn_quantization_scripted_default_quantized.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    2、Helper functions\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def run_benchmark_fcn(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            x = torch.rand([images.shape[0], 7, 7, 512], dtype=torch.float32)\n",
    "            start = time.time()\n",
    "            output = model(x)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def load_model(model_file):\n",
    "    model_name = \"vgg16\"\n",
    "    model = vgg(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def load_model_fcn(model_file):\n",
    "    model_name = \"vgg16_fcn\"\n",
    "    model = vgg_fcn(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train : 1000\n",
      "dataset_test : 1000\n",
      "\n",
      " Before quantization: \n",
      " VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Size of baseline model\n",
      "Size (MB): 553.435632\n",
      "..........Evaluation accuracy on 300 images, 79.33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    3. Define dataset and data loaders\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data_loaders(data_path):\n",
    "    traindir = os.path.join(data_path, 'train')\n",
    "    valdir = os.path.join(data_path, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_train : %d\" % (len(dataset)))\n",
    "\n",
    "    dataset_test = torchvision.datasets.ImageFolder(\n",
    "        valdir,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_test : %d\" % (len(dataset_test)))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "data_path = 'data/imagenet_1k'\n",
    "saved_model_dir = 'model/'\n",
    "float_model_file = 'vgg16_pretrained_float.pth'\n",
    "scripted_float_model_file = 'vgg16_quantization_scripted.pth'\n",
    "scripted_default_quantized_model_file = 'vgg16_quantization_scripted_default_quantized.pth'\n",
    "scripted_optimal_quantized_model_file = 'vgg16_quantization_scripted_optimal_quantized.pth'\n",
    "\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 30\n",
    "\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "print('\\n Before quantization: \\n',float_model)\n",
    "\n",
    "float_model.eval()\n",
    "# Note: vgg-16 has no BN layer so that not need to fuse model\n",
    "\n",
    "num_eval_batches = 10\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "# to get a “baseline” accuracy, see the accuracy of our un-quantized model\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file) # save un_quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "\n",
      "Post Training Quantization Prepare: Inserting Observers by Calibrate\n",
      "..........Calibrate done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " After quantization: \n",
      " VGG(\n",
      "  (features): Sequential(\n",
      "    (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.3200511634349823, zero_point=63, padding=(1, 1))\n",
      "    (1): QuantizedReLU(inplace=True)\n",
      "    (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.8684448599815369, zero_point=68, padding=(1, 1))\n",
      "    (3): QuantizedReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.3496954441070557, zero_point=88, padding=(1, 1))\n",
      "    (6): QuantizedReLU(inplace=True)\n",
      "    (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.3290283679962158, zero_point=65, padding=(1, 1))\n",
      "    (8): QuantizedReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.9683136940002441, zero_point=74, padding=(1, 1))\n",
      "    (11): QuantizedReLU(inplace=True)\n",
      "    (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=1.8908194303512573, zero_point=65, padding=(1, 1))\n",
      "    (13): QuantizedReLU(inplace=True)\n",
      "    (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=2.7874293327331543, zero_point=49, padding=(1, 1))\n",
      "    (15): QuantizedReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=3.2831854820251465, zero_point=59, padding=(1, 1))\n",
      "    (18): QuantizedReLU(inplace=True)\n",
      "    (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.4918758869171143, zero_point=82, padding=(1, 1))\n",
      "    (20): QuantizedReLU(inplace=True)\n",
      "    (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.19400691986084, zero_point=76, padding=(1, 1))\n",
      "    (22): QuantizedReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.8876457214355469, zero_point=56, padding=(1, 1))\n",
      "    (25): QuantizedReLU(inplace=True)\n",
      "    (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.276840925216675, zero_point=71, padding=(1, 1))\n",
      "    (27): QuantizedReLU(inplace=True)\n",
      "    (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.4177485704421997, zero_point=74, padding=(1, 1))\n",
      "    (29): QuantizedReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_features=25088, out_features=4096, scale=0.42810365557670593, zero_point=80, qscheme=torch.per_tensor_affine)\n",
      "    (1): QuantizedReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): QuantizedLinear(in_features=4096, out_features=4096, scale=0.23070572316646576, zero_point=78, qscheme=torch.per_tensor_affine)\n",
      "    (4): QuantizedReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): QuantizedLinear(in_features=4096, out_features=1000, scale=0.44289401173591614, zero_point=28, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n",
      "Size of model after quantization\n",
      "Size (MB): 138.40837\n",
      "..........Evaluation accuracy on 300 images, 78.67\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    4. Post-training static quantization\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "num_calibration_batches = 10\n",
    "\n",
    "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "myModel.eval()\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print(myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate with the training set\n",
    "print('\\nPost Training Quantization Prepare: Inserting Observers by Calibrate')\n",
    "evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "\n",
    "print('\\n After quantization: \\n',myModel)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(myModel), saved_model_dir + scripted_default_quantized_model_file) # save default_quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " optimal quantize config: \n",
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.HistogramObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
      "..........Calibrate done\n",
      "Post Training Optimal Quantization: Convert done\n",
      "Size of model after optimal quantization\n",
      "Size (MB): 138.626255\n",
      "..........Evaluation accuracy on 300 images, 78.33\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    5. optimal\n",
    "    ·Quantizes weights on a per-channel basis\n",
    "    ·Uses a histogram observer that collects a histogram of activations and then picks quantization parameters\n",
    "    in an optimal manner.\n",
    "------------------------------\n",
    "\"\"\"\n",
    "float_model_file = 'vgg16_pretrained_float.pth'\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "# per_channel_quantized_model.fuse_model() # VGG dont need fuse\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') # set the quantize config\n",
    "print('\\n optimal quantize config: ')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True) # execute the quantize config\n",
    "evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches) # calibrate\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True) # convert to quantize model\n",
    "print('Post Training Optimal Quantization: Convert done')\n",
    "\n",
    "print(\"Size of model after optimal quantization\")\n",
    "print_size_of_model(per_channel_quantized_model)\n",
    "\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches) # test acc\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_optimal_quantized_model_file) # save quantized model\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save quantized bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "features.0.scale\n",
      "features.0.zero_point\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "features.2.scale\n",
      "features.2.zero_point\n",
      "features.2.bias\n",
      "features.5.weight\n",
      "features.5.scale\n",
      "features.5.zero_point\n",
      "features.5.bias\n",
      "features.7.weight\n",
      "features.7.scale\n",
      "features.7.zero_point\n",
      "features.7.bias\n",
      "features.10.weight\n",
      "features.10.scale\n",
      "features.10.zero_point\n",
      "features.10.bias\n",
      "features.12.weight\n",
      "features.12.scale\n",
      "features.12.zero_point\n",
      "features.12.bias\n",
      "features.14.weight\n",
      "features.14.scale\n",
      "features.14.zero_point\n",
      "features.14.bias\n",
      "features.17.weight\n",
      "features.17.scale\n",
      "features.17.zero_point\n",
      "features.17.bias\n",
      "features.19.weight\n",
      "features.19.scale\n",
      "features.19.zero_point\n",
      "features.19.bias\n",
      "features.21.weight\n",
      "features.21.scale\n",
      "features.21.zero_point\n",
      "features.21.bias\n",
      "features.24.weight\n",
      "features.24.scale\n",
      "features.24.zero_point\n",
      "features.24.bias\n",
      "features.26.weight\n",
      "features.26.scale\n",
      "features.26.zero_point\n",
      "features.26.bias\n",
      "features.28.weight\n",
      "features.28.scale\n",
      "features.28.zero_point\n",
      "features.28.bias\n",
      "classifier.0.scale\n",
      "classifier.0.zero_point\n",
      "classifier.0._packed_params.weight\n",
      "classifier.0._packed_params.bias\n",
      "classifier.0._packed_params.dtype\n",
      "classifier.3.scale\n",
      "classifier.3.zero_point\n",
      "classifier.3._packed_params.weight\n",
      "classifier.3._packed_params.bias\n",
      "classifier.3._packed_params.dtype\n",
      "classifier.6.scale\n",
      "classifier.6.zero_point\n",
      "classifier.6._packed_params.weight\n",
      "classifier.6._packed_params.bias\n",
      "classifier.6._packed_params.dtype\n",
      "quant.scale\n",
      "quant.zero_point\n"
     ]
    }
   ],
   "source": [
    "for key in per_channel_quantized_model.state_dict().keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "dealing features.0.weight\n",
      "features.0.scale\n",
      "features.0.zero_point\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "dealing features.2.weight\n",
      "features.2.scale\n",
      "features.2.zero_point\n",
      "features.2.bias\n",
      "features.5.weight\n",
      "dealing features.5.weight\n",
      "features.5.scale\n",
      "features.5.zero_point\n",
      "features.5.bias\n",
      "features.7.weight\n",
      "dealing features.7.weight\n",
      "features.7.scale\n",
      "features.7.zero_point\n",
      "features.7.bias\n",
      "features.10.weight\n",
      "dealing features.10.weight\n",
      "features.10.scale\n",
      "features.10.zero_point\n",
      "features.10.bias\n",
      "features.12.weight\n",
      "dealing features.12.weight\n",
      "features.12.scale\n",
      "features.12.zero_point\n",
      "features.12.bias\n",
      "features.14.weight\n",
      "dealing features.14.weight\n",
      "features.14.scale\n",
      "features.14.zero_point\n",
      "features.14.bias\n",
      "features.17.weight\n",
      "dealing features.17.weight\n",
      "features.17.scale\n",
      "features.17.zero_point\n",
      "features.17.bias\n",
      "features.19.weight\n",
      "dealing features.19.weight\n",
      "features.19.scale\n",
      "features.19.zero_point\n",
      "features.19.bias\n",
      "features.21.weight\n",
      "dealing features.21.weight\n",
      "features.21.scale\n",
      "features.21.zero_point\n",
      "features.21.bias\n",
      "features.24.weight\n",
      "dealing features.24.weight\n",
      "features.24.scale\n",
      "features.24.zero_point\n",
      "features.24.bias\n",
      "features.26.weight\n",
      "dealing features.26.weight\n",
      "features.26.scale\n",
      "features.26.zero_point\n",
      "features.26.bias\n",
      "features.28.weight\n",
      "dealing features.28.weight\n",
      "features.28.scale\n",
      "features.28.zero_point\n",
      "features.28.bias\n",
      "classifier.0.scale\n",
      "classifier.0.zero_point\n",
      "classifier.0._packed_params.weight\n",
      "classifier.0._packed_params.bias\n",
      "classifier.0._packed_params.dtype\n",
      "classifier.3.scale\n",
      "classifier.3.zero_point\n",
      "classifier.3._packed_params.weight\n",
      "classifier.3._packed_params.bias\n",
      "classifier.3._packed_params.dtype\n",
      "classifier.6.scale\n",
      "classifier.6.zero_point\n",
      "classifier.6._packed_params.weight\n",
      "classifier.6._packed_params.bias\n",
      "classifier.6._packed_params.dtype\n",
      "quant.scale\n",
      "quant.zero_point\n"
     ]
    }
   ],
   "source": [
    "# print('\\n\\nmodel_int8 conv keys:',per_channel_quantized_model.state_dict().keys())\n",
    "# import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "def getrealzero(_scale : float, _zero_point : int) -> int:\n",
    "    return int(torch.int_repr(torch.quantize_per_tensor(torch.tensor(0.), float(_scale), int(_zero_point), torch.qint8)).numpy())\n",
    "    # return round(-_zero_point / _scale)\n",
    "\n",
    "class hbm_channel_mem(): # each channel save 32 bits data\n",
    "    data_mem = []\n",
    "    zeropoint_mem = []\n",
    "    def __init__(self):\n",
    "        # _list_tmp = list()\n",
    "        # for iter in range(0, 32):\n",
    "            # self.data_mem.append(_list_tmp)\n",
    "        self.data_mem = [[] for x in range(0, 32)]\n",
    "        self.zeropoint_mem = [[] for x in range(0, 32)]\n",
    "\n",
    "    # def set(self, tunnel, addr, value):\n",
    "    #     data_mem\n",
    "    def autofill(self):\n",
    "        for position in range(1, 32):\n",
    "            for addr in range(len(self.data_mem[position]), len(self.data_mem[0])):\n",
    "                self.data_mem[position].append(self.zeropoint_mem[0][addr])\n",
    "            # while (len(self.data_mem[0]) > len(self.data_mem[position])):\n",
    "            #     self.data_mem[position].append(self.zeropoint_mem[0][len(self.data_mem[position])]) # change to real zero point\n",
    "\n",
    "    def print(self, maxaddrlimit = 64):\n",
    "        maxaddr = 0\n",
    "        for position in range(0, 32):\n",
    "            maxaddr = max(maxaddr, len(self.data_mem[position]))\n",
    "\n",
    "        if maxaddrlimit != -1:\n",
    "            maxaddr = min(maxaddr, maxaddrlimit)\n",
    "\n",
    "        for addr in range(0, maxaddr):\n",
    "            for position in range(0, 32):\n",
    "                if len(self.data_mem[position]) <= addr:\n",
    "                    print('NUL\\t', end = '')\n",
    "                else:\n",
    "                    print(f'{self.data_mem[position][addr]}\\t', end = '')\n",
    "            print()\n",
    "\n",
    "    def append(self, position : int, value : int, zeropoint : int):\n",
    "        # print('channel append enter')\n",
    "        # print(self)\n",
    "        # print(position, value)\n",
    "        # print('channel append exit')\n",
    "        self.data_mem[position].append(value)\n",
    "        self.zeropoint_mem[position].append(zeropoint)\n",
    "\n",
    "    def appends(self, valuelist : list):\n",
    "        for iter in range(0, 32):\n",
    "            self.data_mem[iter].append(valuelist[iter])\n",
    "\n",
    "    def save(self, filepath : str, saveaspcieorder = True):\n",
    "        self.autofill()\n",
    "        # self.print()\n",
    "        if saveaspcieorder:\n",
    "            with open(filepath, 'wb+') as f:\n",
    "                for addr_base in range(0, len(self.data_mem[0]), 8):\n",
    "                    for addr in range(addr_base + 7, addr_base - 1, -1):\n",
    "                        for position in range(0, 32):\n",
    "                            # print(type(self.data_mem[addr][position]))\n",
    "                            try:\n",
    "                                # print(addr, ' ', position)\n",
    "                                # print(self.data_mem[position][addr])\n",
    "                                # print(int(self.data_mem[position][addr]))\n",
    "                                # print(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                                f.write(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                            except:\n",
    "                                \n",
    "                                print(self.data_mem[position][addr])\n",
    "                                raise\n",
    "                        # f.write(int.from_bytes(self.data_mem[addr][position].to_bytes(1, 'little', signed = True), 'little', signed = False))\n",
    "                    # raise\n",
    "                    \n",
    "                f.close()\n",
    "        else:\n",
    "            with open(filepath, 'wb+') as f:\n",
    "                for addr in range(len(self.data_mem[0])):\n",
    "                    \n",
    "                    for position in range(31, -1, -1):\n",
    "                        # print(type(self.data_mem[addr][position]))\n",
    "                        try:\n",
    "                            # print(self.data_mem[position][addr])\n",
    "                            # print(int(self.data_mem[position][addr]))\n",
    "                            # print(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                            f.write(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                        except:\n",
    "                            \n",
    "                            print(self.data_mem[position][addr])\n",
    "                            raise\n",
    "                        # f.write(int.from_bytes(self.data_mem[addr][position].to_bytes(1, 'little', signed = True), 'little', signed = False))\n",
    "                    # raise\n",
    "                    \n",
    "                f.close()\n",
    "\n",
    "class hbm_mem():\n",
    "    hbm_data_mem = []\n",
    "    robin = int()\n",
    "\n",
    "    def __init__(self):\n",
    "        # _list_tmp = hbm_channel_mem()\n",
    "        self.robin = int(0)\n",
    "        # for iter in range(0, 32):\n",
    "            # self.hbm_data_mem.append(_list_tmp)\n",
    "        self.hbm_data_mem = [hbm_channel_mem() for x in range(0, 32)]\n",
    "        # print(\"init\")\n",
    "        # for i in self.hbm_data_mem:\n",
    "        #     print(i)\n",
    "    \n",
    "    def append_channel(self, channel : int, tunnel : int, value : int):\n",
    "        self.hbm_data_mem[channel].append(tunnel, value)\n",
    "\n",
    "    def appends_channel(self, channel : int, valuelist : list):\n",
    "        self.hbm_data_mem[channel].appends(valuelist)\n",
    "\n",
    "    def append(self, valuelist : list, zeropoint : int):\n",
    "        # print('append enter')\n",
    "        for channel in range(0, 32):\n",
    "            # print(valuelist[channel])\n",
    "            for value in valuelist[channel]:\n",
    "                self.hbm_data_mem[channel].append(self.robin, value, zeropoint)\n",
    "\n",
    "        if (self.robin == 31):\n",
    "            self.robin = 0\n",
    "        else:\n",
    "            self.robin = self.robin + 1\n",
    "        # print('append return')\n",
    "\n",
    "\n",
    "    def autofill(self):\n",
    "        while (self.robin != 0):\n",
    "            for channel in range(0, 32):\n",
    "                self.hbm_data_mem[channel].append(self.robin, 0) # TODO: change to real zero point\n",
    "\n",
    "            if (self.robin == 31):\n",
    "                self.robin = 0\n",
    "            else:\n",
    "                self.robin = self.robin + 1\n",
    "\n",
    "    def print(self):\n",
    "        for channel in range(0, 32):\n",
    "            print(f'data in {channel}:')\n",
    "            # print(self.hbm_data_mem[channel])\n",
    "            self.hbm_data_mem[channel].print()\n",
    "\n",
    "    def save(self, filepath : str):\n",
    "        for channel in range(0, 32):\n",
    "            self.hbm_data_mem[channel].save(\"{}_{}.bin\".format(filepath, channel))\n",
    "\n",
    "\n",
    "i_hbm_mem = hbm_mem()\n",
    "\n",
    "for key in per_channel_quantized_model.state_dict().keys():\n",
    "    print(key)\n",
    "    # print(layer_numpy.shape)\n",
    "    \n",
    "    if (key.find(\"weight\") >= 0):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if (per_channel_quantized_model.state_dict()[key].dim() == 4):\n",
    "        layer_numpy = torch.int_repr(per_channel_quantized_model.state_dict()[key]).numpy()\n",
    "        layer_zero_point = per_channel_quantized_model.state_dict()[key[0:-6] + \"zero_point\"].numpy()\n",
    "        layer_scale = per_channel_quantized_model.state_dict()[key[0:-6] + \"scale\"].numpy()\n",
    "        realzero = getrealzero(layer_scale, layer_zero_point)\n",
    "        # print(\"zero_point: \", layer_zero_point)\n",
    "        # print(\"scale: \", layer_scale)\n",
    "        # print(\"zero_quan: \", realzero)\n",
    "        \n",
    "        print(f\"dealing {key}\")\n",
    "        for kernel_base in range(0, layer_numpy.shape[0], 16): # tqdm.trange(0, layer_numpy.shape[0], 16): # \n",
    "            multi_channel_valuelist_tmp_channelgroup_1 = []\n",
    "            multi_channel_valuelist_tmp_channelgroup_2 = []\n",
    "            for kernel in range(kernel_base, kernel_base + 16):\n",
    "                single_channel_valuelist_tmp_channelgroup_1 = []\n",
    "                single_channel_valuelist_tmp_channelgroup_2 = []\n",
    "\n",
    "                for channel_base in range(0, layer_numpy.shape[1], 16):\n",
    "                    for channel in range(channel_base, channel_base + 8): # channelgroup 1\n",
    "                        single_channel_valuelist_tmp = [] # one row 1*9\n",
    "                        for width in range(layer_numpy.shape[2]): # probably height and width is in this order\n",
    "                            for height in range(layer_numpy.shape[3]):\n",
    "                                if (channel < layer_numpy.shape[1]): # in case of conv_1 has only 3 channel\n",
    "                                    # single_channel_valuelist_tmp.append(per_channel_quantized_model.state_dict()[key][kernel][channel][width][height])\n",
    "                                    single_channel_valuelist_tmp.append(layer_numpy[kernel][channel][width][height])\n",
    "                                else:\n",
    "                                    single_channel_valuelist_tmp.append(realzero) # change to real zero\n",
    "                        single_channel_valuelist_tmp_channelgroup_1.extend(single_channel_valuelist_tmp) # extends to 8*9 through order is in reverse\n",
    "                            \n",
    "\n",
    "                    for channel in range(channel_base + 8, channel_base + 16): # channelgroup 2\n",
    "                        single_channel_valuelist_tmp = [] # one row 1*9\n",
    "                        for width in range(layer_numpy.shape[2]): # probably height and width is in this order\n",
    "                            for height in range(layer_numpy.shape[3]):\n",
    "                                if (channel < layer_numpy.shape[1]): # in case of conv_1 has only 3 channel\n",
    "                                    # single_channel_valuelist_tmp.append(per_channel_quantized_model.state_dict()[key][kernel][channel][width][height])\n",
    "                                    single_channel_valuelist_tmp.append(layer_numpy[kernel][channel][width][height])\n",
    "                                else:\n",
    "                                    single_channel_valuelist_tmp.append(realzero)  # change to real zero\n",
    "                        single_channel_valuelist_tmp_channelgroup_2.extend(single_channel_valuelist_tmp) # extends to 8*9\n",
    "\n",
    "                multi_channel_valuelist_tmp_channelgroup_1.append(single_channel_valuelist_tmp_channelgroup_1)\n",
    "                multi_channel_valuelist_tmp_channelgroup_2.append(single_channel_valuelist_tmp_channelgroup_2)\n",
    "\n",
    "            # print(\"multi_channel_valuelist_tmp_channelgroup_1\")\n",
    "            # for row in range(0, len(multi_channel_valuelist_tmp_channelgroup_1)):\n",
    "            #     print(len(multi_channel_valuelist_tmp_channelgroup_1[row]), multi_channel_valuelist_tmp_channelgroup_1[row])\n",
    "            # print(\"multi_channel_valuelist_tmp_channelgroup_2\")\n",
    "            # for row in range(0, len(multi_channel_valuelist_tmp_channelgroup_2)):\n",
    "            #     print(len(multi_channel_valuelist_tmp_channelgroup_2[row]), multi_channel_valuelist_tmp_channelgroup_2[row])\n",
    "            # print(multi_channel_valuelist_tmp_channelgroup_1)\n",
    "            # print(multi_channel_valuelist_tmp_channelgroup_2)\n",
    "            total_channel_valuelist_tmp = []\n",
    "            total_channel_valuelist_tmp.extend(multi_channel_valuelist_tmp_channelgroup_1)\n",
    "            total_channel_valuelist_tmp.extend(multi_channel_valuelist_tmp_channelgroup_2)\n",
    "            i_hbm_mem.append(total_channel_valuelist_tmp, realzero)\n",
    "            # i_hbm_mem.print()\n",
    "            # break\n",
    "\n",
    "    elif (per_channel_quantized_model.state_dict()[key].dim() == 3):\n",
    "        pass\n",
    "    elif (per_channel_quantized_model.state_dict()[key].dim() == 2):\n",
    "        pass\n",
    "    elif (per_channel_quantized_model.state_dict()[key].dim() == 1):\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        # raise\n",
    "\n",
    "    # i_hbm_mem.print()\n",
    "    # i_hbm_mem.autofill()\n",
    "    # print('after auto fill')\n",
    "    # i_hbm_mem.print()\n",
    "    # break\n",
    "i_hbm_mem.save(\"quan_bin/quan_bin\")\n",
    "# print('\\n\\nmodel_int8 features.0.weight:',per_channel_quantized_model.state_dict()['features.0.weight'])\n",
    "# print('\\n\\nmodel_int8 features.0.bias:',per_channel_quantized_model.state_dict()['features.0.bias'])\n",
    "# print('\\n\\nmodel_int8 features.0.scale:',per_channel_quantized_model.state_dict()['features.0.scale'])\n",
    "# print('\\n\\nmodel_int8 features.0.zero_point:',per_channel_quantized_model.state_dict()['features.0.zero_point'])\n",
    "\n",
    "# print('\\n\\nmodel_int8 features.2.weight:',per_channel_quantized_model.state_dict()['features.2.weight'])\n",
    "# print('\\n\\nmodel_int8 features.2.bias:',per_channel_quantized_model.state_dict()['features.2.bias'])\n",
    "# print('\\n\\nmodel_int8 features.2.scale:',per_channel_quantized_model.state_dict()['features.2.scale'])\n",
    "# print('\\n\\nmodel_int8 features.2.zero_point:',per_channel_quantized_model.state_dict()['features.2.zero_point'])\n",
    "\n",
    "# print('\\n\\nmodel_int8 quant.scale:',per_channel_quantized_model.state_dict()['quant.scale'])\n",
    "# print('\\n\\nmodel_int8 quant.zero_point:',per_channel_quantized_model.state_dict()['quant.zero_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ -92,   24,   88],\n",
      "          [ -97,   59,  127],\n",
      "          [-115,   -8,   81]],\n",
      "\n",
      "         [[  29,    2,  -14],\n",
      "          [   7,  -12,  -43],\n",
      "          [  22,  -29,  -22]],\n",
      "\n",
      "         [[  52,  -28,  -71],\n",
      "          [  79,  -14,  -81],\n",
      "          [ 105,    3,  -46]]],\n",
      "\n",
      "\n",
      "        [[[  35,   19,   28],\n",
      "          [ -65,  -37,   37],\n",
      "          [ -38,   21,   -1]],\n",
      "\n",
      "         [[ -21,  -33,   23],\n",
      "          [-128,  -53,   85],\n",
      "          [ -37,   79,   82]],\n",
      "\n",
      "         [[ -48,  -56,  -20],\n",
      "          [ -71,  -23,   52],\n",
      "          [   8,   89,   75]]],\n",
      "\n",
      "\n",
      "        [[[  21,   62,    1],\n",
      "          [ -32,  -85,   37],\n",
      "          [  -9,  -26,   40]],\n",
      "\n",
      "         [[  37,   80,    2],\n",
      "          [ -56, -128,   40],\n",
      "          [ -10,  -36,   65]],\n",
      "\n",
      "         [[  38,   50,  -42],\n",
      "          [  10,  -55,    1],\n",
      "          [  12,  -17,   -2]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[  40,   66,   17],\n",
      "          [ 114,  127,  -24],\n",
      "          [  24,   15,    9]],\n",
      "\n",
      "         [[ -95,  -35,   -4],\n",
      "          [ -25,    4,  -67],\n",
      "          [ -33,  -33,   23]],\n",
      "\n",
      "         [[-116,  -62,  -12],\n",
      "          [ -51,   -8,    0],\n",
      "          [ -14,    1,   74]]],\n",
      "\n",
      "\n",
      "        [[[  11,  -21,   -3],\n",
      "          [ -45, -128,  -93],\n",
      "          [ -46, -120, -114]],\n",
      "\n",
      "         [[  28,  -44,   -5],\n",
      "          [   8,  -98,  -81],\n",
      "          [   7,  -68,  -77]],\n",
      "\n",
      "         [[  83,   56,   86],\n",
      "          [ 115,   74,   78],\n",
      "          [  96,   65,   68]]],\n",
      "\n",
      "\n",
      "        [[[   9,  -30,  -73],\n",
      "          [  77,  -10,  -71],\n",
      "          [  97,    8,  -15]],\n",
      "\n",
      "         [[  69,   43,  -48],\n",
      "          [ 109,    9,  -97],\n",
      "          [  53,  -55,  -82]],\n",
      "\n",
      "         [[ 127,  120,   79],\n",
      "          [  45,  -16,  -53],\n",
      "          [ -54, -126, -118]]]], dtype=torch.int8)\n"
     ]
    }
   ],
   "source": [
    "for layer in per_channel_quantized_model.state_dict().keys():\n",
    "    print(torch.int_repr(per_channel_quantized_model.state_dict()[layer]))\n",
    "    # print(per_channel_quantized_model.state_dict()[layer])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load quantized bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int.from_bytes(b'\\xfc', 'little', signed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x06\\x0b\\x00\\x1b\\x0c%\\xff\\xf5\\xfd\\x08\\xe6\\xfa\\xee*\\xf4\\x1b\\xfc\\xec\\x0e\\n\\xf1\\x12#\\xd9\\x05\\x0b\\xd9\\xe9\\x0f\\xcb\\xfc\\xa4\\xef\\x06\\x1a,\\x0c\\x1e\\xf1\\x01\\x17\\t\\xe9\\xe6\\xe4\\x0b\\xe0\\xeb\\xdf\\xe6\\xf5\\xf9\\x0e\\x1a)\\x07\\xf7\\x087\\xb5\\xd7\\x81\\xf5\\x18\\xe2\\x1c;C\\x10\\xfc\\xf8\\x14\\x00\\x01\\xf7\\xf1\\xeb\\xfc\\xe9\\xf5\\xf8\\xf7\\xf6\\xdd\\xfe\"#\\xe7\\xdf\\x04W\\x9b\\xac\\xdf\\xefX\\t\\xef\\x01\\x1f\\x10!\\xf8\\xed\\x0b\\xe6\\x00\\xe8\\xe4\\x18\\xec\\xef\\x05\\xef\\x15\\x10\\xfd\\n\\xf0\\xe0\\x06\\xff\\x01\\x05\\xef\\x14\\x1e\\x9f\\xef\\xe9\\xfaA\\x1c\\x01\\xec\\xe0\\x15\\xfc\\x03\\xf5\\xe7\\xf5\\xe8\\xfc\\xd6\\xe6\\xea\\x054\\xff\\xeb\\xdd\\xf7\\x15[\\xc0\\xb2\\xce1;\\xf8\\x0b\\x04U\\x1b0\\x03\\xf7\\x03\\xea\\xf6\\xe8\\xda\\xf4\\xdf\\t\\x06\\xea\\xeb\\xe8\\x13\\xf7\\x0b\\xb1\\xe1\\xfbt\\x81\\xbf\\x14\\xf1\\x7f\\xeb\\xef\\x02\\x16\\xfe\\xfc\\xf3\\xcb\\xff\\xf8\\x11\\xf3\\x04\\x02\\xff\\x04\\x01\\xf7\\x10\\x14\\xfb\\x04\\xf0\\xf7\\x15\\xfe\\x07\\x18\\'M\\xee\\x8d\\xe6\\xda\\xe9\\x1a\\x01\\x0c\\xf0\\xc9\\xfc\\x14\\x19\\x18\\xfa\\xed\\xd6\\x14\\xee\\xf1\\xf0\\t\\xdf\\xfd\\xea\\xde\\x05\\x0cH\\xce\\xdbY\\x06\\xf8\\xe1\\t\\x0f\\x0f\\x0c\\x12\\r\\xf9\\x0f\\xe8\"\\xfd\\xeb\\xe2\\xdf(\\x03\\xe9\\xf9\\xec\\xf7\\t\\xf2\\xcd\\xdf\\x00O\\x9c\\xcd0\\xf5Q\\x0f\\x06\\x12\\xf9\\xf5\\x1b\\x19\\x14\\x12\\xfd\\xee\\xe2\\xde\\xe6\\xed\\xc7\\x02\\x05\\x0b\\x10\\r\\x06\\xf1\\xe0\\x11\\x0e\\xa4$J\\x0c\\x0e\\x1d#\\x11\\x17\\x02\\x13\\x14\\xfc\\x11\\xf4\\x06\\xed\\xd1\\xcf\\xda\\x14\\xe0\\x02\\xfa\\xfd&\\xf7\\x16\"\\xeb\\xd4\\xf9\\xa7\\xeb\\x7f\\xd3+\\x02\\x03\\xe7\\x08\\xfa\\t\\x1d\\xfd\\x15\\xeb\\xfa\\xd5\\x15\\xde\\xef\\xfe\\x0c\\x03\\x13\\xed\\n\\x0c\\x17\\x07\\xed\\xa7\\xee\\xc0\\xd9\\x16xA\\xf2\\t\\xdb\\n\\x02\\x12-;\\t\\x04\\xf9\\x10\\xe8\\xe8\\xe3\\xf1\\xed\\xff\\xf6\\x15\\x01\\x00\\x10\\xe6\\xec\\xfe\\x13\\x8e51\\xf3:\\x07\\x07\\x0f\\x1b\\x02\\'\\x18\\x18\\x0b\\x01\\xde\\xe5\\xef\\xf0\\xe4\\xed\\xef\\x0c\\xfc\\n\\r\\x0e\\x19\\x1c\\xea\\xc6\\x01\\xac\\x06s\\xb6}\\xf4\\xef\\x12\\x06\\x02\\x1b\\x13\\xe5\\x15\\xfa\\xf1\\xdc\\x10\\xe3\\xe2\\x10\\xf3\\x00\\x01\\xef\\x0b\\x0b\\x0f\\x04\\x02\\xba\\xea\\xb7\\xf2R<=\\xd5\\xff\\xe6\\xfb\\xfe\\r\\xff\\x1c\\x07\\x03\\x0c\\x0e\\xde\\xef\\x07\\xe2\\x16\\xdd\\xf6\\x07\\xed\\xf9\\x0b\\xeb\\xfd\\xf9\\x18\\xc65\\x10\\xe8\\xf6\\x16'\n"
     ]
    }
   ],
   "source": [
    "with open(\"quan_bin/quan_bin_0.bin\", \"rb\") as f:\n",
    "    a = f.read(512)\n",
    "    print(a)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\x9f\\x1e\\x14\\xef\\x05\\x01\\xff\\x06\\xe0\\xf0\\n\\xfd\\x10\\x15\\xef\\x05\\xef\\xec\\x18\\xe4\\xe8\\x00\\xe6\\x0b\\xed\\xf8!\\x10\\x1f\\x01\\xef\\tX\\xef\\xdf\\xac\\x9bW\\x04\\xdf\\xe7#\"\\xfe\\xdd\\xf6\\xf7\\xf8\\xf5\\xe9\\xfc\\xeb\\xf1\\xf7\\x01\\x00\\x14\\xf8\\xfc\\x10C;\\x1c\\xe2\\x18\\xf5\\x81\\xd7\\xb57\\x08\\xf7\\x07)\\x1a\\x0e\\xf9\\xf5\\xe6\\xdf\\xeb\\xe0\\x0b\\xe4\\xe6\\xe9\\t\\x17\\x01\\xf1\\x1e\\x0c,\\x1a\\x06\\xef\\xa4\\xfc\\xcb\\x0f\\xe9\\xd9\\x0b\\x05\\xd9#\\x12\\xf1\\n\\x0e\\xec\\xfc\\x1b\\xf4*\\xee\\xfa\\xe6\\x08\\xfd\\xf5\\xff%\\x0c\\x1b\\x00\\x0b\\x06\\xf8\\x06Y\\xdb\\xceH\\x0c\\x05\\xde\\xea\\xfd\\xdf\\t\\xf0\\xf1\\xee\\x14\\xd6\\xed\\xfa\\x18\\x19\\x14\\xfc\\xc9\\xf0\\x0c\\x01\\x1a\\xe9\\xda\\xe6\\x8d\\xeeM\\'\\x18\\x07\\xfe\\x15\\xf7\\xf0\\x04\\xfb\\x14\\x10\\xf7\\x01\\x04\\xff\\x02\\x04\\xf3\\x11\\xf8\\xff\\xcb\\xf3\\xfc\\xfe\\x16\\x02\\xef\\xeb\\x7f\\xf1\\x14\\xbf\\x81t\\xfb\\xe1\\xb1\\x0b\\xf7\\x13\\xe8\\xeb\\xea\\x06\\t\\xdf\\xf4\\xda\\xe8\\xf6\\xea\\x03\\xf7\\x030\\x1bU\\x04\\x0b\\xf8;1\\xce\\xb2\\xc0[\\x15\\xf7\\xdd\\xeb\\xff4\\x05\\xea\\xe6\\xd6\\xfc\\xe8\\xf5\\xe7\\xf5\\x03\\xfc\\x15\\xe0\\xec\\x01\\x1cA\\xfa\\xe9\\xef'\n",
      "b'\\xf2Ax\\x16\\xd9\\xc0\\xee\\xa7\\xed\\x07\\x17\\x0c\\n\\xed\\x13\\x03\\x0c\\xfe\\xef\\xde\\x15\\xd5\\xfa\\xeb\\x15\\xfd\\x1d\\t\\xfa\\x08\\xe7\\x03\\x02+\\xd3\\x7f\\xeb\\xa7\\xf9\\xd4\\xeb\"\\x16\\xf7&\\xfd\\xfa\\x02\\xe0\\x14\\xda\\xcf\\xd1\\xed\\x06\\xf4\\x11\\xfc\\x14\\x13\\x02\\x17\\x11#\\x1d\\x0e\\x0cJ$\\xa4\\x0e\\x11\\xe0\\xf1\\x06\\r\\x10\\x0b\\x05\\x02\\xc7\\xed\\xe6\\xde\\xe2\\xee\\xfd\\x12\\x14\\x19\\x1b\\xf5\\xf9\\x12\\x06\\x0fQ\\xf50\\xcd\\x9cO\\x00\\xdf\\xcd\\xf2\\t\\xf7\\xec\\xf9\\xe9\\x03(\\xdf\\xe2\\xeb\\xfd\"\\xe8\\x0f\\xf9\\r\\x12\\x0c\\x0f\\x0f\\t\\xe1\\x16\\xf6\\xe8\\x105\\xc6\\x18\\xf9\\xfd\\xeb\\x0b\\xf9\\xed\\x07\\xf6\\xdd\\x16\\xe2\\x07\\xef\\xde\\x0e\\x0c\\x03\\x07\\x1c\\xff\\r\\xfe\\xfb\\xe6\\xff\\xd5=<R\\xf2\\xb7\\xea\\xba\\x02\\x04\\x0f\\x0b\\x0b\\xef\\x01\\x00\\xf3\\x10\\xe2\\xe3\\x10\\xdc\\xf1\\xfa\\x15\\xe5\\x13\\x1b\\x02\\x06\\x12\\xef\\xf4}\\xb6s\\x06\\xac\\x01\\xc6\\xea\\x1c\\x19\\x0e\\r\\n\\xfc\\x0c\\xef\\xed\\xe4\\xf0\\xef\\xe5\\xde\\x01\\x0b\\x18\\x18\\'\\x02\\x1b\\x0f\\x07\\x07:\\xf315\\x8e\\x13\\xfe\\xec\\xe6\\x10\\x00\\x01\\x15\\xf6\\xff\\xed\\xf1\\xe3\\xe8\\xe8\\x10\\xf9\\x04\\t;-\\x12\\x02\\n\\xdb\\t'\n"
     ]
    }
   ],
   "source": [
    "with open(\"quan_bin/quan_bin_0.bin\", \"rb\") as f:\n",
    "    a = f.read(128)\n",
    "    print(a)\n",
    "    a = f.read(128)\n",
    "    print(a)\n",
    "\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nInference time compare: \")\n",
    "run_benchmark_fcn('model/vgg16fcn_quantization_scripted_default_quantized.pth', data_loader_test)\n",
    "run_benchmark_fcn('model/vgg16fcn_quantization_scripted.pth', data_loader_test)\n",
    "# run_benchmark_fcn(saved_model_dir + scripted_optimal_quantized_model_file, data_loader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    6. compare performance\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nInference time compare: \")\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_default_quantized_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_optimal_quantized_model_file, data_loader_test)\n",
    "\n",
    "\"\"\" you can compare the model's size/accuracy/inference time.\n",
    "    ----------------------------------------------------------------------------------------\n",
    "                    | origin model | default quantized model | optimal quantized model\n",
    "    model size:     |    553 MB    |         138 MB          |        138 MB\n",
    "    test accuracy:  |    79.33     |         76.67           |        78.67\n",
    "    inference time: |    317 ms    |         254 ms          |        257 ms\n",
    "    ---------------------------------------------------------------------------------------\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
