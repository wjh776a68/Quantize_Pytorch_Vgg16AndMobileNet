{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train : 1000\n",
      "dataset_test : 1000\n",
      "\n",
      " Before quantization: \n",
      " VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Size of baseline model\n",
      "Size (MB): 553.435632\n",
      "..........Evaluation accuracy on 300 images, 79.33\n",
      "\n",
      " myModel.qconfig: \n",
      " QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "\n",
      "Post Training Quantization Prepare: Inserting Observers by Calibrate\n",
      "..................................Calibrate done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " After quantization: \n",
      " VGG(\n",
      "  (features): Sequential(\n",
      "    (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.3041377365589142, zero_point=62, padding=(1, 1))\n",
      "    (1): QuantizedReLU(inplace=True)\n",
      "    (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.8425370454788208, zero_point=67, padding=(1, 1))\n",
      "    (3): QuantizedReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.1648156642913818, zero_point=79, padding=(1, 1))\n",
      "    (6): QuantizedReLU(inplace=True)\n",
      "    (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.4871504306793213, zero_point=65, padding=(1, 1))\n",
      "    (8): QuantizedReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=2.168154716491699, zero_point=72, padding=(1, 1))\n",
      "    (11): QuantizedReLU(inplace=True)\n",
      "    (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=2.6549391746520996, zero_point=75, padding=(1, 1))\n",
      "    (13): QuantizedReLU(inplace=True)\n",
      "    (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=3.246105670928955, zero_point=54, padding=(1, 1))\n",
      "    (15): QuantizedReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=4.236720561981201, zero_point=58, padding=(1, 1))\n",
      "    (18): QuantizedReLU(inplace=True)\n",
      "    (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=3.1934516429901123, zero_point=79, padding=(1, 1))\n",
      "    (20): QuantizedReLU(inplace=True)\n",
      "    (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.667638063430786, zero_point=78, padding=(1, 1))\n",
      "    (22): QuantizedReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.0082199573516846, zero_point=58, padding=(1, 1))\n",
      "    (25): QuantizedReLU(inplace=True)\n",
      "    (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.2284514904022217, zero_point=70, padding=(1, 1))\n",
      "    (27): QuantizedReLU(inplace=True)\n",
      "    (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.3619482517242432, zero_point=72, padding=(1, 1))\n",
      "    (29): QuantizedReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_features=25088, out_features=4096, scale=0.5563019514083862, zero_point=80, qscheme=torch.per_tensor_affine)\n",
      "    (1): QuantizedReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): QuantizedLinear(in_features=4096, out_features=4096, scale=0.35580703616142273, zero_point=81, qscheme=torch.per_tensor_affine)\n",
      "    (4): QuantizedReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): QuantizedLinear(in_features=4096, out_features=1000, scale=0.5653187036514282, zero_point=28, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==========================\n",
    "    Style: static quantize\n",
    "    Model: VGG-16\n",
    "    Create by: Han_yz @ 2020/1/29\n",
    "    Email: 20125169@bjtu.edu.cn\n",
    "    Github: https://github.com/Forggtensky\n",
    "==========================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    1、Model architecture\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self,features,num_classes=1000,init_weights=False):\n",
    "        super(VGG,self).__init__()\n",
    "        self.features = features  # 提取特征部分的网络，也为Sequential格式\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(  # 分类部分的网络\n",
    "            nn.Linear(512*7*7,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,num_classes)\n",
    "        )\n",
    "        # add the quantize part\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias,0)\n",
    "            elif isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "cfgs = {\n",
    "    'vgg11':[64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg13':[64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg16':[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M'],\n",
    "    'vgg19':[64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M'],\n",
    "}\n",
    "\n",
    "def make_features(cfg:list):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2,stride=2)]  #vgg采用的池化层均为2,2参数\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels,v,kernel_size=3,padding=1)  #vgg卷积层采用的卷积核均为3,1参数\n",
    "            layers += [conv2d,nn.ReLU(True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)  #非关键字的形式输入网络的参数\n",
    "\n",
    "def vgg(model_name='vgg16',**kwargs):\n",
    "    try:\n",
    "        cfg = cfgs[model_name]\n",
    "    except:\n",
    "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
    "        exit(-1)\n",
    "    model = VGG(make_features(cfg),**kwargs)  # **kwargs为可变长度字典，保存多个输入参数\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    2、Helper functions\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def load_model(model_file):\n",
    "    model_name = \"vgg16\"\n",
    "    model = vgg(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    3. Define dataset and data loaders\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data_loaders(data_path):\n",
    "    traindir = os.path.join(data_path, 'train')\n",
    "    valdir = os.path.join(data_path, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_train : %d\" % (len(dataset)))\n",
    "\n",
    "    dataset_test = torchvision.datasets.ImageFolder(\n",
    "        valdir,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_test : %d\" % (len(dataset_test)))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "data_path = 'data/imagenet_1k'\n",
    "saved_model_dir = 'model/'\n",
    "float_model_file = 'vgg16_pretrained_float.pth'\n",
    "scripted_float_model_file = 'vgg16_quantization_scripted.pth'\n",
    "scripted_default_quantized_model_file = 'vgg16_quantization_scripted_default_quantized.pth'\n",
    "scripted_optimal_quantized_model_file = 'vgg16_quantization_scripted_optimal_quantized.pth'\n",
    "\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 30\n",
    "\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "print('\\n Before quantization: \\n',float_model)\n",
    "float_model.eval()\n",
    "\n",
    "# Note: vgg-16 has no BN layer so that not need to fuse model\n",
    "\n",
    "num_eval_batches = 10\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "# to get a “baseline” accuracy, see the accuracy of our un-quantized model\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file) # save un_quantized model\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    4. Post-training static quantization\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "num_calibration_batches = 40\n",
    "\n",
    "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print('\\n myModel.qconfig: \\n',myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate with the training set\n",
    "print('\\nPost Training Quantization Prepare: Inserting Observers by Calibrate')\n",
    "evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "\n",
    "print('\\n After quantization: \\n',myModel)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### save quantified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "dealing features.0.weight\n",
      "features.0.scale\n",
      "features.0.zero_point\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "dealing features.2.weight\n",
      "features.2.scale\n",
      "features.2.zero_point\n",
      "features.2.bias\n",
      "features.5.weight\n",
      "dealing features.5.weight\n",
      "features.5.scale\n",
      "features.5.zero_point\n",
      "features.5.bias\n",
      "features.7.weight\n",
      "dealing features.7.weight\n",
      "features.7.scale\n",
      "features.7.zero_point\n",
      "features.7.bias\n",
      "features.10.weight\n",
      "dealing features.10.weight\n",
      "features.10.scale\n",
      "features.10.zero_point\n",
      "features.10.bias\n",
      "features.12.weight\n",
      "dealing features.12.weight\n",
      "features.12.scale\n",
      "features.12.zero_point\n",
      "features.12.bias\n",
      "features.14.weight\n",
      "dealing features.14.weight\n",
      "features.14.scale\n",
      "features.14.zero_point\n",
      "features.14.bias\n",
      "features.17.weight\n",
      "dealing features.17.weight\n",
      "features.17.scale\n",
      "features.17.zero_point\n",
      "features.17.bias\n",
      "features.19.weight\n",
      "dealing features.19.weight\n",
      "features.19.scale\n",
      "features.19.zero_point\n",
      "features.19.bias\n",
      "features.21.weight\n",
      "dealing features.21.weight\n",
      "features.21.scale\n",
      "features.21.zero_point\n",
      "features.21.bias\n",
      "features.24.weight\n",
      "dealing features.24.weight\n",
      "features.24.scale\n",
      "features.24.zero_point\n",
      "features.24.bias\n",
      "features.26.weight\n",
      "dealing features.26.weight\n",
      "features.26.scale\n",
      "features.26.zero_point\n",
      "features.26.bias\n",
      "features.28.weight\n",
      "dealing features.28.weight\n",
      "features.28.scale\n",
      "features.28.zero_point\n",
      "features.28.bias\n",
      "classifier.0.scale\n",
      "classifier.0.zero_point\n",
      "classifier.0._packed_params.weight\n",
      "classifier.0._packed_params.bias\n",
      "classifier.0._packed_params.dtype\n",
      "classifier.3.scale\n",
      "classifier.3.zero_point\n",
      "classifier.3._packed_params.weight\n",
      "classifier.3._packed_params.bias\n",
      "classifier.3._packed_params.dtype\n",
      "classifier.6.scale\n",
      "classifier.6.zero_point\n",
      "classifier.6._packed_params.weight\n",
      "classifier.6._packed_params.bias\n",
      "classifier.6._packed_params.dtype\n",
      "quant.scale\n",
      "quant.zero_point\n"
     ]
    }
   ],
   "source": [
    "# print('\\n\\nmodel_int8 conv keys:',per_channel_quantized_model.state_dict().keys())\n",
    "# import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "def getrealzero(_scale : float, _zero_point : int) -> int:\n",
    "    return int(torch.int_repr(torch.quantize_per_tensor(torch.tensor(0.), float(_scale), int(_zero_point), torch.qint8)).numpy())\n",
    "    # return round(-_zero_point / _scale)\n",
    "\n",
    "class hbm_channel_mem(): # each channel save 32 bits data\n",
    "    data_mem = []\n",
    "    zeropoint_mem = []\n",
    "    def __init__(self):\n",
    "        # _list_tmp = list()\n",
    "        # for iter in range(0, 32):\n",
    "            # self.data_mem.append(_list_tmp)\n",
    "        self.data_mem = [[] for x in range(0, 32)]\n",
    "        self.zeropoint_mem = [[] for x in range(0, 32)]\n",
    "\n",
    "    # def set(self, tunnel, addr, value):\n",
    "    #     data_mem\n",
    "    def autofill(self):\n",
    "        for position in range(1, 32):\n",
    "            for addr in range(len(self.data_mem[position]), len(self.data_mem[0])):\n",
    "                self.data_mem[position].append(self.zeropoint_mem[0][addr])\n",
    "            # while (len(self.data_mem[0]) > len(self.data_mem[position])):\n",
    "            #     self.data_mem[position].append(self.zeropoint_mem[0][len(self.data_mem[position])]) # change to real zero point\n",
    "\n",
    "    def print(self, maxaddrlimit = 64):\n",
    "        maxaddr = 0\n",
    "        for position in range(0, 32):\n",
    "            maxaddr = max(maxaddr, len(self.data_mem[position]))\n",
    "\n",
    "        if maxaddrlimit != -1:\n",
    "            maxaddr = min(maxaddr, maxaddrlimit)\n",
    "\n",
    "        for addr in range(0, maxaddr):\n",
    "            for position in range(0, 32):\n",
    "                if len(self.data_mem[position]) <= addr:\n",
    "                    print('NUL\\t', end = '')\n",
    "                else:\n",
    "                    print(f'{self.data_mem[position][addr]}\\t', end = '')\n",
    "            print()\n",
    "\n",
    "    def append(self, position : int, value : int, zeropoint : int):\n",
    "        # print('channel append enter')\n",
    "        # print(self)\n",
    "        # print(position, value)\n",
    "        # print('channel append exit')\n",
    "        self.data_mem[position].append(value)\n",
    "        self.zeropoint_mem[position].append(zeropoint)\n",
    "\n",
    "    def appends(self, valuelist : list):\n",
    "        for iter in range(0, 32):\n",
    "            self.data_mem[iter].append(valuelist[iter])\n",
    "\n",
    "    def save(self, filepath : str, saveaspcieorder = True):\n",
    "        self.autofill()\n",
    "        # self.print()\n",
    "        if saveaspcieorder:\n",
    "            with open(filepath, 'wb+') as f:\n",
    "                for addr_base in range(0, len(self.data_mem[0]), 8):\n",
    "                    for addr in range(addr_base + 7, addr_base - 1, -1):\n",
    "                        for position in range(0, 32):\n",
    "                            # print(type(self.data_mem[addr][position]))\n",
    "                            try:\n",
    "                                # print(addr, ' ', position)\n",
    "                                # print(self.data_mem[position][addr])\n",
    "                                # print(int(self.data_mem[position][addr]))\n",
    "                                # print(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                                f.write(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                            except:\n",
    "                                \n",
    "                                print(self.data_mem[position][addr])\n",
    "                                raise\n",
    "                        # f.write(int.from_bytes(self.data_mem[addr][position].to_bytes(1, 'little', signed = True), 'little', signed = False))\n",
    "                    # raise\n",
    "                    \n",
    "                f.close()\n",
    "        else:\n",
    "            with open(filepath, 'wb+') as f:\n",
    "                for addr in range(len(self.data_mem[0])):\n",
    "                    \n",
    "                    for position in range(31, -1, -1):\n",
    "                        # print(type(self.data_mem[addr][position]))\n",
    "                        try:\n",
    "                            # print(self.data_mem[position][addr])\n",
    "                            # print(int(self.data_mem[position][addr]))\n",
    "                            # print(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                            f.write(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                        except:\n",
    "                            \n",
    "                            print(self.data_mem[position][addr])\n",
    "                            raise\n",
    "                        # f.write(int.from_bytes(self.data_mem[addr][position].to_bytes(1, 'little', signed = True), 'little', signed = False))\n",
    "                    # raise\n",
    "                    \n",
    "                f.close()\n",
    "\n",
    "class hbm_mem():\n",
    "    hbm_data_mem = []\n",
    "    robin = int()\n",
    "\n",
    "    def __init__(self):\n",
    "        # _list_tmp = hbm_channel_mem()\n",
    "        self.robin = int(0)\n",
    "        # for iter in range(0, 32):\n",
    "            # self.hbm_data_mem.append(_list_tmp)\n",
    "        self.hbm_data_mem = [hbm_channel_mem() for x in range(0, 32)]\n",
    "        # print(\"init\")\n",
    "        # for i in self.hbm_data_mem:\n",
    "        #     print(i)\n",
    "    \n",
    "    def append_channel(self, channel : int, tunnel : int, value : int):\n",
    "        self.hbm_data_mem[channel].append(tunnel, value)\n",
    "\n",
    "    def appends_channel(self, channel : int, valuelist : list):\n",
    "        self.hbm_data_mem[channel].appends(valuelist)\n",
    "\n",
    "    def append(self, valuelist : list, zeropoint : int):\n",
    "        # print('append enter')\n",
    "        for channel in range(0, 32):\n",
    "            # print(valuelist[channel])\n",
    "            for value in valuelist[channel]:\n",
    "                self.hbm_data_mem[channel].append(self.robin, value, zeropoint)\n",
    "\n",
    "        if (self.robin == 31):\n",
    "            self.robin = 0\n",
    "        else:\n",
    "            self.robin = self.robin + 1\n",
    "        # print('append return')\n",
    "\n",
    "\n",
    "    def autofill(self):\n",
    "        while (self.robin != 0):\n",
    "            for channel in range(0, 32):\n",
    "                self.hbm_data_mem[channel].append(self.robin, 0) # TODO: change to real zero point\n",
    "\n",
    "            if (self.robin == 31):\n",
    "                self.robin = 0\n",
    "            else:\n",
    "                self.robin = self.robin + 1\n",
    "\n",
    "    def print(self):\n",
    "        for channel in range(0, 32):\n",
    "            print(f'data in {channel}:')\n",
    "            # print(self.hbm_data_mem[channel])\n",
    "            self.hbm_data_mem[channel].print()\n",
    "\n",
    "    def save(self, filepath : str):\n",
    "        for channel in range(0, 32):\n",
    "            self.hbm_data_mem[channel].save(\"{}_{}.bin\".format(filepath, channel))\n",
    "\n",
    "\n",
    "i_hbm_mem = hbm_mem()\n",
    "\n",
    "for key in myModel.state_dict().keys():\n",
    "    print(key)\n",
    "    # print(layer_numpy.shape)\n",
    "    \n",
    "    if (key.find(\"weight\") >= 0):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if (myModel.state_dict()[key].dim() == 4):\n",
    "        layer_numpy = torch.int_repr(myModel.state_dict()[key]).numpy()\n",
    "        layer_zero_point = myModel.state_dict()[key].q_zero_point() # myModel.state_dict()[key[0:-6] + \"zero_point\"].numpy()\n",
    "        layer_scale = myModel.state_dict()[key].q_scale() # myModel.state_dict()[key[0:-6] + \"scale\"].numpy()\n",
    "        realzero = getrealzero(layer_scale, layer_zero_point)\n",
    "        # print(\"zero_point: \", layer_zero_point)\n",
    "        # print(\"scale: \", layer_scale)\n",
    "        # print(\"zero_quan: \", realzero)\n",
    "        \n",
    "        print(f\"dealing {key}\")\n",
    "        for kernel_base in range(0, layer_numpy.shape[0], 16): # tqdm.trange(0, layer_numpy.shape[0], 16): # \n",
    "            multi_channel_valuelist_tmp_channelgroup_1 = []\n",
    "            multi_channel_valuelist_tmp_channelgroup_2 = []\n",
    "            for kernel in range(kernel_base, kernel_base + 16):\n",
    "                single_channel_valuelist_tmp_channelgroup_1 = []\n",
    "                single_channel_valuelist_tmp_channelgroup_2 = []\n",
    "\n",
    "                for channel_base in range(0, layer_numpy.shape[1], 16):\n",
    "                    for channel in range(channel_base, channel_base + 8): # channelgroup 1\n",
    "                        single_channel_valuelist_tmp = [] # one row 1*9\n",
    "                        for width in range(layer_numpy.shape[2]): # probably height and width is in this order\n",
    "                            for height in range(layer_numpy.shape[3]):\n",
    "                                if (channel < layer_numpy.shape[1]): # in case of conv_1 has only 3 channel\n",
    "                                    # single_channel_valuelist_tmp.append(per_channel_quantized_model.state_dict()[key][kernel][channel][width][height])\n",
    "                                    single_channel_valuelist_tmp.append(layer_numpy[kernel][channel][width][height])\n",
    "                                else:\n",
    "                                    single_channel_valuelist_tmp.append(realzero) # change to real zero\n",
    "                        single_channel_valuelist_tmp_channelgroup_1.extend(single_channel_valuelist_tmp) # extends to 8*9 through order is in reverse\n",
    "                            \n",
    "\n",
    "                    for channel in range(channel_base + 8, channel_base + 16): # channelgroup 2\n",
    "                        single_channel_valuelist_tmp = [] # one row 1*9\n",
    "                        for width in range(layer_numpy.shape[2]): # probably height and width is in this order\n",
    "                            for height in range(layer_numpy.shape[3]):\n",
    "                                if (channel < layer_numpy.shape[1]): # in case of conv_1 has only 3 channel\n",
    "                                    # single_channel_valuelist_tmp.append(per_channel_quantized_model.state_dict()[key][kernel][channel][width][height])\n",
    "                                    single_channel_valuelist_tmp.append(layer_numpy[kernel][channel][width][height])\n",
    "                                else:\n",
    "                                    single_channel_valuelist_tmp.append(realzero)  # change to real zero\n",
    "                        single_channel_valuelist_tmp_channelgroup_2.extend(single_channel_valuelist_tmp) # extends to 8*9\n",
    "\n",
    "                multi_channel_valuelist_tmp_channelgroup_1.append(single_channel_valuelist_tmp_channelgroup_1)\n",
    "                multi_channel_valuelist_tmp_channelgroup_2.append(single_channel_valuelist_tmp_channelgroup_2)\n",
    "\n",
    "            # print(\"multi_channel_valuelist_tmp_channelgroup_1\")\n",
    "            # for row in range(0, len(multi_channel_valuelist_tmp_channelgroup_1)):\n",
    "            #     print(len(multi_channel_valuelist_tmp_channelgroup_1[row]), multi_channel_valuelist_tmp_channelgroup_1[row])\n",
    "            # print(\"multi_channel_valuelist_tmp_channelgroup_2\")\n",
    "            # for row in range(0, len(multi_channel_valuelist_tmp_channelgroup_2)):\n",
    "            #     print(len(multi_channel_valuelist_tmp_channelgroup_2[row]), multi_channel_valuelist_tmp_channelgroup_2[row])\n",
    "            # print(multi_channel_valuelist_tmp_channelgroup_1)\n",
    "            # print(multi_channel_valuelist_tmp_channelgroup_2)\n",
    "            total_channel_valuelist_tmp = []\n",
    "            total_channel_valuelist_tmp.extend(multi_channel_valuelist_tmp_channelgroup_1)\n",
    "            total_channel_valuelist_tmp.extend(multi_channel_valuelist_tmp_channelgroup_2)\n",
    "            i_hbm_mem.append(total_channel_valuelist_tmp, realzero)\n",
    "            # i_hbm_mem.print()\n",
    "            # break\n",
    "\n",
    "    elif (myModel.state_dict()[key].dim() == 3):\n",
    "        pass\n",
    "    elif (myModel.state_dict()[key].dim() == 2):\n",
    "        pass\n",
    "    elif (myModel.state_dict()[key].dim() == 1):\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        # raise\n",
    "\n",
    "    # i_hbm_mem.print()\n",
    "    # i_hbm_mem.autofill()\n",
    "    # print('after auto fill')\n",
    "    # i_hbm_mem.print()\n",
    "    # break\n",
    "i_hbm_mem.save(\"quan_bin_pertensor/quan_bin_pertensor\")\n",
    "# print('\\n\\nmodel_int8 features.0.weight:',per_channel_quantized_model.state_dict()['features.0.weight'])\n",
    "# print('\\n\\nmodel_int8 features.0.bias:',per_channel_quantized_model.state_dict()['features.0.bias'])\n",
    "# print('\\n\\nmodel_int8 features.0.scale:',per_channel_quantized_model.state_dict()['features.0.scale'])\n",
    "# print('\\n\\nmodel_int8 features.0.zero_point:',per_channel_quantized_model.state_dict()['features.0.zero_point'])\n",
    "\n",
    "# print('\\n\\nmodel_int8 features.2.weight:',per_channel_quantized_model.state_dict()['features.2.weight'])\n",
    "# print('\\n\\nmodel_int8 features.2.bias:',per_channel_quantized_model.state_dict()['features.2.bias'])\n",
    "# print('\\n\\nmodel_int8 features.2.scale:',per_channel_quantized_model.state_dict()['features.2.scale'])\n",
    "# print('\\n\\nmodel_int8 features.2.zero_point:',per_channel_quantized_model.state_dict()['features.2.zero_point'])\n",
    "\n",
    "# print('\\n\\nmodel_int8 quant.scale:',per_channel_quantized_model.state_dict()['quant.scale'])\n",
    "# print('\\n\\nmodel_int8 quant.zero_point:',per_channel_quantized_model.state_dict()['quant.zero_point'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### save scale and zero_point value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# return  n, p\n",
    "def get_mo(M, P):\n",
    "    last_error = None\n",
    "    result = M * P\n",
    "\n",
    "    for n in range(1, 16):\n",
    "        Mo = int(round(2 ** n * M)) # 这里不一定要四舍五入截断，因为python定点数不好表示才这样处理\n",
    "\n",
    "        approx_result = (Mo * P) >> n\n",
    "\n",
    "        cur_error = result - approx_result\n",
    "        if last_error == None:\n",
    "            pass\n",
    "        else:\n",
    "            if (abs(last_error - cur_error) < 1e-1 and abs(cur_error) < 1) or (n == 15):\n",
    "                yield n\n",
    "                break\n",
    "        last_error = cur_error\n",
    "\n",
    "    # print(\"Mo=%d, approx=%f, error=%f\"%\\\n",
    "    #     (Mo, approx_result, result-approx_result))\n",
    "    \n",
    "    yield Mo\n",
    "\n",
    "    return\n",
    "\n",
    "# M = 0.0072474273418460\n",
    "# P = 7091\n",
    "# ret_get_mo = iter(get_mo(M, P))\n",
    "# print(next(ret_get_mo)) # 1\n",
    "# print(next(ret_get_mo)) # 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight torch.Size([64, 3, 3, 3])\n",
      "features.0.scale torch.Size([])\n",
      "features.0.zero_point torch.Size([])\n",
      "features.0.bias torch.Size([64])\n",
      "features.2.weight torch.Size([64, 64, 3, 3])\n",
      "features.2.scale torch.Size([])\n",
      "features.2.zero_point torch.Size([])\n",
      "features.2.bias torch.Size([64])\n",
      "features.5.weight torch.Size([128, 64, 3, 3])\n",
      "features.5.scale torch.Size([])\n",
      "features.5.zero_point torch.Size([])\n",
      "features.5.bias torch.Size([128])\n",
      "features.7.weight torch.Size([128, 128, 3, 3])\n",
      "features.7.scale torch.Size([])\n",
      "features.7.zero_point torch.Size([])\n",
      "features.7.bias torch.Size([128])\n",
      "features.10.weight torch.Size([256, 128, 3, 3])\n",
      "features.10.scale torch.Size([])\n",
      "features.10.zero_point torch.Size([])\n",
      "features.10.bias torch.Size([256])\n",
      "features.12.weight torch.Size([256, 256, 3, 3])\n",
      "features.12.scale torch.Size([])\n",
      "features.12.zero_point torch.Size([])\n",
      "features.12.bias torch.Size([256])\n",
      "features.14.weight torch.Size([256, 256, 3, 3])\n",
      "features.14.scale torch.Size([])\n",
      "features.14.zero_point torch.Size([])\n",
      "features.14.bias torch.Size([256])\n",
      "features.17.weight torch.Size([512, 256, 3, 3])\n",
      "features.17.scale torch.Size([])\n",
      "features.17.zero_point torch.Size([])\n",
      "features.17.bias torch.Size([512])\n",
      "features.19.weight torch.Size([512, 512, 3, 3])\n",
      "features.19.scale torch.Size([])\n",
      "features.19.zero_point torch.Size([])\n",
      "features.19.bias torch.Size([512])\n",
      "features.21.weight torch.Size([512, 512, 3, 3])\n",
      "features.21.scale torch.Size([])\n",
      "features.21.zero_point torch.Size([])\n",
      "features.21.bias torch.Size([512])\n",
      "features.24.weight torch.Size([512, 512, 3, 3])\n",
      "features.24.scale torch.Size([])\n",
      "features.24.zero_point torch.Size([])\n",
      "features.24.bias torch.Size([512])\n",
      "features.26.weight torch.Size([512, 512, 3, 3])\n",
      "features.26.scale torch.Size([])\n",
      "features.26.zero_point torch.Size([])\n",
      "features.26.bias torch.Size([512])\n",
      "features.28.weight torch.Size([512, 512, 3, 3])\n",
      "features.28.scale torch.Size([])\n",
      "features.28.zero_point torch.Size([])\n",
      "features.28.bias torch.Size([512])\n",
      "classifier.0.scale torch.Size([])\n",
      "classifier.0.zero_point torch.Size([])\n",
      "classifier.0._packed_params.weight torch.Size([4096, 25088])\n",
      "classifier.0._packed_params.bias torch.Size([4096])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'torch.dtype' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m myModel\u001b[39m.\u001b[39mstate_dict()\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m----> 2\u001b[0m     \u001b[39mprint\u001b[39m(key, myModel\u001b[39m.\u001b[39;49mstate_dict()[key]\u001b[39m.\u001b[39;49mshape)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'torch.dtype' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for key in myModel.state_dict().keys():\n",
    "    print(key, myModel.state_dict()[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight   0.009980898350477219   0   0.3041377365589142   62\n",
      "[67.014824, 67.012505, 66.99944, 67.11255, 67.00365, 67.061, 66.9992, 67.20313, 67.012115, 66.98165, 66.80699, 66.99638, 66.67418, 66.99047, 66.99919, 66.99518, 66.95112, 66.986336, 66.97603, 67.00203, 66.99663, 67.00346, 67.08957, 66.99276, 67.01384, 66.98022, 67.006516, 66.897644, 66.99607, 66.88489, 67.004196, 67.00024, 67.00769, 66.998955, 67.0065, 66.77249, 66.990295, 66.81822, 66.971504, 67.000984, 66.998436, 67.16307, 67.00127, 67.062126, 66.755516, 66.865974, 66.99053, 67.00317, 66.988075, 66.975975, 67.030106, 66.99986, 67.22038, 66.99952, 67.17542, 67.00086, 67.00975, 67.00179, 66.99977, 67.017494, 66.88105, 66.97106, 66.94674, 66.98962]\n",
      "features.0.scale pass\n",
      "features.0.zero_point pass\n",
      "features.0.bias pass\n",
      "features.2.weight   0.004230252001434565   0   0.8425370454788208   67\n",
      "[78.5343, 78.43863, 78.38007, 78.9694, 78.71542, 78.880165, 78.52515, 78.76662, 78.86514, 77.91949, 78.75966, 78.776344, 77.96307, 79.09815, 78.99924, 79.065636, 78.83472, 78.51425, 78.796165, 79.14389, 79.01925, 78.129715, 78.811066, 80.14511, 78.81095, 78.968704, 80.104416, 78.885956, 78.36462, 78.381546, 78.356, 79.53922, 78.884575, 78.83082, 78.3056, 78.881, 78.89501, 79.42858, 78.124374, 78.9759, 78.52966, 79.24919, 78.50152, 78.89481, 78.82281, 78.71322, 78.901, 78.85381, 78.52337, 78.44301, 78.55046, 78.3015, 79.010185, 78.97186, 79.015144, 78.64736, 78.68426, 78.120476, 78.9776, 80.136154, 78.61542, 79.10292, 79.35197, 78.83918]\n",
      "features.2.scale pass\n",
      "features.2.zero_point pass\n",
      "features.2.bias pass\n",
      "features.5.weight   0.004698746372014284   0   1.1648156642913818   79\n",
      "[64.81863, 64.59556, 64.73, 64.71728, 65.15187, 64.95464, 64.557495, 64.662895, 64.21651, 64.503006, 64.22836, 65.0942, 64.92823, 64.762314, 64.56297, 63.93234, 65.07902, 64.784615, 64.14409, 64.849236, 64.308815, 64.82366, 64.48155, 64.713264, 64.10573, 64.819756, 64.7217, 64.318825, 64.37678, 64.97534, 65.03584, 65.09448, 64.7792, 64.467964, 64.68209, 65.139656, 64.81154, 64.28378, 63.95933, 64.41962, 64.28531, 64.906906, 64.64947, 64.71198, 64.392715, 64.510185, 64.84812, 64.78192, 65.08066, 65.01614, 64.95378, 64.71776, 64.58917, 65.259735, 63.52559, 64.68605, 64.49407, 64.795685, 65.10768, 65.013626, 64.400795, 64.5566, 64.703636, 64.494965, 64.3289, 64.31228, 64.82262, 64.51917, 63.98146, 64.86675, 64.706276, 65.04107, 65.17696, 64.88418, 64.40177, 64.99413, 65.19014, 64.848564, 64.60931, 64.65736, 64.91299, 63.97707, 64.50931, 64.91959, 65.2006, 64.548256, 65.122314, 64.63823, 64.521965, 64.64178, 64.59855, 64.97329, 64.97749, 65.26197, 64.96279, 64.83122, 64.71372, 64.16339, 64.67946, 65.20481, 64.58165, 64.6366, 64.50141, 64.682785, 64.759476, 64.99056, 64.46781, 65.26222, 64.33193, 64.51686, 64.12914, 64.238304, 65.1449, 64.784485, 65.05523, 64.9503, 64.90743, 65.31166, 64.6855, 64.367966, 64.54539, 64.59092, 64.77076, 64.62564, 64.758156, 64.69973, 64.60819, 64.858444]\n",
      "features.5.scale pass\n",
      "features.5.zero_point pass\n",
      "features.5.bias pass\n",
      "features.7.weight   0.002840567845851183   0   1.4871504306793213   65\n",
      "[71.63985, 71.767624, 71.424576, 71.574524, 71.71036, 71.70396, 71.9221, 71.81833, 71.85018, 71.795746, 71.509056, 71.594765, 71.55925, 71.89205, 71.6974, 71.7535, 71.71317, 71.8538, 71.653206, 71.485725, 71.59288, 71.77352, 71.617676, 71.583954, 71.560745, 71.79277, 71.51667, 71.77871, 71.39664, 71.94973, 71.506195, 71.73223, 71.67743, 71.51613, 71.49233, 71.71387, 72.05069, 71.7514, 71.73257, 71.6586, 71.69274, 71.81615, 71.56228, 71.71915, 71.77344, 71.70232, 71.75167, 71.59032, 71.37145, 71.965805, 71.54187, 71.63114, 71.755684, 71.66667, 71.97102, 71.62426, 71.51436, 71.64642, 71.77988, 71.70953, 71.666405, 71.48654, 71.745384, 72.00651, 71.561905, 71.87056, 71.74239, 71.70576, 71.58403, 71.71945, 71.79765, 71.705025, 71.43435, 71.669205, 71.559906, 71.84862, 71.6933, 71.62591, 71.63919, 71.52771, 71.366776, 71.49695, 71.66179, 71.668724, 71.729004, 71.780495, 71.50641, 71.739586, 71.840836, 71.57924, 71.70895, 71.47554, 71.67446, 71.42123, 71.79158, 71.86377, 71.676895, 71.70861, 71.61862, 71.72046, 71.830345, 71.58791, 71.72995, 71.61369, 71.83471, 71.47196, 71.89312, 71.52857, 71.89497, 71.424866, 71.70466, 71.85839, 71.78028, 71.62854, 71.63439, 71.59941, 71.6963, 71.71592, 71.626114, 71.54213, 71.756096, 71.7692, 71.445816, 71.54743, 71.71991, 71.544136, 71.75659, 71.91372]\n",
      "features.7.scale pass\n",
      "features.7.zero_point pass\n",
      "features.7.bias pass\n",
      "features.10.weight   0.004577364306896925   0   2.168154716491699   72\n",
      "[74.666214, 74.50365, 74.810005, 74.47922, 74.504555, 74.66571, 75.10489, 74.35464, 74.797615, 74.76245, 74.627686, 74.7319, 74.62439, 74.21403, 74.429924, 74.62379, 74.57739, 74.96316, 74.6022, 74.92019, 74.085625, 74.709366, 74.680885, 74.11393, 74.42333, 74.56375, 74.348946, 74.65543, 74.73493, 74.52544, 74.48924, 73.78817, 74.20708, 75.09293, 74.631836, 74.37668, 74.79882, 74.8782, 74.67582, 74.70906, 74.6438, 74.55041, 74.81632, 74.35834, 74.52588, 74.634056, 74.269745, 74.810356, 74.76074, 74.54522, 75.107315, 74.28674, 74.20197, 74.63964, 74.29769, 74.06713, 74.56482, 74.694176, 74.43497, 74.28484, 74.80931, 74.37389, 74.79533, 74.52694, 74.509834, 74.82189, 74.74239, 74.53601, 74.77765, 74.67885, 74.27162, 74.551834, 74.7524, 74.77784, 74.6331, 74.198685, 74.24609, 74.44448, 74.476395, 74.4112, 74.50234, 73.942154, 74.62544, 74.69975, 74.76834, 74.51948, 74.00642, 74.49888, 74.53547, 74.55097, 75.02778, 74.1969, 74.89141, 74.673805, 74.89262, 74.55062, 74.98124, 74.70865, 74.72865, 74.996635, 74.5873, 74.09763, 74.67007, 74.64771, 74.470184, 74.72648, 74.69673, 74.86456, 74.73154, 74.334526, 74.17235, 74.71804, 74.27461, 74.43594, 74.486465, 74.72775, 74.2358, 74.24355, 74.794365, 74.5219, 74.667496, 74.42598, 74.273506, 74.64385, 74.601265, 74.18098, 75.029526, 74.76056, 74.4403, 74.86689, 74.709114, 74.70175, 74.482475, 74.67054, 74.90748, 74.701, 74.33487, 74.19888, 74.52852, 74.440125, 74.528046, 74.582855, 74.562645, 74.984535, 74.89886, 74.57495, 74.49476, 74.582405, 74.02176, 74.53099, 74.71457, 74.01902, 74.50859, 74.50972, 74.70302, 74.73394, 74.475555, 74.48847, 74.732124, 74.4419, 74.752495, 74.880585, 74.90381, 74.579834, 74.73843, 74.74138, 74.65482, 74.46174, 74.84687, 74.430824, 74.03041, 74.15153, 74.4531, 74.780815, 74.58167, 74.49352, 74.548996, 74.88243, 74.47778, 74.32125, 74.46577, 74.82149, 75.05657, 74.89177, 74.52247, 74.94404, 74.80926, 74.9905, 74.74504, 74.06535, 74.94485, 74.22909, 74.45803, 74.674644, 74.492386, 74.84831, 74.55386, 74.87849, 74.527145, 74.65274, 75.26683, 74.409134, 74.23202, 74.73647, 74.64488, 74.752846, 74.42614, 74.50106, 74.62786, 74.595695, 74.21505, 74.33446, 74.55696, 74.69563, 74.781075, 74.81467, 74.784096, 74.53501, 74.74876, 75.065605, 74.905815, 74.83137, 74.72513, 74.56579, 74.308815, 74.47393, 74.91831, 74.35077, 74.576004, 74.75879, 74.56755, 74.53697, 74.70454, 74.447525, 74.7945, 74.73767, 74.56777, 74.461555, 74.58097, 74.79698, 74.86322, 74.46868, 74.69423, 74.52374, 74.60426, 74.715355, 74.55026, 74.4673, 74.67076, 74.46571, 74.61903, 74.71489, 74.55387, 74.76256, 74.64072, 74.36101]\n",
      "features.10.scale pass\n",
      "features.10.zero_point pass\n",
      "features.10.bias pass\n",
      "features.12.weight   0.003020108211785555   0   2.6549391746520996   75\n",
      "[53.065098, 53.21055, 53.714317, 53.615723, 53.205956, 53.733788, 53.518276, 53.66893, 53.37638, 53.47597, 53.38193, 53.413033, 53.84117, 53.442703, 53.565224, 53.296688, 53.56412, 53.518364, 52.959255, 53.24022, 53.359623, 52.899635, 53.598324, 53.17284, 53.427876, 53.568134, 52.79873, 53.514385, 53.466415, 53.382744, 53.798866, 53.695957, 53.313576, 53.35021, 53.101456, 53.62645, 53.26466, 53.19711, 53.429726, 53.629585, 53.424217, 53.25062, 53.354717, 53.412186, 53.11857, 53.357594, 53.373528, 53.65483, 53.529736, 53.51277, 53.24438, 53.543945, 53.537262, 53.16835, 52.834675, 53.428387, 53.4824, 53.648113, 53.80709, 53.31627, 53.308517, 53.34382, 53.413704, 53.320786, 53.358265, 52.712906, 53.320847, 53.28577, 53.35711, 53.443104, 53.356777, 53.339596, 52.834034, 53.602295, 53.13096, 53.073524, 53.662186, 53.55009, 53.403717, 53.316353, 53.31859, 53.574474, 53.730213, 53.500008, 53.590622, 53.536198, 53.375217, 53.28636, 53.27242, 53.19597, 52.46545, 52.789642, 53.494324, 53.55069, 53.2788, 53.530796, 53.296814, 53.28507, 53.226425, 52.856113, 53.4879, 53.463387, 53.558838, 53.518826, 53.684475, 53.24492, 53.54134, 53.703224, 53.14189, 53.54871, 53.51699, 53.604973, 53.39548, 53.497814, 53.37174, 53.61303, 53.319267, 53.305145, 53.154335, 53.169617, 53.536354, 53.348804, 53.571503, 53.228317, 52.90161, 53.32081, 53.002113, 53.578953, 53.647247, 53.20508, 53.387604, 53.451824, 53.459225, 53.08326, 53.109898, 53.832516, 53.300602, 53.1453, 53.50043, 53.10463, 53.534195, 53.394943, 53.48145, 53.27843, 53.69401, 53.60853, 53.2622, 53.256424, 53.316383, 53.136524, 53.520603, 53.5083, 53.49528, 53.410416, 53.42418, 53.644234, 52.86815, 53.511505, 53.34479, 53.533096, 53.47409, 53.230278, 53.28139, 53.26408, 53.587788, 53.586468, 53.572445, 53.42101, 53.3783, 53.27346, 53.533455, 53.154217, 53.281727, 52.992634, 53.644238, 53.34821, 53.396187, 53.364414, 53.64082, 53.34194, 53.2678, 53.464672, 53.38858, 53.325817, 53.46099, 53.468513, 53.199287, 53.33945, 53.49656, 53.31398, 53.734932, 53.35795, 53.252655, 53.215427, 53.42413, 53.506226, 53.545017, 53.54521, 53.41637, 53.316, 53.21872, 53.493004, 53.5912, 53.2593, 53.431126, 53.28188, 53.689396, 52.830067, 53.828365, 53.54363, 53.41003, 53.669094, 53.117218, 53.64983, 53.56097, 53.463787, 53.490685, 53.47041, 53.264256, 53.42685, 53.35133, 53.060368, 53.24766, 53.532913, 53.267143, 53.482246, 53.449055, 52.971783, 53.41944, 53.288326, 53.618393, 53.65827, 53.779842, 53.564186, 53.495052, 53.451202, 53.507187, 53.237125, 53.36335, 53.35671, 53.758377, 52.76447, 53.437237, 53.528263, 53.181904, 53.478054, 53.616806, 53.58062, 53.3879, 53.57544, 53.286602, 53.453438, 53.403923, 53.636482, 53.137787, 52.9427]\n",
      "features.12.scale pass\n",
      "features.12.zero_point pass\n",
      "features.12.bias pass\n",
      "features.14.weight   0.004730787593871355   0   3.246105670928955   54\n",
      "[56.958847, 56.816624, 56.993523, 56.715908, 56.728, 56.682476, 56.931576, 56.798813, 56.97097, 57.02571, 56.88242, 56.620956, 56.6336, 56.974953, 56.347176, 57.13559, 57.11835, 56.851456, 57.11981, 57.142002, 56.99216, 57.238205, 56.591705, 56.928974, 56.690014, 56.957127, 56.65927, 56.927402, 56.789604, 57.027264, 56.79091, 56.825817, 56.77134, 56.95644, 56.798126, 57.13839, 56.704334, 56.734097, 57.01162, 56.739002, 57.16942, 56.825565, 57.091705, 57.248615, 56.87315, 57.021152, 56.85654, 56.592484, 56.91683, 56.86016, 56.797024, 57.288593, 57.007595, 57.204746, 56.652523, 57.183388, 57.08513, 56.770454, 57.162163, 56.92871, 56.87568, 56.84714, 56.63646, 56.77476, 57.05187, 56.726894, 56.86221, 57.025486, 56.78646, 56.78251, 56.818222, 56.776035, 57.020298, 56.94757, 56.83443, 56.94468, 56.96561, 57.069386, 56.970043, 56.70172, 56.796665, 56.83211, 56.877438, 56.9702, 56.921043, 57.468006, 57.0648, 56.977562, 56.884727, 57.270523, 56.40891, 57.280506, 56.8494, 56.47032, 57.036148, 57.174866, 57.21467, 57.311623, 56.425972, 56.783375, 56.732365, 57.05836, 56.460983, 57.120255, 56.91761, 57.11423, 56.76416, 57.221195, 57.086834, 56.822285, 56.711956, 57.183052, 56.7833, 56.84973, 57.13602, 57.11489, 56.75577, 56.96192, 56.920784, 57.077847, 56.283775, 56.678383, 57.13209, 56.75553, 56.651413, 56.8974, 56.710964, 56.81308, 56.649693, 57.380627, 57.261475, 57.04992, 57.14613, 56.56639, 57.102352, 57.3456, 57.19611, 57.18432, 56.888905, 56.54243, 57.04886, 56.912075, 57.249138, 57.141415, 57.37226, 56.410824, 57.05256, 57.096107, 57.138985, 57.20866, 57.20756, 56.712883, 56.91909, 56.98605, 57.080376, 56.97776, 57.320087, 56.70366, 57.037117, 56.860085, 56.849236, 56.61849, 56.48, 57.029537, 56.409874, 56.882153, 56.57642, 57.06075, 57.08254, 56.886494, 57.098602, 57.14264, 56.995953, 56.967915, 56.69198, 56.77191, 57.395313, 56.953583, 56.992855, 56.74608, 56.94721, 57.164078, 57.11651, 57.110035, 56.877335, 56.686073, 56.75368, 56.096043, 56.883007, 57.00847, 56.901756, 57.176727, 56.69329, 56.473965, 56.78966, 56.446148, 57.113937, 57.004227, 56.70737, 57.01875, 56.911842, 56.339104, 56.32313, 56.60802, 57.033466, 56.812294, 57.2112, 56.996014, 57.03157, 56.848804, 56.867165, 57.08806, 57.191177, 56.942146, 56.80627, 56.820496, 56.003273, 57.09654, 57.118774, 56.790977, 57.112812, 56.706387, 56.657337, 56.43417, 56.22023, 57.015743, 57.174496, 57.00685, 56.909695, 56.82743, 56.78115, 56.76921, 56.79645, 56.99506, 56.729443, 56.72485, 56.967606, 57.068924, 57.12494, 57.11251, 56.491478, 56.949127, 56.66822, 57.0419, 56.73076, 57.028786, 56.950054, 57.20079, 56.787785, 57.120903, 56.68253, 56.86377, 56.714535, 56.709988, 57.123512, 56.591835]\n",
      "features.14.scale pass\n",
      "features.14.zero_point pass\n",
      "features.14.bias pass\n",
      "features.17.weight   0.0036248809192329645   0   4.236720561981201   58\n",
      "[77.27236, 77.57234, 78.17148, 78.0034, 77.57664, 78.079796, 77.40612, 77.75352, 77.859116, 78.04391, 77.39886, 78.25894, 77.90792, 78.15766, 78.23418, 78.07628, 78.134384, 77.95765, 78.070015, 77.71401, 77.7244, 78.62619, 77.95533, 77.07002, 78.002075, 77.97344, 78.196075, 78.06262, 77.92252, 77.90813, 78.114105, 78.477196, 78.12924, 78.122734, 77.91769, 77.545975, 77.93717, 77.95043, 77.72656, 78.04232, 77.96975, 78.05354, 78.17778, 77.88151, 77.408264, 77.922455, 77.815575, 77.82894, 78.157455, 77.59251, 78.0111, 78.20533, 77.959114, 78.19296, 78.250824, 77.99995, 77.35127, 77.900696, 77.76752, 78.06907, 77.67653, 77.861534, 77.11763, 77.66206, 78.160065, 77.7455, 78.28014, 77.07306, 77.62983, 77.7791, 77.60534, 78.02089, 77.92852, 77.886536, 77.56301, 77.8357, 77.94161, 77.977356, 77.86738, 78.209015, 77.63338, 78.19889, 77.99341, 78.02916, 77.91301, 78.14447, 77.98885, 77.90089, 78.14362, 78.12356, 77.65255, 78.10033, 77.2419, 77.595535, 77.51454, 78.33405, 77.674355, 78.07393, 78.11754, 78.04517, 77.73756, 78.34182, 77.94023, 77.822754, 77.85656, 78.179146, 78.318596, 78.14363, 78.05871, 77.99303, 77.79385, 78.12246, 78.219215, 77.034874, 78.10659, 78.36463, 77.45044, 77.87874, 77.92375, 77.82487, 78.04936, 77.722206, 78.174965, 78.07468, 77.782524, 77.22492, 77.94765, 77.70986, 78.29905, 78.19461, 77.94151, 78.42565, 77.90901, 77.767265, 78.366104, 77.648384, 77.6339, 78.30457, 78.230644, 77.70632, 77.86855, 78.1296, 77.591095, 77.71496, 78.231895, 78.03535, 78.27265, 77.44362, 77.85643, 78.338524, 78.32349, 77.61418, 78.54462, 77.27066, 78.35966, 77.92515, 78.06911, 77.88369, 78.072136, 78.023224, 78.005905, 78.13623, 77.782524, 78.37842, 77.8718, 77.991875, 78.096725, 77.730896, 78.25414, 78.297035, 78.24178, 78.11646, 78.16779, 78.40419, 77.99315, 77.78644, 78.02892, 77.91716, 78.27548, 78.072, 78.241684, 77.62808, 78.095085, 77.86557, 77.652, 77.76675, 78.22926, 77.65286, 77.62528, 77.172195, 78.09959, 77.64048, 77.68345, 77.94856, 78.38948, 77.831474, 77.17334, 77.78352, 78.50876, 78.04964, 77.62904, 77.95973, 78.137474, 78.149994, 78.003006, 77.56747, 78.116394, 77.75194, 77.89438, 77.96947, 77.91246, 77.77864, 77.35216, 77.488, 78.212166, 77.30704, 77.74854, 78.01311, 78.25882, 77.829254, 77.48274, 78.1698, 77.69144, 77.87326, 78.20312, 77.92934, 77.58485, 78.486145, 77.799805, 78.14964, 78.03892, 77.77761, 78.00244, 77.78039, 78.25483, 78.285774, 77.802, 77.54506, 77.39707, 78.28659, 77.98776, 78.11849, 78.11909, 77.68362, 78.34801, 77.99182, 77.82202, 77.92337, 77.895706, 77.34985, 77.698555, 77.85651, 78.16264, 78.016945, 78.00441, 78.40263, 77.99121, 77.865036, 77.01467, 78.23942, 78.0292, 77.735794, 78.44488, 77.58071, 78.18793, 78.2498, 77.91411, 77.603195, 77.94016, 78.06238, 78.18622, 77.541504, 77.39476, 77.58954, 77.44293, 77.87471, 77.41541, 78.417625, 78.21639, 77.90744, 77.98677, 78.06937, 77.98155, 78.011185, 78.02915, 78.12754, 77.83504, 77.90302, 77.52924, 77.82472, 77.508705, 78.04748, 77.683556, 77.90039, 78.12176, 78.3237, 77.88158, 78.22071, 77.809074, 77.59368, 77.97974, 77.86626, 77.48439, 78.160805, 76.3959, 77.68398, 78.37801, 77.58838, 77.89506, 77.77294, 78.19804, 77.95542, 78.12067, 78.26412, 77.771095, 77.98889, 78.30899, 78.04183, 77.3662, 77.44571, 78.00005, 77.98647, 77.33688, 78.009186, 78.15273, 77.46266, 77.944336, 78.22822, 78.12444, 77.74679, 77.95655, 78.00304, 77.72595, 78.44917, 78.11284, 77.505974, 77.35867, 77.08621, 77.55881, 77.54814, 77.893265, 77.59285, 77.88258, 78.36862, 78.09771, 77.970116, 78.05041, 77.5262, 78.0431, 77.90862, 78.11398, 77.965, 78.234924, 77.94876, 77.631256, 77.67909, 78.1825, 77.99388, 77.05687, 78.224205, 77.168724, 75.898415, 77.76152, 77.81089, 78.110214, 77.89911, 77.59736, 77.75481, 78.058334, 77.8283, 77.93482, 78.19013, 78.418564, 77.81188, 78.23962, 77.953026, 77.62856, 78.08255, 77.6893, 78.116455, 77.958824, 77.99984, 78.11173, 77.47859, 77.98379, 77.8283, 77.9269, 77.7693, 78.17717, 77.880554, 78.03617, 77.49418, 77.9408, 77.46661, 77.92276, 77.993744, 77.9856, 78.725845, 76.8351, 78.15865, 77.85932, 77.98966, 77.91897, 77.79703, 78.00815, 77.468796, 77.79128, 77.69822, 77.92203, 77.69082, 78.07554, 77.79441, 77.94261, 77.58271, 77.8326, 77.97224, 78.0463, 78.010254, 77.951645, 78.12447, 78.08494, 77.64672, 76.817345, 77.91243, 77.79099, 77.317085, 78.066505, 77.95251, 77.917885, 77.726036, 78.12783, 78.030716, 78.21619, 77.81294, 77.87131, 77.88778, 78.072624, 77.877, 77.95279, 77.859314, 78.11543, 78.002686, 78.1203, 77.802536, 78.60451, 78.49427, 77.72015, 78.02226, 77.22781, 77.63877, 77.718414, 77.492516, 78.26685, 78.252884, 77.772675, 77.39635, 77.69692, 77.82572, 77.37368, 77.04808, 77.948, 77.58306, 78.124344, 78.27297, 77.99378, 77.958435, 77.84295, 77.87273, 78.0462, 78.08814, 77.95018, 78.05893, 77.54472, 78.22758, 77.99681, 78.139206, 77.96278, 77.863266, 77.805214, 78.18327, 78.43381, 78.367714, 78.02796, 78.068794, 77.27465, 78.340904, 77.70297, 78.30301, 78.00261, 77.78002, 78.33808, 77.40153, 78.08055, 77.877686, 78.18474, 78.076, 77.93405, 77.89171, 78.07351, 76.67349, 78.13417, 77.797005, 77.4359, 77.84828, 78.03845, 77.93589, 77.887825, 77.755936, 77.67986, 77.91369, 78.16158, 78.06165]\n",
      "features.17.scale pass\n",
      "features.17.zero_point pass\n",
      "features.17.bias pass\n",
      "features.19.weight   0.002142609329894185   0   3.1934516429901123   79\n",
      "[76.70069, 76.4861, 76.17292, 76.699745, 76.43595, 76.396996, 76.47662, 76.87776, 76.39702, 76.91673, 76.17445, 76.52011, 76.81162, 75.94709, 76.65796, 76.75156, 76.57547, 76.291626, 76.59229, 76.32185, 76.47418, 76.05023, 76.649124, 76.780655, 76.391136, 76.27301, 76.554115, 76.34907, 76.073044, 77.68062, 76.63415, 77.0554, 76.37769, 76.6162, 76.83924, 76.37368, 76.67068, 76.21496, 76.864586, 76.219444, 76.82989, 76.94987, 76.73149, 75.9869, 76.50199, 76.658104, 76.624245, 76.629196, 76.42986, 76.78718, 76.64188, 76.892166, 76.417534, 76.81515, 76.81544, 76.64033, 75.432816, 76.15006, 76.56922, 76.69813, 76.596596, 76.14619, 76.67236, 76.95005, 76.16793, 75.86105, 76.480804, 76.446236, 77.183525, 75.87117, 76.51053, 76.591385, 76.58314, 76.27381, 76.42216, 76.53376, 75.83078, 77.08173, 76.29856, 76.872604, 76.38104, 76.18913, 76.518936, 76.423744, 76.48228, 76.12111, 76.29958, 75.938545, 76.796295, 76.29589, 76.86978, 76.1066, 76.68481, 76.3744, 76.2207, 76.31661, 76.2451, 76.30755, 76.44496, 75.78201, 75.99034, 76.2377, 76.32817, 76.32975, 76.22618, 76.309586, 76.488174, 76.1068, 76.7773, 75.88784, 76.90681, 76.83976, 76.675095, 76.14309, 76.31358, 76.91029, 76.394775, 76.824165, 76.45657, 76.50084, 76.25427, 75.94383, 76.6834, 76.47948, 77.00842, 76.62735, 75.63635, 76.78969, 76.607346, 76.69784, 77.03288, 77.09489, 76.31707, 76.08695, 76.64612, 76.87694, 76.37053, 75.77894, 76.47054, 76.86258, 75.61695, 75.68524, 76.865906, 76.95404, 76.59864, 75.881355, 76.44801, 76.46605, 76.827126, 76.57232, 76.82756, 76.31158, 76.50459, 77.019775, 76.399536, 76.31223, 76.0078, 76.72246, 76.47497, 77.25873, 76.419464, 75.382935, 76.65725, 76.58754, 76.56299, 76.430664, 75.19681, 76.891426, 76.53763, 76.64502, 76.23591, 76.440544, 76.4427, 75.17828, 75.211006, 76.72815, 75.4481, 76.190994, 76.63205, 76.71939, 76.32537, 76.127144, 76.28592, 76.52611, 76.25671, 76.49724, 76.23035, 76.86616, 75.961235, 76.52079, 76.78411, 75.97287, 76.99883, 76.52272, 76.06412, 76.36973, 76.26782, 76.6409, 76.27759, 76.69417, 77.05718, 76.404366, 75.978424, 75.95678, 76.320946, 76.10559, 76.8643, 76.58068, 76.84244, 77.1565, 76.571075, 76.05112, 75.899315, 76.536385, 77.0472, 76.84451, 76.00978, 76.27765, 76.48732, 77.57323, 76.987335, 76.48376, 76.81399, 76.51019, 76.19318, 76.91625, 75.86, 76.432144, 76.7494, 76.94585, 76.27324, 76.51081, 76.99426, 76.34228, 76.70891, 76.32461, 76.550865, 76.79177, 76.81804, 75.90782, 76.23628, 76.3977, 76.64745, 76.47176, 76.132774, 76.51099, 76.3135, 76.48936, 76.86331, 76.82418, 76.809364, 76.93142, 76.8287, 76.66879, 76.212265, 75.8734, 75.58865, 76.84775, 76.54463, 75.99932, 76.787285, 76.6819, 76.36173, 76.20645, 76.79456, 76.988174, 76.37069, 76.47192, 76.58738, 76.3325, 76.692184, 76.51319, 76.649345, 77.0846, 76.7005, 76.30569, 76.47755, 76.77862, 76.373634, 76.59307, 77.05416, 76.51451, 76.70629, 76.23638, 76.31804, 76.75381, 75.80168, 76.39622, 76.81096, 76.22099, 76.3695, 76.34047, 76.63804, 76.64601, 76.24884, 76.83394, 76.1405, 76.53416, 76.83431, 76.647354, 76.74889, 76.54156, 76.898186, 76.49466, 76.7992, 76.41452, 76.3973, 76.60993, 76.95428, 76.65237, 76.54041, 76.509766, 76.10414, 76.52705, 76.60507, 76.11551, 76.09022, 76.70353, 76.69797, 76.6225, 76.541595, 76.838104, 76.48211, 76.629875, 76.799866, 76.594215, 77.05607, 76.41528, 76.347336, 76.72397, 76.53799, 76.64863, 76.56155, 76.693115, 76.43369, 75.82587, 76.54614, 76.76836, 77.093636, 77.120964, 76.786415, 76.37091, 76.77506, 76.02011, 76.28969, 76.00306, 75.61124, 76.570145, 76.53844, 76.17701, 76.82921, 76.88838, 76.532196, 76.38085, 76.428505, 76.79838, 76.19624, 75.76031, 75.98085, 76.424095, 76.27921, 76.7621, 76.03535, 76.86377, 75.93652, 76.49442, 77.16798, 76.91946, 76.96368, 76.39459, 76.30739, 76.781715, 76.58506, 76.17843, 76.115295, 76.38982, 76.64322, 76.70065, 76.05808, 76.77183, 75.504875, 76.80088, 76.13834, 76.43655, 77.160286, 76.16235, 76.19797, 76.57323, 76.25727, 76.660545, 77.2554, 76.37131, 76.3215, 75.190025, 76.0675, 75.82475, 76.531204, 76.466774, 76.59807, 76.708046, 76.62168, 76.50801, 76.704315, 76.278244, 76.599266, 76.20385, 76.47847, 76.28609, 76.31601, 76.307045, 77.088936, 76.15843, 77.00938, 76.69329, 76.17598, 76.892426, 76.4304, 76.06591, 76.68425, 76.70007, 76.49175, 77.05071, 76.182686, 76.42806, 76.55921, 76.390076, 77.002144, 76.71801, 76.881516, 76.04935, 76.426384, 76.37769, 76.828606, 76.75262, 76.887764, 76.39272, 76.242195, 76.66335, 76.41632, 76.364586, 76.613, 76.34116, 76.540436, 76.802605, 75.334274, 77.0213, 76.83462, 76.36277, 76.64829, 76.51021, 76.89, 76.22207, 76.28249, 76.71099, 76.7995, 75.95345, 76.147644, 76.357124, 76.51668, 76.97322, 76.05898, 76.18939, 76.6966, 76.761406, 77.25329, 76.13905, 77.0622, 76.75722, 76.84599, 76.49439, 76.14478, 76.884415, 76.40684, 76.17532, 76.24986, 76.33296, 76.70226, 76.886475, 76.43575, 75.9398, 77.122505, 76.49299, 76.41562, 76.40919, 76.478065, 76.70301, 76.74931, 76.82781, 76.63546, 76.878006, 76.68492, 76.50097, 76.57036, 76.98314, 76.78459, 76.45474, 75.92435, 76.71653, 75.69414, 76.30191, 76.16352, 76.756966, 77.06916, 76.550995, 76.996254, 77.03959, 76.052216, 76.28759, 76.487495, 77.102486, 76.81907, 76.74008]\n",
      "features.19.scale pass\n",
      "features.19.zero_point pass\n",
      "features.19.bias pass\n",
      "features.21.weight   0.0017746191006153822   0   2.667638063430786   78\n",
      "[55.754826, 55.89275, 56.28778, 56.207714, 56.576874, 56.02085, 56.282345, 55.970222, 55.792797, 56.055416, 55.718887, 56.422417, 55.977966, 55.790527, 55.707127, 55.966198, 56.157074, 56.436283, 56.14425, 55.971123, 56.32196, 55.95281, 56.228672, 55.928505, 56.082672, 56.211315, 56.057858, 56.056324, 56.214973, 55.818718, 56.503407, 56.384052, 56.185116, 56.030075, 56.81885, 55.754417, 56.20514, 55.993183, 55.54231, 56.180706, 56.199684, 56.279617, 56.16948, 56.32193, 55.84141, 55.819546, 56.127235, 56.173603, 56.34487, 56.284286, 56.18279, 56.086197, 56.56506, 56.097954, 56.054184, 56.184128, 55.928253, 56.042496, 55.81723, 56.16493, 55.74974, 56.43014, 55.581463, 56.40612, 56.366707, 56.115322, 55.78843, 56.131744, 56.012108, 56.312096, 56.547405, 56.221794, 55.509644, 56.177048, 56.264904, 56.32, 56.094196, 55.972595, 56.0155, 56.09975, 56.472454, 56.312008, 55.898197, 56.085007, 56.288635, 56.409393, 56.26905, 56.178116, 56.596504, 55.865746, 56.016663, 55.48188, 55.814037, 56.769135, 56.007874, 56.36342, 56.040726, 56.423656, 55.64914, 56.253284, 56.03663, 56.212814, 56.270504, 55.956947, 56.74948, 56.008884, 56.410103, 56.290176, 56.49877, 56.14734, 56.37131, 55.88335, 56.381397, 55.97339, 56.122887, 56.442913, 56.34051, 56.02556, 55.861755, 56.345894, 56.2195, 56.348927, 56.343292, 54.979404, 56.251427, 56.442684, 55.70969, 55.992073, 55.8241, 56.099644, 55.645744, 55.338177, 55.86992, 56.312973, 56.104362, 56.302803, 55.998722, 56.31017, 55.86908, 56.1897, 56.417656, 55.79343, 56.008015, 56.09412, 56.002197, 55.456726, 55.206207, 56.00123, 55.814053, 56.528168, 56.445763, 56.561096, 56.527065, 56.820316, 55.87731, 56.377533, 56.113, 56.496193, 56.404076, 55.938896, 55.94589, 56.550102, 56.283516, 56.38894, 56.314163, 55.8788, 56.111443, 56.138702, 56.530594, 55.707035, 56.292656, 56.06478, 56.102406, 56.117027, 56.077347, 55.491314, 56.50502, 56.328957, 56.19947, 56.762203, 56.525917, 56.22293, 55.52382, 56.12551, 56.451923, 56.24901, 56.680904, 55.892334, 56.890667, 56.415085, 56.714596, 56.12147, 56.55155, 55.70907, 56.551765, 56.639652, 55.920967, 55.888348, 55.98856, 55.55764, 56.038418, 55.960247, 55.929092, 56.608242, 56.289474, 55.504288, 55.837093, 56.332775, 56.312847, 56.058704, 56.18423, 55.62033, 56.23561, 56.168102, 56.0752, 56.53832, 55.876186, 56.303078, 56.49389, 56.06217, 55.61765, 56.46486, 56.197742, 56.279827, 56.073956, 55.936604, 56.004875, 56.273773, 56.320965, 56.339127, 55.810608, 55.905052, 55.913666, 55.856148, 56.00828, 55.722282, 56.40385, 55.991608, 56.49718, 55.644226, 55.759865, 56.030376, 56.45873, 55.957634, 55.962067, 56.147663, 55.795357, 55.81078, 55.916275, 55.971363, 56.128418, 55.943367, 55.835377, 55.824196, 55.857338, 55.789116, 55.73862, 56.333538, 56.288418, 56.241177, 56.508015, 55.8669, 55.719273, 56.26574, 55.90654, 55.784405, 55.820026, 55.993557, 56.376717, 56.238255, 56.335888, 55.851692, 56.230824, 56.0046, 56.221527, 55.955387, 56.47574, 56.16474, 56.092598, 56.105934, 55.768913, 55.67611, 56.2622, 55.75899, 56.051178, 56.181175, 56.00418, 56.14133, 56.09985, 56.425457, 55.759842, 55.945187, 56.136158, 56.086235, 56.11553, 56.546124, 56.27955, 56.070312, 56.45703, 56.392406, 55.741226, 55.841663, 55.819088, 56.057575, 56.303543, 56.761147, 56.459305, 56.519325, 56.31169, 55.75208, 56.116177, 56.135857, 55.63018, 56.12702, 55.661423, 56.41217, 56.69606, 55.94203, 56.328896, 56.208904, 55.9682, 56.312504, 55.938698, 55.571465, 56.03955, 55.709724, 56.18068, 55.48052, 56.132294, 55.915802, 55.76938, 55.49603, 55.898365, 55.878323, 56.000095, 55.40296, 56.22189, 56.3818, 56.54628, 55.842865, 56.21612, 55.80398, 56.232807, 56.290417, 55.95968, 56.101048, 56.17334, 56.306747, 55.975548, 56.20148, 56.278687, 56.655525, 56.010593, 56.25566, 56.378468, 56.273235, 56.177383, 56.09793, 55.77357, 56.107548, 55.696198, 55.97462, 56.225613, 56.459297, 55.99721, 56.190502, 55.977345, 55.47911, 55.932106, 55.810295, 56.69034, 56.141613, 56.130356, 56.013298, 56.00652, 56.25556, 56.14914, 56.343082, 55.662754, 55.9196, 56.243454, 56.344715, 56.27013, 55.57444, 56.16178, 55.46145, 55.819027, 55.415005, 56.267803, 55.72136, 55.993828, 55.953384, 56.57141, 55.99823, 56.240623, 56.178967, 56.503613, 55.85737, 56.143578, 56.25373, 56.170258, 55.696804, 56.05922, 56.134632, 55.969128, 55.956154, 56.11537, 56.075775, 56.123505, 56.125145, 56.01321, 55.619476, 56.476826, 56.140285, 56.005993, 56.17211, 55.645573, 56.212185, 56.477776, 56.098934, 56.21674, 56.148254, 56.444717, 55.840565, 55.924168, 56.1931, 56.72627, 56.06673, 55.491833, 56.211185, 56.326996, 56.05891, 56.03717, 56.238003, 56.58405, 55.844044, 55.47369, 56.260498, 56.475, 56.395065, 56.479767, 56.35288, 56.76005, 56.1474, 56.558952, 56.263195, 56.177372, 56.024254, 55.892548, 56.347954, 56.041237, 55.86809, 56.09131, 56.30987, 55.793602, 56.341278, 55.634014, 56.42192, 56.043213, 56.345284, 56.138985, 56.104347, 56.287075, 56.163982, 56.309895, 56.221867, 56.210655, 56.363434, 56.226604, 56.30482, 56.44335, 56.339813, 55.983612, 55.85664, 55.677773, 56.200092, 56.110798, 55.977234, 55.544647, 56.45394, 55.850937, 55.973232, 56.00402, 55.901466, 56.13417, 56.399364, 56.001076, 56.139908, 56.356304, 56.43065, 55.822784, 56.111244, 56.94977, 55.59038, 55.4217, 56.061905, 55.34807, 55.743702, 56.561863, 56.041912, 56.016838, 56.22756, 56.426594, 55.55089, 56.33971, 56.279747, 55.606846, 56.1561, 55.54056, 56.12868, 56.28449, 56.008125]\n",
      "features.21.scale pass\n",
      "features.21.zero_point pass\n",
      "features.21.bias pass\n",
      "features.24.weight   0.002479980466887355   0   2.0082199573516846   58\n",
      "[68.99192, 69.14535, 68.83269, 69.15641, 69.0218, 69.15164, 68.844345, 68.89041, 69.091705, 69.29977, 68.873764, 69.23243, 69.19359, 68.9545, 68.58718, 68.251884, 69.04052, 69.1718, 69.03969, 68.48913, 69.07411, 68.69977, 69.02858, 69.05943, 69.275604, 69.27575, 69.534195, 69.11081, 68.90409, 68.55366, 69.17862, 69.08493, 69.097374, 68.95887, 68.968666, 68.86065, 69.28548, 69.24908, 69.19614, 69.31501, 69.22495, 68.85994, 69.08896, 69.16764, 69.181526, 68.51503, 69.12161, 68.88688, 68.98524, 68.75371, 69.049225, 69.29187, 68.84977, 69.202515, 68.83929, 69.10227, 69.08726, 68.84164, 68.91977, 69.2234, 68.83345, 68.846695, 68.98015, 69.051765, 68.72387, 69.03738, 69.34731, 68.72332, 69.308235, 69.10583, 68.90942, 68.66782, 68.806145, 69.20868, 69.303535, 68.89017, 69.059906, 69.35382, 68.83923, 69.12151, 69.284775, 69.31114, 69.145294, 69.18399, 69.105095, 69.213104, 68.91723, 69.36957, 69.17308, 69.0731, 69.31481, 68.94745, 68.92276, 68.93459, 68.84238, 69.29379, 68.91104, 68.718895, 69.17134, 69.05408, 69.10919, 69.00631, 69.03826, 69.01044, 69.4311, 69.01823, 68.88035, 69.00704, 69.203865, 69.088684, 69.10341, 69.30447, 69.45393, 68.71684, 69.510376, 68.9281, 68.94079, 68.8911, 68.76978, 68.832405, 69.27704, 69.213875, 68.9376, 68.840775, 69.02395, 69.04511, 68.82707, 69.422615, 68.811195, 69.21512, 68.886086, 69.19737, 69.17904, 68.920494, 69.202415, 69.16899, 69.25205, 69.17935, 68.95063, 69.120705, 69.19815, 69.069115, 68.97755, 68.5958, 69.238976, 68.90011, 69.42105, 69.20913, 69.510506, 68.96423, 68.35502, 69.10541, 69.099724, 68.79538, 68.96508, 69.547005, 68.89768, 69.23199, 69.20537, 68.92109, 69.156494, 69.09833, 69.12221, 69.14769, 69.37936, 69.107155, 69.048935, 69.12407, 68.852066, 69.133575, 68.85993, 69.11799, 69.11613, 69.265495, 68.87641, 69.217575, 69.04882, 68.7874, 69.02312, 69.16094, 69.24626, 69.24258, 68.8214, 68.96415, 68.7896, 69.23691, 69.32881, 68.90563, 69.11171, 68.597404, 68.66502, 69.08536, 69.067726, 68.822914, 69.0258, 68.89261, 69.1519, 69.12617, 69.21829, 68.14882, 69.0585, 69.30559, 69.135864, 69.31383, 69.065056, 68.74216, 69.00379, 69.18494, 68.70627, 68.90374, 69.47251, 68.98283, 69.04381, 69.09053, 68.87683, 68.993484, 69.253395, 68.8657, 69.28719, 69.10215, 69.28128, 69.489655, 69.13921, 69.221794, 68.933266, 69.15603, 69.40402, 69.55447, 69.2315, 68.68816, 68.87457, 68.9434, 68.68394, 69.24483, 68.96359, 68.527855, 69.24729, 69.0138, 68.83614, 69.36077, 68.97094, 69.154526, 69.07396, 69.03833, 69.01112, 68.78605, 68.80585, 69.064735, 69.251686, 68.97274, 69.168846, 68.75764, 69.042885, 69.110634, 69.30225, 69.25756, 69.337296, 69.07423, 69.304886, 69.02637, 68.82246, 69.349556, 69.20697, 68.6643, 68.96405, 69.14802, 68.99218, 69.09482, 68.30386, 69.3443, 68.902115, 68.66407, 69.020546, 69.22356, 69.40915, 69.0857, 68.92859, 69.361176, 68.84189, 68.98801, 69.004944, 69.08391, 68.62981, 68.76825, 69.20669, 69.25935, 69.3512, 69.33223, 68.85325, 69.04635, 69.11865, 69.03458, 69.32293, 69.14338, 69.023605, 68.87713, 69.16476, 69.25706, 69.0738, 68.643074, 68.97645, 69.09357, 69.09573, 69.202484, 69.08933, 68.890366, 69.065254, 69.44783, 69.31363, 69.04448, 69.29254, 68.98581, 69.36248, 69.0516, 69.057655, 69.11587, 69.09375, 69.37251, 68.44886, 68.62354, 68.92984, 69.39331, 69.31362, 69.193634, 68.93708, 69.03393, 68.94806, 69.373856, 69.12717, 69.02708, 69.22909, 68.87312, 69.13651, 68.96277, 68.978905, 69.34808, 69.04126, 69.196205, 68.819145, 68.82641, 68.42745, 69.49403, 68.72911, 69.35236, 69.18371, 69.080826, 69.47, 68.78935, 68.96361, 69.03089, 69.25637, 69.01206, 69.13633, 69.269806, 69.16523, 68.74572, 69.21249, 69.21097, 69.10671, 69.40511, 68.326324, 69.26513, 69.36515, 68.98794, 68.891, 68.95504, 69.11797, 68.870834, 68.818054, 69.31842, 68.94156, 68.98629, 69.33038, 69.15498, 69.14951, 68.90205, 69.36038, 69.204025, 69.338875, 69.31953, 69.23668, 69.29211, 68.837265, 69.3014, 69.11332, 68.89137, 68.74288, 68.988304, 68.88834, 68.96851, 69.364174, 68.34762, 69.15656, 68.828705, 69.24354, 69.00204, 69.16533, 69.088036, 69.30179, 69.23305, 69.01821, 69.09387, 68.69427, 68.77099, 69.24332, 69.06251, 68.76193, 69.027626, 69.380264, 68.94138, 68.67566, 69.234474, 68.7987, 69.3265, 69.067215, 69.259834, 68.930984, 68.887924, 68.971466, 69.29333, 69.052216, 69.36119, 69.23222, 69.22335, 69.13754, 69.123535, 69.086586, 69.05144, 68.77255, 69.24721, 69.169205, 69.322105, 68.90466, 69.1789, 69.15937, 69.2123, 68.99275, 69.387695, 69.192375, 69.267944, 69.40696, 69.37844, 68.95031, 68.80843, 69.09371, 69.05747, 69.067924, 69.32939, 69.22568, 69.0147, 68.84844, 69.028076, 69.11653, 69.142876, 69.01238, 69.110825, 69.41422, 68.74457, 69.47212, 69.22675, 69.10315, 69.19725, 69.282394, 68.562805, 69.166695, 68.79842, 69.379456, 68.91297, 68.931564, 68.884544, 68.989075, 69.1937, 69.017525, 69.04007, 68.710236, 69.40494, 69.04897, 69.36844, 69.23145, 68.94194, 69.21191, 69.12652, 68.6634, 69.18269, 68.836555, 69.14292, 69.29356, 69.28929, 69.07395, 68.52416, 68.94929, 69.375145, 69.41267, 69.10636, 69.344765, 68.883934, 69.15763, 69.12196, 68.87836, 69.46312, 69.101234, 68.88527, 69.067314, 69.363884, 69.050766, 69.07986, 68.35669, 69.49913, 68.77699, 69.16486, 69.15924, 69.06766]\n",
      "features.24.scale pass\n",
      "features.24.zero_point pass\n",
      "features.24.bias pass\n",
      "features.26.weight   0.0019614389166235924   0   2.2284514904022217   70\n",
      "[69.591866, 69.66651, 70.633156, 69.57984, 70.3515, 69.89445, 70.312744, 70.01606, 69.97709, 70.12034, 69.80946, 70.71659, 70.63447, 69.39789, 70.53093, 70.30995, 70.26551, 70.1436, 70.20857, 70.16352, 69.91157, 70.53428, 70.337105, 70.40694, 70.51587, 69.3434, 69.94515, 69.311554, 70.03048, 70.20889, 69.73472, 69.90973, 70.17375, 69.95646, 70.2733, 70.04596, 71.39427, 70.15631, 70.05495, 70.03962, 70.145226, 69.7772, 70.427086, 70.55009, 70.064285, 70.22935, 70.55016, 70.6954, 69.91562, 70.42997, 70.09261, 70.25734, 70.566055, 69.90598, 69.01445, 70.24005, 70.09757, 69.41661, 69.88129, 69.662445, 70.2997, 70.59943, 70.48487, 69.99888, 69.886055, 69.94411, 70.821686, 70.82719, 69.02118, 70.090706, 69.9964, 70.44069, 70.253365, 70.15659, 69.8905, 70.402245, 70.21507, 69.83485, 69.913605, 69.78718, 70.58828, 70.335915, 69.55709, 69.92154, 70.34448, 70.21739, 70.139336, 70.15328, 70.36728, 70.337425, 69.92168, 70.04389, 69.85918, 69.868996, 69.03369, 70.0858, 70.908, 70.37084, 70.30792, 70.16524, 69.812096, 70.38904, 69.74322, 70.550674, 69.24761, 70.03629, 70.46279, 69.8662, 70.55028, 69.89691, 70.19406, 70.024925, 70.59133, 69.875175, 69.85298, 69.60213, 69.86696, 70.34191, 70.34236, 70.159836, 70.674805, 70.497696, 69.5681, 69.78039, 69.923065, 70.320465, 69.853584, 70.253944, 69.31814, 69.87887, 70.0362, 70.04615, 69.83812, 69.53266, 69.551605, 69.91883, 70.60948, 69.78891, 70.172, 69.803444, 70.12543, 69.95978, 69.511826, 70.18958, 70.06964, 69.281555, 69.44264, 70.25362, 69.63066, 69.41452, 69.56453, 69.83145, 70.30902, 70.32947, 69.95056, 70.30194, 70.22587, 69.76586, 70.03443, 69.911385, 70.09856, 71.105194, 70.33449, 69.85803, 70.28007, 69.958664, 70.316574, 70.38317, 69.84536, 70.00749, 69.04166, 69.879974, 70.10903, 69.74612, 69.810455, 70.07269, 69.93036, 69.694534, 69.97572, 70.21759, 70.07894, 70.07201, 69.99501, 70.65118, 69.4519, 70.25846, 70.472725, 70.02528, 69.57977, 69.22457, 70.14612, 69.31083, 69.8717, 70.598495, 70.322426, 69.85609, 69.84439, 69.74186, 70.35559, 69.901566, 69.8912, 70.9277, 70.471245, 70.32961, 70.273224, 70.315834, 70.15852, 70.82261, 69.83438, 70.147026, 69.6666, 70.45703, 70.53863, 69.945564, 70.126076, 69.73287, 70.147835, 69.795074, 71.62111, 69.90348, 69.104126, 70.260185, 69.567154, 70.39566, 70.560394, 70.43521, 70.277336, 70.642235, 70.18782, 70.55039, 69.84189, 70.00568, 69.92386, 70.55582, 69.807945, 70.44391, 70.108604, 70.28111, 70.0559, 70.24249, 69.863815, 70.600746, 70.080154, 70.66094, 69.91698, 70.0812, 70.61978, 70.73684, 70.783066, 69.67029, 70.05176, 70.37824, 70.503075, 69.90474, 70.11666, 70.11579, 69.73822, 69.94925, 70.47563, 69.908615, 70.552155, 69.900894, 69.21605, 70.0257, 69.852264, 70.21087, 70.06411, 70.65931, 69.71903, 69.49886, 70.25638, 70.185326, 70.26648, 70.54519, 70.30535, 70.32381, 70.06881, 70.154945, 69.59435, 70.36147, 70.6793, 69.66869, 69.99704, 70.71108, 70.61948, 70.046715, 69.7862, 70.244255, 70.48971, 70.44437, 70.728035, 69.86329, 70.35538, 69.79209, 70.55629, 70.26724, 69.86925, 70.06465, 70.21341, 69.97301, 70.31925, 70.30132, 70.17776, 70.13505, 70.40032, 68.41033, 70.00547, 69.8414, 69.89408, 70.277855, 70.41003, 69.42422, 70.33779, 70.35221, 70.311226, 69.861885, 69.847855, 69.75653, 70.43718, 70.387024, 69.65693, 70.85442, 70.55998, 69.89273, 70.15812, 70.209175, 70.65567, 69.6016, 69.98046, 70.57879, 70.30719, 70.6691, 69.5595, 70.17271, 69.790115, 69.95166, 69.31761, 70.58175, 69.740425, 70.70363, 70.34715, 69.69595, 69.99858, 70.03318, 69.92363, 70.156624, 70.145454, 70.11227, 70.380165, 70.33442, 70.20881, 70.2298, 69.130104, 70.58192, 69.64349, 69.35365, 70.34195, 69.85851, 70.112564, 70.19238, 69.51963, 70.344086, 70.62847, 70.29951, 70.55182, 70.0082, 68.92956, 69.70695, 69.730934, 70.29478, 70.08627, 69.66258, 69.56368, 70.3598, 69.59809, 69.72524, 70.58151, 70.1748, 70.67364, 69.980835, 70.7019, 69.68313, 70.32557, 70.20374, 70.20106, 69.53212, 70.18704, 70.23642, 70.25994, 69.776726, 68.744484, 70.02306, 69.61057, 70.38873, 69.73372, 69.94845, 69.87775, 69.648186, 69.85926, 70.8655, 70.50585, 69.68395, 70.15467, 70.269424, 70.73339, 70.51732, 69.919304, 70.334076, 70.23755, 70.75482, 69.75098, 70.40303, 69.903145, 70.0856, 70.00516, 70.06703, 70.02613, 70.56649, 70.13415, 69.93882, 69.58502, 70.21198, 70.320114, 69.91396, 69.621414, 70.649025, 70.508606, 70.72063, 69.95436, 69.63897, 70.33739, 70.35116, 70.278244, 70.00698, 70.135345, 70.46586, 69.57475, 69.96658, 70.46162, 69.70899, 69.75797, 70.329094, 70.30884, 71.21489, 69.77595, 69.59927, 70.558655, 70.1488, 70.92431, 70.69671, 70.42296, 69.869675, 69.94439, 70.161674, 70.52303, 69.30105, 69.05536, 69.81023, 70.37431, 70.18719, 70.27847, 69.93013, 69.39915, 69.958275, 69.57278, 70.24592, 70.24897, 69.4383, 69.46639, 70.29491, 69.75333, 69.83276, 70.301285, 69.54671, 70.04979, 69.74947, 69.59439, 70.35374, 70.174416, 69.745346, 69.59518, 70.30228, 69.672134, 70.57828, 69.684425, 69.84759, 69.723915, 69.821686, 70.89239, 70.10126, 69.55781, 70.54791, 70.62947, 69.86399, 70.15788, 69.34951, 70.124825, 70.046234, 70.02935, 70.98921, 69.886215, 68.86615, 70.68475, 70.22371, 70.016235, 70.40062, 69.589874, 69.783775, 69.87571, 70.019135, 70.17331, 72.560524]\n",
      "features.26.scale pass\n",
      "features.26.zero_point pass\n",
      "features.26.bias pass\n",
      "features.28.weight   0.0010372619144618511   0   1.3619482517242432   72\n",
      "[78.34313, 78.48547, 78.42003, 77.6272, 77.885216, 77.725, 77.75926, 78.10393, 77.708595, 78.22307, 78.01862, 78.76998, 78.35646, 77.62898, 78.60894, 78.19249, 77.95823, 78.176025, 77.967384, 78.167206, 78.17825, 78.98314, 77.58706, 78.406044, 77.60781, 77.212524, 77.276184, 78.091415, 77.944565, 77.45342, 78.02536, 77.78832, 77.835205, 78.478, 78.04521, 78.22809, 76.89606, 78.178925, 78.08803, 78.05915, 77.48583, 78.165054, 78.15759, 78.0492, 77.61807, 78.30036, 78.48917, 78.06754, 78.25465, 77.806564, 78.37371, 77.725365, 79.72956, 78.51606, 78.60431, 77.88721, 78.24913, 78.367905, 78.0373, 77.94954, 77.74306, 78.08535, 77.83844, 78.92245, 78.28585, 77.636185, 78.320526, 78.23715, 78.20225, 77.88695, 78.261055, 77.64803, 78.93576, 78.109886, 78.238495, 78.02587, 78.23449, 78.31013, 78.0115, 77.92205, 78.277054, 78.21005, 77.81294, 77.93057, 78.54422, 77.93013, 78.19187, 78.31357, 77.93127, 78.709335, 77.89219, 78.651306, 78.22969, 78.04068, 77.75748, 77.970924, 77.69619, 78.095535, 78.07463, 77.60064, 78.52834, 78.05252, 77.865, 78.3708, 78.1266, 78.3164, 78.239845, 77.11443, 77.59778, 78.69668, 78.060974, 77.2819, 78.0741, 77.86096, 77.38547, 78.04721, 77.78527, 78.54009, 77.53213, 78.7302, 78.30509, 78.15818, 78.066376, 77.80138, 78.33821, 77.58192, 78.19064, 78.1815, 78.19536, 77.92467, 78.45389, 78.4654, 78.294846, 78.078255, 77.2065, 78.56032, 78.00866, 78.5714, 77.80096, 78.04107, 78.08934, 78.63333, 78.41606, 78.73555, 78.25748, 77.54474, 77.72036, 77.8752, 78.349205, 78.372574, 78.83115, 77.74351, 78.14635, 78.7013, 78.148865, 77.897804, 77.45693, 78.23834, 77.98948, 78.545166, 78.03168, 77.97905, 78.23173, 78.2647, 78.15879, 78.89117, 79.67175, 78.38273, 78.88695, 78.765915, 78.91651, 78.53424, 77.891685, 77.85117, 78.07836, 78.22919, 77.675095, 78.26502, 78.636734, 78.65256, 78.354294, 78.423805, 78.22591, 77.8667, 77.58816, 77.63973, 77.70399, 78.04835, 78.29042, 78.688484, 78.26411, 78.65632, 78.774925, 77.83077, 78.900406, 78.35382, 78.34109, 78.37049, 77.45239, 78.00865, 77.80151, 77.907005, 78.275826, 79.44491, 77.254745, 77.98946, 78.946266, 78.44468, 78.169174, 78.122215, 78.44461, 78.23725, 77.38828, 78.36622, 78.02975, 78.97034, 77.607285, 77.53531, 78.278244, 78.29374, 77.88062, 78.47523, 77.87549, 76.92839, 78.27097, 78.62753, 78.23312, 77.99109, 77.795006, 77.95662, 77.90939, 78.27617, 78.09664, 78.13462, 77.9972, 78.42613, 78.74626, 78.45862, 78.17334, 78.07443, 78.231926, 78.82809, 77.91915, 77.98239, 78.52628, 77.97975, 78.489296, 78.65443, 77.71276, 78.155624, 78.16747, 78.37729, 78.28826, 79.0759, 77.9077, 78.0989, 77.48358, 78.3983, 78.03229, 77.25777, 77.74546, 78.66176, 78.10815, 78.22421, 78.614174, 78.291084, 78.54701, 78.29186, 77.795845, 77.91565, 78.506516, 77.889366, 78.13105, 78.75297, 77.46943, 78.01398, 77.908806, 78.12578, 77.86178, 78.21474, 78.35233, 78.162964, 78.25101, 78.287735, 78.69094, 78.36532, 78.78806, 78.36191, 77.87412, 77.627, 77.766945, 77.88326, 78.18492, 78.065865, 78.0203, 78.09006, 77.79203, 77.77612, 78.46957, 77.259544, 77.70002, 78.3702, 77.80898, 78.16722, 78.169365, 78.59076, 78.69676, 78.37991, 78.07491, 78.45483, 78.32089, 78.16089, 77.479256, 78.086876, 78.14745, 78.113754, 78.17978, 77.70761, 77.96369, 77.84898, 77.55256, 78.54412, 77.79355, 78.19393, 78.15685, 77.3461, 77.68425, 77.93189, 77.67699, 77.5674, 78.052704, 77.7912, 77.26022, 77.649376, 77.48289, 78.15955, 78.89415, 79.64098, 77.88239, 78.48969, 77.45898, 78.042244, 78.55654, 77.97037, 77.99754, 77.86275, 78.494865, 77.72432, 78.619995, 78.70474, 78.259285, 78.80196, 78.29287, 77.017685, 78.445045, 79.18447, 78.432846, 78.65586, 78.07844, 78.50499, 78.04656, 78.43213, 78.25009, 78.47715, 78.46884, 78.22365, 78.523865, 77.89472, 78.04098, 79.0538, 78.425575, 77.11204, 78.35992, 77.966774, 77.67437, 77.99697, 77.97774, 78.282974, 77.703354, 78.418045, 78.0017, 78.21872, 77.81633, 77.692345, 77.82362, 78.189156, 78.57866, 77.767105, 77.930855, 78.028984, 78.528694, 78.292114, 77.695694, 77.51887, 78.416626, 77.578, 78.94944, 78.732056, 78.21204, 77.95402, 77.834946, 78.15618, 78.22992, 78.2057, 78.37301, 77.75503, 78.06797, 78.25601, 78.02334, 78.06847, 78.3603, 77.38572, 78.426186, 77.54284, 77.96279, 77.993195, 77.657906, 77.689415, 78.050285, 78.96769, 78.31941, 77.7496, 77.93914, 78.791435, 78.06838, 77.94161, 78.267395, 78.83697, 77.996666, 78.7188, 78.05941, 78.00665, 77.92399, 78.26362, 78.42078, 77.9135, 77.648346, 77.73978, 77.77689, 78.338066, 78.34524, 77.9734, 78.16797, 77.59395, 78.591225, 78.5487, 77.59664, 78.14617, 77.98494, 78.499626, 78.681305, 78.295525, 78.05879, 78.84045, 77.76723, 77.7382, 78.094955, 78.253624, 78.268974, 77.888756, 77.93228, 78.36721, 78.50544, 77.35069, 77.94904, 78.55405, 78.48928, 78.392654, 78.80112, 78.24465, 78.298355, 78.96857, 78.34435, 78.77698, 77.80309, 77.89017, 77.662445, 77.9613, 77.84236, 78.620056, 78.6642, 77.621956, 77.971886, 78.14683, 78.03126, 78.25041, 77.971275, 78.32189, 77.95638, 78.03567, 78.24057, 77.70889, 78.9981, 78.39556, 77.67712, 78.30826, 77.686005, 79.83608, 77.86215, 78.167534, 78.14235, 78.57073, 78.17058, 78.84043, 78.45616, 77.92186, 76.9039, 79.2687, 78.805855, 77.558365, 78.386086, 77.45821]\n",
      "features.28.scale pass\n",
      "features.28.zero_point pass\n",
      "features.28.bias pass\n",
      "classifier.0.scale pass\n",
      "classifier.0.zero_point pass\n",
      "classifier.0._packed_params.weight pass\n",
      "classifier.0._packed_params.bias pass\n",
      "classifier.0._packed_params.dtype pass\n",
      "classifier.3.scale pass\n",
      "classifier.3.zero_point pass\n",
      "classifier.3._packed_params.weight pass\n",
      "classifier.3._packed_params.bias pass\n",
      "classifier.3._packed_params.dtype pass\n",
      "classifier.6.scale pass\n",
      "classifier.6.zero_point pass\n",
      "classifier.6._packed_params.weight pass\n",
      "classifier.6._packed_params.bias pass\n",
      "classifier.6._packed_params.dtype pass\n",
      "quant.scale pass\n",
      "quant.zero_point pass\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# def getSandZ():\n",
    "scale_chain = dict()\n",
    "prev_scale_layer = None\n",
    "zero_point_chain = dict()\n",
    "prev_zero_point_layer = None\n",
    "# fetch network order\n",
    "for key in myModel.state_dict().keys():\n",
    "    if 'scale' in key:\n",
    "        if prev_scale_layer is not None:\n",
    "            scale_chain[prev_scale_layer] = key\n",
    "        prev_scale_layer = key \n",
    "    elif 'zero_point' in key:\n",
    "        if prev_zero_point_layer is not None:\n",
    "            zero_point_chain[prev_zero_point_layer] = key\n",
    "        prev_zero_point_layer = key\n",
    "    else:\n",
    "        pass\n",
    "# print(scale_chain)\n",
    "# print(zero_point_chain)\n",
    "input_layer_size_list = [(1, 3, 224 + 2, 224 + 2), (1, 64, 224 + 2, 224 + 2), \n",
    "                         (1, 64, 112 + 2, 112 + 2), (1, 128, 112 + 2, 112 + 2), \n",
    "                         (1, 128, 56 + 2, 56 + 2), (1, 256, 56 + 2, 56 + 2), (1, 256, 56 + 2, 56 + 2), \n",
    "                         (1, 256, 28 + 2, 28 + 2), (1, 512, 28 + 2, 28 + 2), (1, 512, 28 + 2, 28 + 2), \n",
    "                         (1, 512, 14 + 2, 14 + 2), (1, 512, 14 + 2, 14 + 2), (1, 512, 14 + 2, 14 + 2)]\n",
    "input_counter = 0\n",
    "P = 7091\n",
    "for key in myModel.state_dict().keys():\n",
    "    if 'features' in key and 'weight' in key:\n",
    "        WeightScaleValue = myModel.state_dict()[key].q_scale()\n",
    "        WeightZeroPointValue = myModel.state_dict()[key].q_zero_point()\n",
    "        InputScaleValue = myModel.state_dict()[f'{key[0:-6]}scale'].item()\n",
    "        InputZeroPointValue = myModel.state_dict()[f'{key[0:-6]}zero_point'].item()\n",
    "        print(key, 'dealing') # , ' ', WeightScaleValue, ' ', WeightZeroPointValue, ' ', InputScaleValue, ' ', InputZeroPointValue)\n",
    "        assert(WeightZeroPointValue == 0)\n",
    "        NextInputScaleName = scale_chain[f'{key[0:-6]}scale']\n",
    "        NextInputScaleValue = myModel.state_dict()[NextInputScaleName].item()\n",
    "        NextInputZeroPointName = zero_point_chain[f'{key[0:-6]}zero_point']\n",
    "        NextInputZeroPointValue = myModel.state_dict()[NextInputZeroPointName].item()\n",
    "        M = WeightScaleValue * InputScaleValue / NextInputScaleValue\n",
    "        assert(M > 0 and M < 1)\n",
    "        # NandMo = iter(get_mo(M, P))\n",
    "        # yield next(NandMo) # N\n",
    "        # yield next(NandMo) # Mo\n",
    "        x = myModel.state_dict()[key].dequantize()\n",
    "        # bias = torch.ops.quantized.conv2d(myModel.state_dict()[key],myModel.state_dict()[f'{key[0:-6]}zero_point'], (3, 3), (1, 1), (1, 1), 0, 0, 1)\n",
    "        sub_bias = M * myModel.state_dict()[f'{key[0:-6]}bias'] + NextInputZeroPointValue\n",
    "        bias = torch.conv2d(torch.full(input_layer_size_list[input_counter], M * InputZeroPointValue), x, sub_bias, 1, 0, 1, 1) # no padding, padding zero has been quantified\n",
    "        # print(x.shape, bias.shape, bias[0][0][0][0].item())\n",
    "        bias_quantified_list = []\n",
    "        bias_numpy = bias.detach().numpy()\n",
    "        for channel in range(0, bias_numpy.shape[1]):\n",
    "            bias_quantified_list.append(bias_numpy[0][channel][0][0]) # bias\n",
    "        print(bias_quantified_list)\n",
    "        # yield bias_quantified_list\n",
    "        # print(myModel.state_dict()[f'{key[0:-6]}bias'])\n",
    "        # print(NextInputZeroPointValue)\n",
    "        # in_channels = 3\n",
    "        # out_channels = 64\n",
    "        # m = torch.nn.quantized.Conv2d(in_channels, out_channels, 3, stride=1, padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "        # bias = M * InputZeroPointValue * conv_result + M * myModel.state_dict()[f'{key[0:-6]}bias'] + NextInputZeroPointValue\n",
    "        # bias = M * myModel.state_dict()[key] * InputZeroPointValue + M * myModel.state_dict()[f'{key[0:-6]}bias'] + NextInputZeroPointValue\n",
    "        # print(f'M: {M} bias: {bias}')\n",
    "        input_counter = input_counter + 1\n",
    "    else:\n",
    "        print(key, 'pass')\n",
    "        pass\n",
    "# return\n",
    "\n",
    "# result = iter(getSandZ())\n",
    "# next(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "model_int8 conv keys: odict_keys(['features.0.weight', 'features.0.bias', 'features.0.scale', 'features.0.zero_point', 'features.2.weight', 'features.2.bias', 'features.2.scale', 'features.2.zero_point', 'features.5.weight', 'features.5.bias', 'features.5.scale', 'features.5.zero_point', 'features.7.weight', 'features.7.bias', 'features.7.scale', 'features.7.zero_point', 'features.10.weight', 'features.10.bias', 'features.10.scale', 'features.10.zero_point', 'features.12.weight', 'features.12.bias', 'features.12.scale', 'features.12.zero_point', 'features.14.weight', 'features.14.bias', 'features.14.scale', 'features.14.zero_point', 'features.17.weight', 'features.17.bias', 'features.17.scale', 'features.17.zero_point', 'features.19.weight', 'features.19.bias', 'features.19.scale', 'features.19.zero_point', 'features.21.weight', 'features.21.bias', 'features.21.scale', 'features.21.zero_point', 'features.24.weight', 'features.24.bias', 'features.24.scale', 'features.24.zero_point', 'features.26.weight', 'features.26.bias', 'features.26.scale', 'features.26.zero_point', 'features.28.weight', 'features.28.bias', 'features.28.scale', 'features.28.zero_point', 'classifier.0.scale', 'classifier.0.zero_point', 'classifier.0._packed_params.dtype', 'classifier.0._packed_params._packed_params', 'classifier.3.scale', 'classifier.3.zero_point', 'classifier.3._packed_params.dtype', 'classifier.3._packed_params._packed_params', 'classifier.6.scale', 'classifier.6.zero_point', 'classifier.6._packed_params.dtype', 'classifier.6._packed_params._packed_params', 'quant.scale', 'quant.zero_point'])\n",
      "\n",
      "\n",
      "model_int8 features.0.weight: tensor([[[[-0.5489,  0.1397,  0.5290],\n",
      "          [-0.5789,  0.3593,  0.7685],\n",
      "          [-0.6887, -0.0499,  0.4891]],\n",
      "\n",
      "         [[ 0.1797,  0.0100, -0.0798],\n",
      "          [ 0.0399, -0.0699, -0.2595],\n",
      "          [ 0.1298, -0.1697, -0.1298]],\n",
      "\n",
      "         [[ 0.3094, -0.1697, -0.4292],\n",
      "          [ 0.4791, -0.0798, -0.4891],\n",
      "          [ 0.6288,  0.0200, -0.2795]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2296,  0.1298,  0.1896],\n",
      "          [-0.4292, -0.2395,  0.2495],\n",
      "          [-0.2495,  0.1397, -0.0100]],\n",
      "\n",
      "         [[-0.1397, -0.2196,  0.1497],\n",
      "          [-0.8384, -0.3493,  0.5689],\n",
      "          [-0.2395,  0.5190,  0.5390]],\n",
      "\n",
      "         [[-0.3094, -0.3693, -0.1298],\n",
      "          [-0.4691, -0.1597,  0.3493],\n",
      "          [ 0.0499,  0.5889,  0.4990]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1797,  0.5190,  0.0100],\n",
      "          [-0.2695, -0.7186,  0.3094],\n",
      "          [-0.0798, -0.2196,  0.3394]],\n",
      "\n",
      "         [[ 0.3094,  0.6687,  0.0200],\n",
      "          [-0.4691, -1.0680,  0.3394],\n",
      "          [-0.0798, -0.3094,  0.5489]],\n",
      "\n",
      "         [[ 0.3194,  0.4192, -0.3493],\n",
      "          [ 0.0898, -0.4691,  0.0100],\n",
      "          [ 0.1098, -0.1497, -0.0200]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0798,  0.1298,  0.0299],\n",
      "          [ 0.2196,  0.2495, -0.0499],\n",
      "          [ 0.0499,  0.0299,  0.0200]],\n",
      "\n",
      "         [[-0.1797, -0.0699, -0.0100],\n",
      "          [-0.0499,  0.0100, -0.1298],\n",
      "          [-0.0599, -0.0599,  0.0399]],\n",
      "\n",
      "         [[-0.2296, -0.1198, -0.0200],\n",
      "          [-0.0998, -0.0200,  0.0000],\n",
      "          [-0.0299,  0.0000,  0.1397]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0200, -0.0299,  0.0000],\n",
      "          [-0.0699, -0.1896, -0.1397],\n",
      "          [-0.0699, -0.1797, -0.1697]],\n",
      "\n",
      "         [[ 0.0399, -0.0699, -0.0100],\n",
      "          [ 0.0100, -0.1497, -0.1198],\n",
      "          [ 0.0100, -0.0998, -0.1198]],\n",
      "\n",
      "         [[ 0.1298,  0.0898,  0.1298],\n",
      "          [ 0.1797,  0.1098,  0.1198],\n",
      "          [ 0.1497,  0.0998,  0.0998]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299, -0.1098, -0.2595],\n",
      "          [ 0.2795, -0.0399, -0.2595],\n",
      "          [ 0.3493,  0.0299, -0.0599]],\n",
      "\n",
      "         [[ 0.2495,  0.1597, -0.1697],\n",
      "          [ 0.3893,  0.0299, -0.3493],\n",
      "          [ 0.1896, -0.1996, -0.2994]],\n",
      "\n",
      "         [[ 0.4591,  0.4292,  0.2795],\n",
      "          [ 0.1597, -0.0599, -0.1896],\n",
      "          [-0.1996, -0.4591, -0.4292]]]], size=(64, 3, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.009980898350477219, zero_point=0)\n",
      "\n",
      "\n",
      "model_int8 features.0.bias: Parameter containing:\n",
      "tensor([ 0.4034,  0.3778,  0.4644, -0.3228,  0.3940, -0.3953,  0.3951, -0.5496,\n",
      "         0.2693, -0.7602, -0.3508,  0.2334, -1.3239, -0.1694,  0.3938, -0.1026,\n",
      "         0.0460, -0.6995,  0.1549,  0.5628,  0.3011,  0.3425,  0.1073,  0.4651,\n",
      "         0.1295,  0.0788, -0.0492, -0.5638,  0.1465, -0.3890, -0.0715,  0.0649,\n",
      "         0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,\n",
      "         0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,\n",
      "         0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,\n",
      "         0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "model_int8 features.0.scale: tensor(0.3098)\n",
      "\n",
      "\n",
      "model_int8 features.0.zero_point: tensor(61)\n",
      "\n",
      "\n",
      "model_int8 features.2.weight: tensor([[[[-0.0296, -0.0973, -0.1311],\n",
      "          [ 0.0085, -0.0846, -0.1650],\n",
      "          [ 0.0296, -0.0677, -0.1311]],\n",
      "\n",
      "         [[ 0.0465, -0.0296, -0.0508],\n",
      "          [ 0.0719,  0.0085, -0.0169],\n",
      "          [ 0.0719,  0.0381,  0.0169]],\n",
      "\n",
      "         [[ 0.0719,  0.0042, -0.0465],\n",
      "          [ 0.0846,  0.0508,  0.0000],\n",
      "          [ 0.0085,  0.0212,  0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0296,  0.0212, -0.0085],\n",
      "          [ 0.0254,  0.0042, -0.0338],\n",
      "          [ 0.0212,  0.0381, -0.0127]],\n",
      "\n",
      "         [[ 0.0212,  0.0423,  0.0592],\n",
      "          [ 0.0254,  0.0381,  0.0338],\n",
      "          [-0.0085,  0.0212,  0.0508]],\n",
      "\n",
      "         [[ 0.0212, -0.0212, -0.0973],\n",
      "          [-0.0592, -0.0719, -0.0761],\n",
      "          [-0.0381, -0.0254, -0.0042]]],\n",
      "\n",
      "\n",
      "        [[[-0.0127, -0.0761, -0.1354],\n",
      "          [-0.0381, -0.0804, -0.1438],\n",
      "          [-0.0423, -0.1058, -0.1607]],\n",
      "\n",
      "         [[-0.0085,  0.0465,  0.0169],\n",
      "          [ 0.0169,  0.0085, -0.0169],\n",
      "          [ 0.0000, -0.0338, -0.0338]],\n",
      "\n",
      "         [[ 0.0127, -0.0127, -0.0465],\n",
      "          [-0.0169,  0.0169, -0.0804],\n",
      "          [ 0.0169,  0.0042,  0.0085]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0423,  0.0381,  0.0254],\n",
      "          [ 0.0169,  0.0042, -0.0127],\n",
      "          [ 0.0254,  0.0169, -0.0254]],\n",
      "\n",
      "         [[-0.0212,  0.0042, -0.0212],\n",
      "          [ 0.0000, -0.0085,  0.0169],\n",
      "          [-0.0042, -0.0212, -0.0212]],\n",
      "\n",
      "         [[-0.0296, -0.0254,  0.0254],\n",
      "          [-0.0592, -0.0550,  0.0719],\n",
      "          [-0.0635, -0.0381,  0.0423]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0127,  0.0254,  0.0550],\n",
      "          [-0.0254, -0.0169,  0.0296],\n",
      "          [ 0.0127,  0.0085,  0.0296]],\n",
      "\n",
      "         [[-0.0804, -0.0888, -0.0550],\n",
      "          [-0.0465, -0.1100, -0.0127],\n",
      "          [ 0.0338,  0.0254,  0.1100]],\n",
      "\n",
      "         [[-0.0423, -0.0169, -0.0085],\n",
      "          [ 0.0296,  0.0465,  0.0169],\n",
      "          [ 0.0888,  0.1269,  0.0719]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0169, -0.0169],\n",
      "          [-0.0085, -0.0127,  0.0000],\n",
      "          [ 0.0085,  0.0000, -0.0338]],\n",
      "\n",
      "         [[-0.0085, -0.0042,  0.0000],\n",
      "          [ 0.0000, -0.0042,  0.0085],\n",
      "          [ 0.0296,  0.0169,  0.0338]],\n",
      "\n",
      "         [[ 0.0592, -0.0085,  0.0381],\n",
      "          [-0.0212, -0.0508,  0.0423],\n",
      "          [ 0.0212,  0.0085,  0.0338]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0465, -0.0296,  0.0085],\n",
      "          [-0.0465, -0.0508,  0.0169],\n",
      "          [-0.0423, -0.0254, -0.0254]],\n",
      "\n",
      "         [[-0.0212,  0.0169, -0.0635],\n",
      "          [ 0.0296,  0.0296, -0.0635],\n",
      "          [-0.0127,  0.0042, -0.0635]],\n",
      "\n",
      "         [[-0.0296, -0.0212, -0.0508],\n",
      "          [ 0.0127, -0.0508, -0.0550],\n",
      "          [-0.0381,  0.0000,  0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0085],\n",
      "          [ 0.0000,  0.0085,  0.0169],\n",
      "          [ 0.0127,  0.0085,  0.0338]],\n",
      "\n",
      "         [[ 0.0085,  0.0338,  0.0381],\n",
      "          [-0.0042,  0.0000,  0.0127],\n",
      "          [-0.0042,  0.0085,  0.0296]],\n",
      "\n",
      "         [[-0.0296, -0.0677, -0.0761],\n",
      "          [-0.0381, -0.0085,  0.0635],\n",
      "          [-0.0169,  0.0592,  0.1269]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0423,  0.0381,  0.0169],\n",
      "          [ 0.0423,  0.0296,  0.0169],\n",
      "          [-0.0042,  0.0212, -0.0042]],\n",
      "\n",
      "         [[ 0.0296, -0.0169,  0.0127],\n",
      "          [-0.0592, -0.0465, -0.0296],\n",
      "          [-0.0296,  0.0169, -0.0127]],\n",
      "\n",
      "         [[-0.0973, -0.0973, -0.1311],\n",
      "          [ 0.0719, -0.0085, -0.0761],\n",
      "          [ 0.1692,  0.1819,  0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0254,  0.0338,  0.0042],\n",
      "          [-0.0042,  0.0212,  0.0169],\n",
      "          [-0.0042, -0.0296,  0.0000]],\n",
      "\n",
      "         [[ 0.0085, -0.0042,  0.0042],\n",
      "          [ 0.0000,  0.0127,  0.0212],\n",
      "          [ 0.0042, -0.0127,  0.0042]],\n",
      "\n",
      "         [[ 0.0338,  0.0127,  0.0296],\n",
      "          [-0.0085,  0.0592,  0.0338],\n",
      "          [-0.0212,  0.0169, -0.0127]]],\n",
      "\n",
      "\n",
      "        [[[-0.0677,  0.0127,  0.0804],\n",
      "          [ 0.0085, -0.0338,  0.0508],\n",
      "          [-0.0381, -0.0296,  0.0635]],\n",
      "\n",
      "         [[ 0.0550, -0.1904,  0.1015],\n",
      "          [-0.1861,  0.0127,  0.2496],\n",
      "          [-0.0042,  0.0296, -0.0888]],\n",
      "\n",
      "         [[-0.1481, -0.1311,  0.1777],\n",
      "          [-0.1565,  0.1523,  0.0846],\n",
      "          [ 0.0931,  0.0846, -0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0127,  0.0338, -0.0085],\n",
      "          [ 0.0508, -0.0085, -0.0338],\n",
      "          [ 0.0127, -0.0296,  0.0254]],\n",
      "\n",
      "         [[ 0.0296,  0.0296,  0.0042],\n",
      "          [ 0.0042, -0.0169, -0.0212],\n",
      "          [-0.0085, -0.0381, -0.0212]],\n",
      "\n",
      "         [[ 0.0254,  0.0465,  0.0677],\n",
      "          [ 0.0127,  0.0338, -0.0592],\n",
      "          [ 0.0254, -0.0931, -0.0423]]]], size=(64, 64, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004230252001434565, zero_point=0)\n",
      "\n",
      "\n",
      "model_int8 features.2.bias: Parameter containing:\n",
      "tensor([ 0.0020, -0.0902,  0.6164, -0.0818,  0.2450, -0.0488,  0.1307, -0.0290,\n",
      "        -0.1429,  0.3068, -0.0399, -0.2524,  0.0999, -0.2326,  0.0353, -0.0904,\n",
      "         0.1138, -0.0307, -0.0108, -0.0215,  0.0554,  0.1382,  0.0362, -0.4511,\n",
      "         0.0056, -0.0246, -0.4296, -0.1458,  0.3813, -0.0359,  0.1184, -0.3527,\n",
      "        -0.0239, -0.0235,  0.6499, -0.0634, -0.0152, -0.2285,  0.0941, -0.5053,\n",
      "         0.1906,  0.0944,  0.3406, -0.0833,  0.1924, -0.1953, -0.0421, -0.1606,\n",
      "         0.3964,  0.2068,  0.1812, -0.1198, -0.0724, -0.1240,  0.1313,  0.1043,\n",
      "         0.5469,  0.5208,  0.0509, -0.8278,  0.4372, -0.3734, -0.3264, -0.1213],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "model_int8 features.2.scale: tensor(0.8419)\n",
      "\n",
      "\n",
      "model_int8 features.2.zero_point: tensor(70)\n",
      "\n",
      "\n",
      "model_int8 quant.scale: tensor([0.0375])\n",
      "\n",
      "\n",
      "model_int8 quant.zero_point: tensor([57])\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nmodel_int8 conv keys:',myModel.state_dict().keys())\n",
    "print('\\n\\nmodel_int8 features.0.weight:',myModel.state_dict()['features.0.weight'])\n",
    "print('\\n\\nmodel_int8 features.0.bias:',myModel.state_dict()['features.0.bias'])\n",
    "print('\\n\\nmodel_int8 features.0.scale:',myModel.state_dict()['features.0.scale'])\n",
    "print('\\n\\nmodel_int8 features.0.zero_point:',myModel.state_dict()['features.0.zero_point'])\n",
    "\n",
    "print('\\n\\nmodel_int8 features.2.weight:',myModel.state_dict()['features.2.weight'])\n",
    "print('\\n\\nmodel_int8 features.2.bias:',myModel.state_dict()['features.2.bias'])\n",
    "print('\\n\\nmodel_int8 features.2.scale:',myModel.state_dict()['features.2.scale'])\n",
    "print('\\n\\nmodel_int8 features.2.zero_point:',myModel.state_dict()['features.2.zero_point'])\n",
    "\n",
    "print('\\n\\nmodel_int8 quant.scale:',myModel.state_dict()['quant.scale'])\n",
    "print('\\n\\nmodel_int8 quant.zero_point:',myModel.state_dict()['quant.zero_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "conv_weight0 tensor([[[[ -55,   14,   53],\n",
      "          [ -58,   36,   77],\n",
      "          [ -69,   -5,   49]],\n",
      "\n",
      "         [[  18,    1,   -8],\n",
      "          [   4,   -7,  -26],\n",
      "          [  13,  -17,  -13]],\n",
      "\n",
      "         [[  31,  -17,  -43],\n",
      "          [  48,   -8,  -49],\n",
      "          [  63,    2,  -28]]],\n",
      "\n",
      "\n",
      "        [[[  23,   13,   19],\n",
      "          [ -43,  -24,   25],\n",
      "          [ -25,   14,   -1]],\n",
      "\n",
      "         [[ -14,  -22,   15],\n",
      "          [ -84,  -35,   57],\n",
      "          [ -24,   52,   54]],\n",
      "\n",
      "         [[ -31,  -37,  -13],\n",
      "          [ -47,  -16,   35],\n",
      "          [   5,   59,   50]]],\n",
      "\n",
      "\n",
      "        [[[  18,   52,    1],\n",
      "          [ -27,  -72,   31],\n",
      "          [  -8,  -22,   34]],\n",
      "\n",
      "         [[  31,   67,    2],\n",
      "          [ -47, -107,   34],\n",
      "          [  -8,  -31,   55]],\n",
      "\n",
      "         [[  32,   42,  -35],\n",
      "          [   9,  -47,    1],\n",
      "          [  11,  -15,   -2]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[   8,   13,    3],\n",
      "          [  22,   25,   -5],\n",
      "          [   5,    3,    2]],\n",
      "\n",
      "         [[ -18,   -7,   -1],\n",
      "          [  -5,    1,  -13],\n",
      "          [  -6,   -6,    4]],\n",
      "\n",
      "         [[ -23,  -12,   -2],\n",
      "          [ -10,   -2,    0],\n",
      "          [  -3,    0,   14]]],\n",
      "\n",
      "\n",
      "        [[[   2,   -3,    0],\n",
      "          [  -7,  -19,  -14],\n",
      "          [  -7,  -18,  -17]],\n",
      "\n",
      "         [[   4,   -7,   -1],\n",
      "          [   1,  -15,  -12],\n",
      "          [   1,  -10,  -12]],\n",
      "\n",
      "         [[  13,    9,   13],\n",
      "          [  18,   11,   12],\n",
      "          [  15,   10,   10]]],\n",
      "\n",
      "\n",
      "        [[[   3,  -11,  -26],\n",
      "          [  28,   -4,  -26],\n",
      "          [  35,    3,   -6]],\n",
      "\n",
      "         [[  25,   16,  -17],\n",
      "          [  39,    3,  -35],\n",
      "          [  19,  -20,  -30]],\n",
      "\n",
      "         [[  46,   43,   28],\n",
      "          [  16,   -6,  -19],\n",
      "          [ -20,  -46,  -43]]]], dtype=torch.int8)\n",
      "\n",
      "\n",
      "conv_weight2 tensor([[[[ -7, -23, -31],\n",
      "          [  2, -20, -39],\n",
      "          [  7, -16, -31]],\n",
      "\n",
      "         [[ 11,  -7, -12],\n",
      "          [ 17,   2,  -4],\n",
      "          [ 17,   9,   4]],\n",
      "\n",
      "         [[ 17,   1, -11],\n",
      "          [ 20,  12,   0],\n",
      "          [  2,   5,   1]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  7,   5,  -2],\n",
      "          [  6,   1,  -8],\n",
      "          [  5,   9,  -3]],\n",
      "\n",
      "         [[  5,  10,  14],\n",
      "          [  6,   9,   8],\n",
      "          [ -2,   5,  12]],\n",
      "\n",
      "         [[  5,  -5, -23],\n",
      "          [-14, -17, -18],\n",
      "          [ -9,  -6,  -1]]],\n",
      "\n",
      "\n",
      "        [[[ -3, -18, -32],\n",
      "          [ -9, -19, -34],\n",
      "          [-10, -25, -38]],\n",
      "\n",
      "         [[ -2,  11,   4],\n",
      "          [  4,   2,  -4],\n",
      "          [  0,  -8,  -8]],\n",
      "\n",
      "         [[  3,  -3, -11],\n",
      "          [ -4,   4, -19],\n",
      "          [  4,   1,   2]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 10,   9,   6],\n",
      "          [  4,   1,  -3],\n",
      "          [  6,   4,  -6]],\n",
      "\n",
      "         [[ -5,   1,  -5],\n",
      "          [  0,  -2,   4],\n",
      "          [ -1,  -5,  -5]],\n",
      "\n",
      "         [[ -7,  -6,   6],\n",
      "          [-14, -13,  17],\n",
      "          [-15,  -9,  10]]],\n",
      "\n",
      "\n",
      "        [[[  3,   6,  13],\n",
      "          [ -6,  -4,   7],\n",
      "          [  3,   2,   7]],\n",
      "\n",
      "         [[-19, -21, -13],\n",
      "          [-11, -26,  -3],\n",
      "          [  8,   6,  26]],\n",
      "\n",
      "         [[-10,  -4,  -2],\n",
      "          [  7,  11,   4],\n",
      "          [ 21,  30,  17]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,   4,  -4],\n",
      "          [ -2,  -3,   0],\n",
      "          [  2,   0,  -8]],\n",
      "\n",
      "         [[ -2,  -1,   0],\n",
      "          [  0,  -1,   2],\n",
      "          [  7,   4,   8]],\n",
      "\n",
      "         [[ 14,  -2,   9],\n",
      "          [ -5, -12,  10],\n",
      "          [  5,   2,   8]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-11,  -7,   2],\n",
      "          [-11, -12,   4],\n",
      "          [-10,  -6,  -6]],\n",
      "\n",
      "         [[ -5,   4, -15],\n",
      "          [  7,   7, -15],\n",
      "          [ -3,   1, -15]],\n",
      "\n",
      "         [[ -7,  -5, -12],\n",
      "          [  3, -12, -13],\n",
      "          [ -9,   0,   8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,   0,  -2],\n",
      "          [  0,   2,   4],\n",
      "          [  3,   2,   8]],\n",
      "\n",
      "         [[  2,   8,   9],\n",
      "          [ -1,   0,   3],\n",
      "          [ -1,   2,   7]],\n",
      "\n",
      "         [[ -7, -16, -18],\n",
      "          [ -9,  -2,  15],\n",
      "          [ -4,  14,  30]]],\n",
      "\n",
      "\n",
      "        [[[ 10,   9,   4],\n",
      "          [ 10,   7,   4],\n",
      "          [ -1,   5,  -1]],\n",
      "\n",
      "         [[  7,  -4,   3],\n",
      "          [-14, -11,  -7],\n",
      "          [ -7,   4,  -3]],\n",
      "\n",
      "         [[-23, -23, -31],\n",
      "          [ 17,  -2, -18],\n",
      "          [ 40,  43,   8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  6,   8,   1],\n",
      "          [ -1,   5,   4],\n",
      "          [ -1,  -7,   0]],\n",
      "\n",
      "         [[  2,  -1,   1],\n",
      "          [  0,   3,   5],\n",
      "          [  1,  -3,   1]],\n",
      "\n",
      "         [[  8,   3,   7],\n",
      "          [ -2,  14,   8],\n",
      "          [ -5,   4,  -3]]],\n",
      "\n",
      "\n",
      "        [[[-16,   3,  19],\n",
      "          [  2,  -8,  12],\n",
      "          [ -9,  -7,  15]],\n",
      "\n",
      "         [[ 13, -45,  24],\n",
      "          [-44,   3,  59],\n",
      "          [ -1,   7, -21]],\n",
      "\n",
      "         [[-35, -31,  42],\n",
      "          [-37,  36,  20],\n",
      "          [ 22,  20,  -8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  3,   8,  -2],\n",
      "          [ 12,  -2,  -8],\n",
      "          [  3,  -7,   6]],\n",
      "\n",
      "         [[  7,   7,   1],\n",
      "          [  1,  -4,  -5],\n",
      "          [ -2,  -9,  -5]],\n",
      "\n",
      "         [[  6,  11,  16],\n",
      "          [  3,   8, -14],\n",
      "          [  6, -22, -10]]]], dtype=torch.int8)\n",
      "Size of model after quantization\n",
      "Size (MB): 138.416915\n",
      "..........Evaluation accuracy on 300 images, 78.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "conv_weight0 = myModel.state_dict()['features.0.weight']\n",
    "conv_weight0.int_repr()\n",
    "print('\\n\\nconv_weight0',conv_weight0.int_repr())\n",
    "\n",
    "conv_weight2 = myModel.state_dict()['features.2.weight']\n",
    "conv_weight2.int_repr()\n",
    "print('\\n\\nconv_weight2',conv_weight2.int_repr())\n",
    "\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(myModel), saved_model_dir + scripted_default_quantized_model_file) # save default_quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    5. optimal\n",
    "    ·Quantizes weights on a per-channel basis\n",
    "    ·Uses a histogram observer that collects a histogram of activations and then picks quantization parameters\n",
    "    in an optimal manner.\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "# per_channel_quantized_model.fuse_model() # VGG dont need fuse\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') # set the quantize config\n",
    "print('\\n optimal quantize config: ')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True) # execute the quantize config\n",
    "evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches) # calibrate\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True) # convert to quantize model\n",
    "print('Post Training Optimal Quantization: Convert done')\n",
    "\n",
    "print(\"Size of model after optimal quantization\")\n",
    "print_size_of_model(per_channel_quantized_model)\n",
    "\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches) # test acc\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_optimal_quantized_model_file) # save quantized model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    6. compare performance\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nInference time compare: \")\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_default_quantized_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_optimal_quantized_model_file, data_loader_test)\n",
    "\n",
    "\"\"\" you can compare the model's size/accuracy/inference time.\n",
    "    ----------------------------------------------------------------------------------------\n",
    "                    | origin model | default quantized model | optimal quantized model\n",
    "    model size:     |    553 MB    |         138 MB          |        138 MB\n",
    "    test accuracy:  |    79.33     |         76.67           |        78.67\n",
    "    inference time: |    317 ms    |         254 ms          |        257 ms\n",
    "    ---------------------------------------------------------------------------------------\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
