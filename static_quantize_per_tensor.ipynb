{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_train : 1000\n",
      "dataset_test : 1000\n",
      "\n",
      " Before quantization: \n",
      " VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      ")\n",
      "Size of baseline model\n",
      "Size (MB): 553.435632\n",
      "..........Evaluation accuracy on 300 images, 79.33\n",
      "\n",
      " myModel.qconfig: \n",
      " QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "\n",
      "Post Training Quantization Prepare: Inserting Observers by Calibrate\n",
      "..................................Calibrate done\n",
      "Post Training Quantization: Convert done\n",
      "\n",
      " After quantization: \n",
      " VGG(\n",
      "  (features): Sequential(\n",
      "    (0): QuantizedConv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.3096976578235626, zero_point=62, padding=(1, 1))\n",
      "    (1): QuantizedReLU(inplace=True)\n",
      "    (2): QuantizedConv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), scale=0.7554243803024292, zero_point=66, padding=(1, 1))\n",
      "    (3): QuantizedReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): QuantizedConv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.2297083139419556, zero_point=82, padding=(1, 1))\n",
      "    (6): QuantizedReLU(inplace=True)\n",
      "    (7): QuantizedConv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), scale=1.5904121398925781, zero_point=70, padding=(1, 1))\n",
      "    (8): QuantizedReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): QuantizedConv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), scale=2.224628210067749, zero_point=65, padding=(1, 1))\n",
      "    (11): QuantizedReLU(inplace=True)\n",
      "    (12): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=2.7630984783172607, zero_point=72, padding=(1, 1))\n",
      "    (13): QuantizedReLU(inplace=True)\n",
      "    (14): QuantizedConv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), scale=3.4732987880706787, zero_point=51, padding=(1, 1))\n",
      "    (15): QuantizedReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): QuantizedConv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), scale=3.9609615802764893, zero_point=58, padding=(1, 1))\n",
      "    (18): QuantizedReLU(inplace=True)\n",
      "    (19): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=3.1670966148376465, zero_point=72, padding=(1, 1))\n",
      "    (20): QuantizedReLU(inplace=True)\n",
      "    (21): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.778215169906616, zero_point=81, padding=(1, 1))\n",
      "    (22): QuantizedReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.1221671104431152, zero_point=59, padding=(1, 1))\n",
      "    (25): QuantizedReLU(inplace=True)\n",
      "    (26): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.160407066345215, zero_point=72, padding=(1, 1))\n",
      "    (27): QuantizedReLU(inplace=True)\n",
      "    (28): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.580568790435791, zero_point=65, padding=(1, 1))\n",
      "    (29): QuantizedReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): QuantizedLinear(in_features=25088, out_features=4096, scale=0.49767252802848816, zero_point=77, qscheme=torch.per_tensor_affine)\n",
      "    (1): QuantizedReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): QuantizedLinear(in_features=4096, out_features=4096, scale=0.30696651339530945, zero_point=70, qscheme=torch.per_tensor_affine)\n",
      "    (4): QuantizedReLU()\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): QuantizedLinear(in_features=4096, out_features=1000, scale=0.558856189250946, zero_point=25, qscheme=torch.per_tensor_affine)\n",
      "  )\n",
      "  (quant): Quantize(scale=tensor([0.0375]), zero_point=tensor([57]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "==========================\n",
    "    Style: static quantize\n",
    "    Model: VGG-16\n",
    "    Create by: Han_yz @ 2020/1/29\n",
    "    Email: 20125169@bjtu.edu.cn\n",
    "    Github: https://github.com/Forggtensky\n",
    "==========================\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    1、Model architecture\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self,features,num_classes=1000,init_weights=False):\n",
    "        super(VGG,self).__init__()\n",
    "        self.features = features  # 提取特征部分的网络，也为Sequential格式\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n",
    "        self.classifier = nn.Sequential(  # 分类部分的网络\n",
    "            nn.Linear(512*7*7,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096,num_classes)\n",
    "        )\n",
    "        # add the quantize part\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "        if init_weights:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x,start_dim=1)\n",
    "        # x = x.mean([2, 3])\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module,nn.Conv2d):\n",
    "                # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.constant_(module.bias,0)\n",
    "            elif isinstance(module,nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "                # nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(module.bias,0)\n",
    "\n",
    "cfgs = {\n",
    "    'vgg11':[64,'M',128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg13':[64,64,'M',128,128,'M',256,256,'M',512,512,'M',512,512,'M'],\n",
    "    'vgg16':[64,64,'M',128,128,'M',256,256,256,'M',512,512,512,'M',512,512,512,'M'],\n",
    "    'vgg19':[64,64,'M',128,128,'M',256,256,256,256,'M',512,512,512,512,'M',512,512,512,512,'M'],\n",
    "}\n",
    "\n",
    "def make_features(cfg:list):\n",
    "    layers = []\n",
    "    in_channels = 3\n",
    "    for v in cfg:\n",
    "        if v == 'M':\n",
    "            layers += [nn.MaxPool2d(kernel_size=2,stride=2)]  #vgg采用的池化层均为2,2参数\n",
    "        else:\n",
    "            conv2d = nn.Conv2d(in_channels,v,kernel_size=3,padding=1)  #vgg卷积层采用的卷积核均为3,1参数\n",
    "            layers += [conv2d,nn.ReLU(True)]\n",
    "            in_channels = v\n",
    "    return nn.Sequential(*layers)  #非关键字的形式输入网络的参数\n",
    "\n",
    "def vgg(model_name='vgg16',**kwargs):\n",
    "    try:\n",
    "        cfg = cfgs[model_name]\n",
    "    except:\n",
    "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
    "        exit(-1)\n",
    "    model = VGG(make_features(cfg),**kwargs)  # **kwargs为可变长度字典，保存多个输入参数\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    2、Helper functions\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n",
    "\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "\n",
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 5\n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time: %3.0f ms' % (elapsed/num_images*1000))\n",
    "    return elapsed\n",
    "\n",
    "def load_model(model_file):\n",
    "    model_name = \"vgg16\"\n",
    "    model = vgg(model_name=model_name,num_classes=1000,init_weights=False)\n",
    "    state_dict = torch.load(model_file)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    3. Define dataset and data loaders\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "def prepare_data_loaders(data_path):\n",
    "    traindir = os.path.join(data_path, 'train')\n",
    "    valdir = os.path.join(data_path, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    dataset = torchvision.datasets.ImageFolder(\n",
    "        traindir,\n",
    "        transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_train : %d\" % (len(dataset)))\n",
    "\n",
    "    dataset_test = torchvision.datasets.ImageFolder(\n",
    "        valdir,\n",
    "        transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ]))\n",
    "    print(\"dataset_test : %d\" % (len(dataset_test)))\n",
    "\n",
    "    train_sampler = torch.utils.data.RandomSampler(dataset)\n",
    "    test_sampler = torch.utils.data.SequentialSampler(dataset_test)\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=train_batch_size,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        dataset_test, batch_size=eval_batch_size,\n",
    "        sampler=test_sampler)\n",
    "\n",
    "    return data_loader, data_loader_test\n",
    "\n",
    "# Specify random seed for repeatable results\n",
    "torch.manual_seed(191009)\n",
    "\n",
    "data_path = 'data/imagenet_1k'\n",
    "saved_model_dir = 'model/'\n",
    "float_model_file = 'vgg16_pretrained_float.pth'\n",
    "scripted_float_model_file = 'vgg16_quantization_scripted.pth'\n",
    "scripted_default_quantized_model_file = 'vgg16_quantization_scripted_default_quantized.pth'\n",
    "scripted_optimal_quantized_model_file = 'vgg16_quantization_scripted_optimal_quantized.pth'\n",
    "\n",
    "train_batch_size = 30\n",
    "eval_batch_size = 30\n",
    "\n",
    "data_loader, data_loader_test = prepare_data_loaders(data_path)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "float_model = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "print('\\n Before quantization: \\n',float_model)\n",
    "float_model.eval()\n",
    "\n",
    "# Note: vgg-16 has no BN layer so that not need to fuse model\n",
    "\n",
    "num_eval_batches = 10\n",
    "\n",
    "print(\"Size of baseline model\")\n",
    "print_size_of_model(float_model)\n",
    "\n",
    "# to get a “baseline” accuracy, see the accuracy of our un-quantized model\n",
    "top1, top5 = evaluate(float_model, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(float_model), saved_model_dir + scripted_float_model_file) # save un_quantized model\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    4. Post-training static quantization\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "num_calibration_batches = 40\n",
    "\n",
    "myModel = load_model(saved_model_dir + float_model_file).to('cpu')\n",
    "\n",
    "\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print('\\n myModel.qconfig: \\n',myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate with the training set\n",
    "print('\\nPost Training Quantization Prepare: Inserting Observers by Calibrate')\n",
    "evaluate(myModel, criterion, data_loader, neval_batches=num_calibration_batches)\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "\n",
    "\n",
    "print('\\n After quantization: \\n',myModel)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### save quantified data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight\n",
      "dealing features.0.weight\n",
      "features.0.scale\n",
      "features.0.zero_point\n",
      "features.0.bias\n",
      "features.2.weight\n",
      "dealing features.2.weight\n",
      "features.2.scale\n",
      "features.2.zero_point\n",
      "features.2.bias\n",
      "features.5.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dealing features.5.weight\n",
      "features.5.scale\n",
      "features.5.zero_point\n",
      "features.5.bias\n",
      "features.7.weight\n",
      "dealing features.7.weight\n",
      "features.7.scale\n",
      "features.7.zero_point\n",
      "features.7.bias\n",
      "features.10.weight\n",
      "dealing features.10.weight\n",
      "features.10.scale\n",
      "features.10.zero_point\n",
      "features.10.bias\n",
      "features.12.weight\n",
      "dealing features.12.weight\n",
      "features.12.scale\n",
      "features.12.zero_point\n",
      "features.12.bias\n",
      "features.14.weight\n",
      "dealing features.14.weight\n",
      "features.14.scale\n",
      "features.14.zero_point\n",
      "features.14.bias\n",
      "features.17.weight\n",
      "dealing features.17.weight\n",
      "features.17.scale\n",
      "features.17.zero_point\n",
      "features.17.bias\n",
      "features.19.weight\n",
      "dealing features.19.weight\n",
      "features.19.scale\n",
      "features.19.zero_point\n",
      "features.19.bias\n",
      "features.21.weight\n",
      "dealing features.21.weight\n",
      "features.21.scale\n",
      "features.21.zero_point\n",
      "features.21.bias\n",
      "features.24.weight\n",
      "dealing features.24.weight\n",
      "features.24.scale\n",
      "features.24.zero_point\n",
      "features.24.bias\n",
      "features.26.weight\n",
      "dealing features.26.weight\n",
      "features.26.scale\n",
      "features.26.zero_point\n",
      "features.26.bias\n",
      "features.28.weight\n",
      "dealing features.28.weight\n",
      "features.28.scale\n",
      "features.28.zero_point\n",
      "features.28.bias\n",
      "classifier.0.scale\n",
      "classifier.0.zero_point\n",
      "classifier.0._packed_params.weight\n",
      "classifier.0._packed_params.bias\n",
      "classifier.0._packed_params.dtype\n",
      "classifier.3.scale\n",
      "classifier.3.zero_point\n",
      "classifier.3._packed_params.weight\n",
      "classifier.3._packed_params.bias\n",
      "classifier.3._packed_params.dtype\n",
      "classifier.6.scale\n",
      "classifier.6.zero_point\n",
      "classifier.6._packed_params.weight\n",
      "classifier.6._packed_params.bias\n",
      "classifier.6._packed_params.dtype\n",
      "quant.scale\n",
      "quant.zero_point\n"
     ]
    }
   ],
   "source": [
    "# print('\\n\\nmodel_int8 conv keys:',per_channel_quantized_model.state_dict().keys())\n",
    "# import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "\n",
    "def getrealzero(_scale : float, _zero_point : int) -> int:\n",
    "    return int(torch.int_repr(torch.quantize_per_tensor(torch.tensor(0.), float(_scale), int(_zero_point), torch.qint8)).numpy())\n",
    "    # return round(-_zero_point / _scale)\n",
    "\n",
    "class hbm_channel_mem(): # each channel save 32 bits data\n",
    "    data_mem = []\n",
    "    zeropoint_mem = []\n",
    "    def __init__(self):\n",
    "        # _list_tmp = list()\n",
    "        # for iter in range(0, 32):\n",
    "            # self.data_mem.append(_list_tmp)\n",
    "        self.data_mem = [[] for x in range(0, 32)]\n",
    "        self.zeropoint_mem = [[] for x in range(0, 32)]\n",
    "\n",
    "    # def set(self, tunnel, addr, value):\n",
    "    #     data_mem\n",
    "    def autofill(self):\n",
    "        for position in range(1, 32):\n",
    "            for addr in range(len(self.data_mem[position]), len(self.data_mem[0])):\n",
    "                self.data_mem[position].append(self.zeropoint_mem[0][addr])\n",
    "            # while (len(self.data_mem[0]) > len(self.data_mem[position])):\n",
    "            #     self.data_mem[position].append(self.zeropoint_mem[0][len(self.data_mem[position])]) # change to real zero point\n",
    "\n",
    "    def print(self, maxaddrlimit = 64):\n",
    "        maxaddr = 0\n",
    "        for position in range(0, 32):\n",
    "            maxaddr = max(maxaddr, len(self.data_mem[position]))\n",
    "\n",
    "        if maxaddrlimit != -1:\n",
    "            maxaddr = min(maxaddr, maxaddrlimit)\n",
    "\n",
    "        for addr in range(0, maxaddr):\n",
    "            for position in range(0, 32):\n",
    "                if len(self.data_mem[position]) <= addr:\n",
    "                    print('NUL\\t', end = '')\n",
    "                else:\n",
    "                    print(f'{self.data_mem[position][addr]}\\t', end = '')\n",
    "            print()\n",
    "\n",
    "    def append(self, position : int, value : int, zeropoint : int):\n",
    "        # print('channel append enter')\n",
    "        # print(self)\n",
    "        # print(position, value)\n",
    "        # print('channel append exit')\n",
    "        self.data_mem[position].append(value)\n",
    "        self.zeropoint_mem[position].append(zeropoint)\n",
    "\n",
    "    def appends(self, valuelist : list):\n",
    "        for iter in range(0, 32):\n",
    "            self.data_mem[iter].append(valuelist[iter])\n",
    "\n",
    "    def save(self, filepath : str, saveaspcieorder = True):\n",
    "        self.autofill()\n",
    "        # self.print()\n",
    "        if saveaspcieorder:\n",
    "            with open(filepath, 'wb+') as f:\n",
    "                for addr_base in range(0, len(self.data_mem[0]), 8):\n",
    "                    for addr in range(addr_base + 7, addr_base - 1, -1):\n",
    "                        for position in range(0, 32):\n",
    "                            # print(type(self.data_mem[addr][position]))\n",
    "                            try:\n",
    "                                # print(addr, ' ', position)\n",
    "                                # print(self.data_mem[position][addr])\n",
    "                                # print(int(self.data_mem[position][addr]))\n",
    "                                # print(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                                f.write(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                            except:\n",
    "                                \n",
    "                                print(self.data_mem[position][addr])\n",
    "                                raise\n",
    "                        # f.write(int.from_bytes(self.data_mem[addr][position].to_bytes(1, 'little', signed = True), 'little', signed = False))\n",
    "                    # raise\n",
    "                    \n",
    "                f.close()\n",
    "        else:\n",
    "            with open(filepath, 'wb+') as f:\n",
    "                for addr in range(len(self.data_mem[0])):\n",
    "                    \n",
    "                    for position in range(31, -1, -1):\n",
    "                        # print(type(self.data_mem[addr][position]))\n",
    "                        try:\n",
    "                            # print(self.data_mem[position][addr])\n",
    "                            # print(int(self.data_mem[position][addr]))\n",
    "                            # print(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                            f.write(int(self.data_mem[position][addr]).to_bytes(1, 'little', signed = True))\n",
    "                        except:\n",
    "                            \n",
    "                            print(self.data_mem[position][addr])\n",
    "                            raise\n",
    "                        # f.write(int.from_bytes(self.data_mem[addr][position].to_bytes(1, 'little', signed = True), 'little', signed = False))\n",
    "                    # raise\n",
    "                    \n",
    "                f.close()\n",
    "\n",
    "class hbm_mem():\n",
    "    hbm_data_mem = []\n",
    "    robin = int()\n",
    "\n",
    "    def __init__(self):\n",
    "        # _list_tmp = hbm_channel_mem()\n",
    "        self.robin = int(0)\n",
    "        # for iter in range(0, 32):\n",
    "            # self.hbm_data_mem.append(_list_tmp)\n",
    "        self.hbm_data_mem = [hbm_channel_mem() for x in range(0, 32)]\n",
    "        # print(\"init\")\n",
    "        # for i in self.hbm_data_mem:\n",
    "        #     print(i)\n",
    "    \n",
    "    def append_channel(self, channel : int, tunnel : int, value : int):\n",
    "        self.hbm_data_mem[channel].append(tunnel, value)\n",
    "\n",
    "    def appends_channel(self, channel : int, valuelist : list):\n",
    "        self.hbm_data_mem[channel].appends(valuelist)\n",
    "\n",
    "    def append(self, valuelist : list, zeropoint : int):\n",
    "        # print('append enter')\n",
    "        for channel in range(0, 32):\n",
    "            # print(valuelist[channel])\n",
    "            for value in valuelist[channel]:\n",
    "                self.hbm_data_mem[channel].append(self.robin, value, zeropoint)\n",
    "\n",
    "        if (self.robin == 31):\n",
    "            self.robin = 0\n",
    "        else:\n",
    "            self.robin = self.robin + 1\n",
    "        # print('append return')\n",
    "\n",
    "\n",
    "    def autofill(self):\n",
    "        while (self.robin != 0):\n",
    "            for channel in range(0, 32):\n",
    "                self.hbm_data_mem[channel].append(self.robin, 0) # TODO: change to real zero point\n",
    "\n",
    "            if (self.robin == 31):\n",
    "                self.robin = 0\n",
    "            else:\n",
    "                self.robin = self.robin + 1\n",
    "\n",
    "    def print(self):\n",
    "        for channel in range(0, 32):\n",
    "            print(f'data in {channel}:')\n",
    "            # print(self.hbm_data_mem[channel])\n",
    "            self.hbm_data_mem[channel].print()\n",
    "\n",
    "    def save(self, filepath : str):\n",
    "        for channel in range(0, 32):\n",
    "            self.hbm_data_mem[channel].save(\"{}_{}.bin\".format(filepath, channel))\n",
    "\n",
    "\n",
    "i_hbm_mem = hbm_mem()\n",
    "\n",
    "for key in myModel.state_dict().keys():\n",
    "    print(key)\n",
    "    # print(layer_numpy.shape)\n",
    "    \n",
    "    if (key.find(\"weight\") >= 0):\n",
    "        pass\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    if (myModel.state_dict()[key].dim() == 4):\n",
    "        layer_numpy = torch.int_repr(myModel.state_dict()[key]).numpy()\n",
    "        layer_zero_point = myModel.state_dict()[key].q_zero_point() # myModel.state_dict()[key[0:-6] + \"zero_point\"].numpy()\n",
    "        layer_scale = myModel.state_dict()[key].q_scale() # myModel.state_dict()[key[0:-6] + \"scale\"].numpy()\n",
    "        realzero = getrealzero(layer_scale, layer_zero_point)\n",
    "        # print(\"zero_point: \", layer_zero_point)\n",
    "        # print(\"scale: \", layer_scale)\n",
    "        # print(\"zero_quan: \", realzero)\n",
    "        \n",
    "        print(f\"dealing {key}\")\n",
    "        for kernel_base in range(0, layer_numpy.shape[0], 16): # tqdm.trange(0, layer_numpy.shape[0], 16): # \n",
    "            multi_channel_valuelist_tmp_channelgroup_1 = []\n",
    "            multi_channel_valuelist_tmp_channelgroup_2 = []\n",
    "            for kernel in range(kernel_base, kernel_base + 16):\n",
    "                single_channel_valuelist_tmp_channelgroup_1 = []\n",
    "                single_channel_valuelist_tmp_channelgroup_2 = []\n",
    "\n",
    "                for channel_base in range(0, layer_numpy.shape[1], 16):\n",
    "                    for channel in range(channel_base, channel_base + 8): # channelgroup 1\n",
    "                        single_channel_valuelist_tmp = [] # one row 1*9\n",
    "                        for width in range(layer_numpy.shape[2]): # probably height and width is in this order\n",
    "                            for height in range(layer_numpy.shape[3]):\n",
    "                                if (channel < layer_numpy.shape[1]): # in case of conv_1 has only 3 channel\n",
    "                                    # single_channel_valuelist_tmp.append(per_channel_quantized_model.state_dict()[key][kernel][channel][width][height])\n",
    "                                    single_channel_valuelist_tmp.append(layer_numpy[kernel][channel][width][height])\n",
    "                                else:\n",
    "                                    single_channel_valuelist_tmp.append(realzero) # change to real zero\n",
    "                        single_channel_valuelist_tmp_channelgroup_1.extend(single_channel_valuelist_tmp) # extends to 8*9 through order is in reverse\n",
    "                            \n",
    "\n",
    "                    for channel in range(channel_base + 8, channel_base + 16): # channelgroup 2\n",
    "                        single_channel_valuelist_tmp = [] # one row 1*9\n",
    "                        for width in range(layer_numpy.shape[2]): # probably height and width is in this order\n",
    "                            for height in range(layer_numpy.shape[3]):\n",
    "                                if (channel < layer_numpy.shape[1]): # in case of conv_1 has only 3 channel\n",
    "                                    # single_channel_valuelist_tmp.append(per_channel_quantized_model.state_dict()[key][kernel][channel][width][height])\n",
    "                                    single_channel_valuelist_tmp.append(layer_numpy[kernel][channel][width][height])\n",
    "                                else:\n",
    "                                    single_channel_valuelist_tmp.append(realzero)  # change to real zero\n",
    "                        single_channel_valuelist_tmp_channelgroup_2.extend(single_channel_valuelist_tmp) # extends to 8*9\n",
    "\n",
    "                multi_channel_valuelist_tmp_channelgroup_1.append(single_channel_valuelist_tmp_channelgroup_1)\n",
    "                multi_channel_valuelist_tmp_channelgroup_2.append(single_channel_valuelist_tmp_channelgroup_2)\n",
    "\n",
    "            # print(\"multi_channel_valuelist_tmp_channelgroup_1\")\n",
    "            # for row in range(0, len(multi_channel_valuelist_tmp_channelgroup_1)):\n",
    "            #     print(len(multi_channel_valuelist_tmp_channelgroup_1[row]), multi_channel_valuelist_tmp_channelgroup_1[row])\n",
    "            # print(\"multi_channel_valuelist_tmp_channelgroup_2\")\n",
    "            # for row in range(0, len(multi_channel_valuelist_tmp_channelgroup_2)):\n",
    "            #     print(len(multi_channel_valuelist_tmp_channelgroup_2[row]), multi_channel_valuelist_tmp_channelgroup_2[row])\n",
    "            # print(multi_channel_valuelist_tmp_channelgroup_1)\n",
    "            # print(multi_channel_valuelist_tmp_channelgroup_2)\n",
    "            total_channel_valuelist_tmp = []\n",
    "            total_channel_valuelist_tmp.extend(multi_channel_valuelist_tmp_channelgroup_1)\n",
    "            total_channel_valuelist_tmp.extend(multi_channel_valuelist_tmp_channelgroup_2)\n",
    "            i_hbm_mem.append(total_channel_valuelist_tmp, realzero)\n",
    "            # i_hbm_mem.print()\n",
    "            # break\n",
    "\n",
    "    elif (myModel.state_dict()[key].dim() == 3):\n",
    "        pass\n",
    "    elif (myModel.state_dict()[key].dim() == 2):\n",
    "        pass\n",
    "    elif (myModel.state_dict()[key].dim() == 1):\n",
    "        pass\n",
    "    else:\n",
    "        pass\n",
    "        # raise\n",
    "\n",
    "    # i_hbm_mem.print()\n",
    "    # i_hbm_mem.autofill()\n",
    "    # print('after auto fill')\n",
    "    # i_hbm_mem.print()\n",
    "    # break\n",
    "i_hbm_mem.save(\"quan_bin_pertensor/quan_bin_pertensor\")\n",
    "# print('\\n\\nmodel_int8 features.0.weight:',per_channel_quantized_model.state_dict()['features.0.weight'])\n",
    "# print('\\n\\nmodel_int8 features.0.bias:',per_channel_quantized_model.state_dict()['features.0.bias'])\n",
    "# print('\\n\\nmodel_int8 features.0.scale:',per_channel_quantized_model.state_dict()['features.0.scale'])\n",
    "# print('\\n\\nmodel_int8 features.0.zero_point:',per_channel_quantized_model.state_dict()['features.0.zero_point'])\n",
    "\n",
    "# print('\\n\\nmodel_int8 features.2.weight:',per_channel_quantized_model.state_dict()['features.2.weight'])\n",
    "# print('\\n\\nmodel_int8 features.2.bias:',per_channel_quantized_model.state_dict()['features.2.bias'])\n",
    "# print('\\n\\nmodel_int8 features.2.scale:',per_channel_quantized_model.state_dict()['features.2.scale'])\n",
    "# print('\\n\\nmodel_int8 features.2.zero_point:',per_channel_quantized_model.state_dict()['features.2.zero_point'])\n",
    "\n",
    "# print('\\n\\nmodel_int8 quant.scale:',per_channel_quantized_model.state_dict()['quant.scale'])\n",
    "# print('\\n\\nmodel_int8 quant.zero_point:',per_channel_quantized_model.state_dict()['quant.zero_point'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ### save scale and zero_point value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# return  n, p\n",
    "def get_mo(M, P):\n",
    "    last_error = None\n",
    "    result = M * P\n",
    "    ret_n = 0\n",
    "\n",
    "    for n in range(1, 16):\n",
    "        ret_n = n\n",
    "        ret_n = 8\n",
    "        Mo = int(round(2 ** ret_n * M)) # 这里不一定要四舍五入截断，因为python定点数不好表示才这样处理\n",
    "        approx_result = (Mo * P) >> ret_n\n",
    "\n",
    "        cur_error = result - approx_result\n",
    "        if last_error == None:\n",
    "            pass\n",
    "        else:\n",
    "            if (abs(last_error - cur_error) < 1e-1 and abs(cur_error) < 1): # or (n == 15):\n",
    "                # yield n\n",
    "                break\n",
    "        last_error = cur_error\n",
    "    # print(\"Mo=%d, approx=%f, error=%f\"%\\\n",
    "    #     (Mo, approx_result, result-approx_result))\n",
    "    yield ret_n\n",
    "    yield Mo\n",
    "\n",
    "    return\n",
    "\n",
    "# M = 0.0072474273418460\n",
    "# P = 7091\n",
    "# ret_get_mo = iter(get_mo(M, P))\n",
    "# print(next(ret_get_mo)) # 1\n",
    "# print(next(ret_get_mo)) # 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight dealing\n",
      "8 \t 1 \t [66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66, 66]\n",
      "features.0.scale pass\n",
      "features.0.zero_point pass\n",
      "features.0.bias pass\n",
      "features.2.weight dealing\n",
      "8 \t 1 \t [82, 82, 81, 82, 82, 82, 82, 82, 82, 81, 82, 82, 81, 82, 82, 82, 82, 82, 82, 82, 82, 81, 82, 83, 82, 82, 83, 82, 81, 81, 81, 82, 82, 82, 81, 82, 82, 82, 81, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 81, 82, 82, 82, 82, 82, 81, 82, 83, 82, 82, 82, 82]\n",
      "features.2.scale pass\n",
      "features.2.zero_point pass\n",
      "features.2.bias pass\n",
      "features.5.weight dealing\n",
      "8 \t 1 \t [70, 70, 70, 70, 70, 70, 70, 70, 69, 69, 69, 70, 70, 70, 70, 69, 70, 70, 69, 70, 69, 70, 69, 70, 69, 70, 70, 69, 69, 70, 70, 70, 70, 69, 70, 70, 70, 69, 69, 69, 69, 70, 70, 70, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 68, 70, 69, 70, 70, 70, 69, 70, 70, 69, 69, 69, 70, 70, 69, 70, 70, 70, 70, 70, 69, 70, 70, 70, 70, 70, 70, 69, 69, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 70, 69, 70, 70, 70, 70, 69, 70, 70, 70, 69, 70, 69, 70, 69, 69, 70, 70, 70, 70, 70, 70, 70, 69, 70, 70, 70, 70, 70, 70, 70, 70]\n",
      "features.5.scale pass\n",
      "features.5.zero_point pass\n",
      "features.5.bias pass\n",
      "features.7.weight dealing\n",
      "8 \t 1 \t [65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 64, 65, 64, 65, 64, 65, 65, 64, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 64, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 65, 65, 65, 65, 64, 64, 64, 65, 65, 65, 65, 64, 65, 65, 65, 65, 64, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 64, 65, 64, 65, 65, 65, 65, 65, 65, 65, 65, 65, 64, 65, 65, 64, 64, 65, 64, 65, 65]\n",
      "features.7.scale pass\n",
      "features.7.zero_point pass\n",
      "features.7.bias pass\n",
      "features.10.weight dealing\n",
      "8 \t 1 \t [72, 72, 72, 72, 72, 72, 72, 71, 72, 72, 72, 72, 72, 71, 71, 72, 72, 72, 72, 72, 71, 72, 72, 71, 71, 72, 71, 72, 72, 72, 72, 71, 71, 72, 72, 71, 72, 72, 72, 72, 72, 72, 72, 71, 72, 72, 71, 72, 72, 72, 72, 71, 71, 72, 71, 71, 72, 72, 71, 71, 72, 71, 72, 72, 72, 72, 72, 72, 72, 72, 71, 72, 72, 72, 72, 71, 71, 72, 72, 71, 72, 71, 72, 72, 72, 72, 71, 72, 72, 72, 72, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 72, 72, 72, 72, 72, 72, 72, 71, 71, 72, 71, 71, 72, 72, 71, 71, 72, 72, 72, 71, 71, 72, 72, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 72, 72, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 71, 72, 72, 72, 72, 72, 72, 72, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 72, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 71, 72, 72, 72, 71, 72, 72, 72, 71, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71, 72, 72, 71, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 71]\n",
      "features.10.scale pass\n",
      "features.10.zero_point pass\n",
      "features.10.bias pass\n",
      "features.12.weight dealing\n",
      "8 \t 1 \t [50, 50, 51, 51, 50, 51, 51, 51, 50, 51, 50, 50, 51, 50, 51, 50, 51, 51, 50, 50, 50, 50, 51, 50, 50, 51, 50, 51, 51, 50, 51, 51, 50, 50, 50, 51, 50, 50, 50, 51, 50, 50, 50, 50, 50, 50, 50, 51, 51, 51, 50, 51, 51, 50, 50, 50, 51, 51, 51, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 51, 50, 50, 51, 51, 50, 50, 50, 51, 51, 51, 51, 51, 50, 50, 50, 50, 50, 50, 51, 51, 50, 51, 50, 50, 50, 50, 51, 50, 51, 51, 51, 50, 51, 51, 50, 51, 51, 51, 50, 51, 50, 51, 50, 50, 50, 50, 51, 50, 51, 50, 50, 50, 50, 51, 51, 50, 50, 50, 50, 50, 50, 51, 50, 50, 51, 50, 51, 50, 51, 50, 51, 51, 50, 50, 50, 50, 51, 51, 51, 50, 50, 51, 50, 51, 50, 51, 51, 50, 50, 50, 51, 51, 51, 50, 50, 50, 51, 50, 50, 50, 51, 50, 50, 50, 51, 50, 50, 51, 50, 50, 50, 51, 50, 50, 51, 50, 51, 50, 50, 50, 50, 51, 51, 51, 50, 50, 50, 51, 51, 50, 50, 50, 51, 50, 51, 51, 50, 51, 50, 51, 51, 50, 51, 51, 50, 50, 50, 50, 50, 51, 50, 51, 50, 50, 50, 50, 51, 51, 51, 51, 51, 50, 51, 50, 50, 50, 51, 50, 50, 51, 50, 51, 51, 51, 50, 51, 50, 50, 50, 51, 50, 50]\n",
      "features.12.scale pass\n",
      "features.12.zero_point pass\n",
      "features.12.bias pass\n",
      "features.14.weight dealing\n",
      "8 \t 1 \t [57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 56, 57, 57, 57, 57, 56, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 56, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 56, 57, 56, 57, 57, 57, 57, 57, 56, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 56, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56]\n",
      "features.14.scale pass\n",
      "features.14.zero_point pass\n",
      "features.14.bias pass\n",
      "features.17.weight dealing\n",
      "8 \t 1 \t [70, 71, 71, 71, 71, 71, 70, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 71, 70, 71, 71, 71, 71, 71, 71, 71, 72, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 70, 71, 72, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 70, 69, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 72, 71, 71, 70, 71, 71, 71, 71, 71, 71, 70, 71, 71, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71]\n",
      "features.17.scale pass\n",
      "features.17.zero_point pass\n",
      "features.17.bias pass\n",
      "features.19.weight dealing\n",
      "8 \t 1 \t [80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 81, 80, 80, 80, 80, 80, 80, 80, 79, 80, 79, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 79, 80, 80, 80, 79, 80, 80, 79, 79, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 79, 80, 80, 80, 79, 80, 79, 80, 80, 80, 79, 80, 80, 79, 80, 79, 80, 80, 79, 79, 79, 80, 80, 79, 80, 80, 79, 80, 79, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 79, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 79, 80, 80, 79, 79, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 79, 80, 80, 80, 80, 79, 80, 80, 80, 79, 80, 80, 79, 79, 80, 79, 79, 80, 80, 80, 79, 80, 80, 79, 80, 79, 80, 79, 80, 80, 79, 80, 80, 79, 80, 79, 80, 80, 80, 80, 80, 79, 79, 80, 79, 80, 80, 80, 80, 80, 79, 79, 80, 80, 80, 79, 80, 80, 81, 80, 80, 80, 80, 79, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 79, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 79, 79, 80, 80, 79, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 79, 80, 80, 79, 80, 80, 80, 80, 79, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 79, 80, 79, 79, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 79, 79, 80, 80, 80, 79, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 79, 79, 80, 80, 80, 79, 80, 79, 80, 79, 80, 80, 79, 79, 80, 79, 80, 80, 80, 80, 79, 79, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 79, 80, 80, 79, 80, 80, 79, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 79, 80, 80, 80, 79, 79, 80, 80, 80, 79, 79, 80, 80, 80, 79, 80, 80, 80, 80, 79, 80, 80, 79, 79, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 79, 80, 79, 80, 79, 80, 80, 80, 80, 80, 79, 80, 80, 80, 80, 80]\n",
      "features.19.scale pass\n",
      "features.19.zero_point pass\n",
      "features.19.bias pass\n",
      "features.21.weight dealing\n",
      "8 \t 1 \t [57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 56, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 56, 57, 57, 57, 57, 58, 57, 58, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 58, 57, 57, 56, 57, 57, 57, 58, 57, 58, 57, 58, 57, 58, 57, 58, 58, 57, 57, 57, 57, 57, 57, 57, 58, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 56, 57, 57, 57, 56, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 56, 57, 57, 57, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 56, 57, 57, 57, 57, 57, 58, 57, 56, 57, 57, 57, 57, 57, 58, 57, 58, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 56, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 57, 58, 57, 56, 57, 56, 57, 58, 57, 57, 57, 57, 56, 57, 57, 57, 57, 56, 57, 57, 57]\n",
      "features.21.scale pass\n",
      "features.21.zero_point pass\n",
      "features.21.bias pass\n",
      "features.24.weight dealing\n",
      "8 \t 1 \t [71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 72, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 71, 70, 71, 71, 71, 71, 71]\n",
      "features.24.scale pass\n",
      "features.24.zero_point pass\n",
      "features.24.bias pass\n",
      "features.26.weight dealing\n",
      "8 \t 1 \t [63, 63, 64, 63, 64, 63, 64, 63, 63, 63, 63, 64, 64, 63, 64, 64, 64, 63, 63, 63, 63, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 63, 64, 63, 63, 63, 63, 63, 64, 64, 63, 63, 64, 64, 63, 64, 63, 64, 64, 63, 62, 63, 63, 63, 63, 63, 64, 64, 64, 63, 63, 63, 64, 64, 62, 63, 63, 64, 63, 63, 63, 64, 63, 63, 63, 63, 64, 64, 63, 63, 64, 63, 63, 63, 64, 64, 63, 63, 63, 63, 62, 63, 64, 64, 64, 63, 63, 64, 63, 64, 63, 63, 64, 63, 64, 63, 63, 63, 64, 63, 63, 63, 63, 64, 64, 63, 64, 64, 63, 63, 63, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 64, 63, 64, 63, 63, 63, 63, 63, 64, 64, 63, 64, 63, 64, 64, 63, 63, 62, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 63, 64, 63, 64, 64, 63, 63, 63, 63, 63, 63, 64, 64, 63, 63, 63, 64, 63, 63, 64, 64, 64, 64, 64, 63, 64, 63, 63, 63, 64, 64, 63, 63, 63, 63, 63, 65, 63, 63, 64, 63, 64, 64, 64, 64, 64, 63, 64, 63, 63, 63, 64, 63, 64, 63, 64, 63, 63, 63, 64, 63, 64, 63, 63, 64, 64, 64, 63, 63, 64, 64, 63, 63, 63, 63, 63, 64, 63, 64, 63, 63, 63, 63, 63, 63, 64, 63, 63, 64, 63, 64, 64, 64, 64, 63, 63, 63, 64, 64, 63, 63, 64, 64, 63, 63, 63, 64, 64, 64, 63, 64, 63, 64, 64, 63, 63, 63, 63, 64, 64, 63, 63, 64, 62, 63, 63, 63, 64, 64, 63, 64, 64, 64, 63, 63, 63, 64, 64, 63, 64, 64, 63, 63, 63, 64, 63, 63, 64, 64, 64, 63, 63, 63, 63, 63, 64, 63, 64, 64, 63, 63, 63, 63, 63, 63, 63, 64, 64, 63, 63, 63, 64, 63, 63, 64, 63, 63, 63, 63, 64, 64, 64, 64, 63, 62, 63, 63, 64, 63, 63, 63, 64, 63, 63, 64, 63, 64, 63, 64, 63, 64, 63, 63, 63, 63, 63, 64, 63, 62, 63, 63, 64, 63, 63, 63, 63, 63, 64, 64, 63, 63, 64, 64, 64, 63, 64, 63, 64, 63, 64, 63, 63, 63, 63, 63, 64, 63, 63, 63, 63, 64, 63, 63, 64, 64, 64, 63, 63, 64, 64, 64, 63, 63, 64, 63, 63, 64, 63, 63, 64, 64, 64, 63, 63, 64, 63, 64, 64, 64, 63, 63, 63, 64, 63, 62, 63, 64, 63, 64, 63, 63, 63, 63, 63, 63, 63, 63, 64, 63, 63, 64, 63, 63, 63, 63, 64, 63, 63, 63, 64, 63, 64, 63, 63, 63, 63, 64, 63, 63, 64, 64, 63, 63, 63, 63, 63, 63, 64, 63, 62, 64, 63, 63, 64, 63, 63, 63, 63, 63, 65]\n",
      "features.26.scale pass\n",
      "features.26.zero_point pass\n",
      "features.26.bias pass\n",
      "features.28.weight dealing\n",
      "8 \t 1 \t [75, 75, 75, 74, 75, 74, 74, 75, 74, 75, 75, 76, 75, 74, 75, 75, 75, 75, 75, 75, 75, 76, 74, 75, 74, 74, 74, 75, 75, 74, 75, 74, 74, 75, 75, 75, 73, 75, 75, 75, 74, 75, 75, 75, 74, 75, 75, 75, 75, 74, 75, 74, 77, 75, 75, 75, 75, 75, 75, 75, 74, 75, 74, 76, 75, 74, 75, 75, 75, 75, 75, 74, 76, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 75, 74, 75, 75, 74, 75, 75, 74, 75, 75, 75, 75, 74, 74, 75, 75, 74, 75, 74, 74, 75, 74, 75, 74, 76, 75, 75, 75, 74, 75, 74, 75, 75, 75, 75, 75, 75, 75, 75, 74, 75, 75, 75, 74, 75, 75, 75, 75, 76, 75, 74, 74, 75, 75, 75, 76, 74, 75, 75, 75, 75, 74, 75, 75, 75, 75, 75, 75, 75, 75, 76, 77, 75, 76, 76, 76, 75, 75, 74, 75, 75, 74, 75, 75, 75, 75, 75, 75, 75, 74, 74, 74, 75, 75, 75, 75, 75, 76, 74, 76, 75, 75, 75, 74, 75, 74, 75, 75, 76, 74, 75, 76, 75, 75, 75, 75, 75, 74, 75, 75, 76, 74, 74, 75, 75, 75, 75, 75, 73, 75, 75, 75, 75, 74, 75, 75, 75, 75, 75, 75, 75, 76, 75, 75, 75, 75, 76, 75, 75, 75, 75, 75, 75, 74, 75, 75, 75, 75, 76, 75, 75, 74, 75, 75, 74, 74, 75, 75, 75, 75, 75, 75, 75, 74, 75, 75, 75, 75, 76, 74, 75, 75, 75, 74, 75, 75, 75, 75, 75, 75, 75, 76, 75, 75, 74, 74, 75, 75, 75, 75, 75, 74, 74, 75, 74, 74, 75, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 75, 75, 75, 75, 74, 75, 74, 74, 75, 74, 75, 75, 74, 74, 75, 74, 74, 75, 74, 74, 74, 74, 75, 76, 77, 75, 75, 74, 75, 75, 75, 75, 74, 75, 74, 75, 75, 75, 76, 75, 74, 75, 76, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 75, 76, 75, 74, 75, 75, 74, 75, 75, 75, 74, 75, 75, 75, 74, 74, 74, 75, 75, 74, 75, 75, 75, 75, 74, 74, 75, 74, 76, 76, 75, 75, 74, 75, 75, 75, 75, 74, 75, 75, 75, 75, 75, 74, 75, 74, 75, 75, 74, 74, 75, 76, 75, 74, 75, 76, 75, 75, 75, 76, 75, 75, 75, 75, 75, 75, 75, 75, 74, 74, 74, 75, 75, 75, 75, 74, 75, 75, 74, 75, 75, 75, 75, 75, 75, 76, 74, 74, 75, 75, 75, 75, 75, 75, 75, 74, 75, 75, 75, 75, 76, 75, 75, 76, 75, 76, 74, 75, 74, 75, 74, 75, 75, 74, 75, 75, 75, 75, 75, 75, 75, 75, 75, 74, 76, 75, 74, 75, 74, 77, 74, 75, 75, 75, 75, 76, 75, 75, 73, 76, 76, 74, 75, 74]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "def getSandZ():\n",
    "    scale_chain = dict()\n",
    "    prev_scale_layer = None\n",
    "    zero_point_chain = dict()\n",
    "    prev_zero_point_layer = None\n",
    "    # fetch network order\n",
    "    for key in myModel.state_dict().keys():\n",
    "        if 'scale' in key:\n",
    "            if prev_scale_layer is not None:\n",
    "                scale_chain[prev_scale_layer] = key\n",
    "            prev_scale_layer = key \n",
    "        elif 'zero_point' in key:\n",
    "            if prev_zero_point_layer is not None:\n",
    "                zero_point_chain[prev_zero_point_layer] = key\n",
    "            prev_zero_point_layer = key\n",
    "        else:\n",
    "            pass\n",
    "    # print(scale_chain)\n",
    "    # print(zero_point_chain)\n",
    "    input_layer_size_list = [(1, 3, 224 + 2, 224 + 2), (1, 64, 224 + 2, 224 + 2), \n",
    "                            (1, 64, 112 + 2, 112 + 2), (1, 128, 112 + 2, 112 + 2), \n",
    "                            (1, 128, 56 + 2, 56 + 2), (1, 256, 56 + 2, 56 + 2), (1, 256, 56 + 2, 56 + 2), \n",
    "                            (1, 256, 28 + 2, 28 + 2), (1, 512, 28 + 2, 28 + 2), (1, 512, 28 + 2, 28 + 2), \n",
    "                            (1, 512, 14 + 2, 14 + 2), (1, 512, 14 + 2, 14 + 2), (1, 512, 14 + 2, 14 + 2)]\n",
    "    input_counter = 0\n",
    "    P = 7091\n",
    "    for key in myModel.state_dict().keys():\n",
    "        if 'features' in key and 'weight' in key:\n",
    "            WeightScaleValue = myModel.state_dict()[key].q_scale()\n",
    "            WeightZeroPointValue = myModel.state_dict()[key].q_zero_point()\n",
    "            InputScaleValue = myModel.state_dict()[f'{key[0:-6]}scale'].item()\n",
    "            InputZeroPointValue = myModel.state_dict()[f'{key[0:-6]}zero_point'].item()\n",
    "            print(key, 'dealing') # , ' ', WeightScaleValue, ' ', WeightZeroPointValue, ' ', InputScaleValue, ' ', InputZeroPointValue)\n",
    "            assert(WeightZeroPointValue == 0)\n",
    "            NextInputScaleName = scale_chain[f'{key[0:-6]}scale']\n",
    "            NextInputScaleValue = myModel.state_dict()[NextInputScaleName].item()\n",
    "            NextInputZeroPointName = zero_point_chain[f'{key[0:-6]}zero_point']\n",
    "            NextInputZeroPointValue = myModel.state_dict()[NextInputZeroPointName].item()\n",
    "            M = WeightScaleValue * InputScaleValue / NextInputScaleValue\n",
    "            assert(M > 0 and M < 1)\n",
    "            NandMo = iter(get_mo(M, P))\n",
    "            yield next(NandMo) # N\n",
    "            Mo = next(NandMo) # Mo\n",
    "            assert(Mo <= 127 and Mo >= -128)\n",
    "            yield Mo\n",
    "            x = myModel.state_dict()[key].dequantize()\n",
    "            # bias = torch.ops.quantized.conv2d(myModel.state_dict()[key],myModel.state_dict()[f'{key[0:-6]}zero_point'], (3, 3), (1, 1), (1, 1), 0, 0, 1)\n",
    "            sub_bias = M * myModel.state_dict()[f'{key[0:-6]}bias'] + NextInputZeroPointValue\n",
    "            bias = torch.conv2d(torch.full(input_layer_size_list[input_counter], M * InputZeroPointValue), x, sub_bias, 1, 0, 1, 1) # no padding, padding zero has been quantified\n",
    "            # print(x.shape, bias.shape, bias[0][0][0][0].item())\n",
    "            bias_quantified_list = []\n",
    "            bias_numpy = bias.detach().numpy()\n",
    "            for channel in range(0, bias_numpy.shape[1]):\n",
    "                bias_quantified_list_toinsert = round(bias_numpy[0][channel][0][0])\n",
    "                # print(bias_numpy[0][channel][0][0], end = ' ')\n",
    "                assert(bias_quantified_list_toinsert <= 127 and bias_quantified_list_toinsert >= -128)\n",
    "                bias_quantified_list.append(bias_quantified_list_toinsert) # bias\n",
    "            # print()\n",
    "            # print(bias_quantified_list)\n",
    "            yield bias_quantified_list\n",
    "            # print(myModel.state_dict()[f'{key[0:-6]}bias'])\n",
    "            # print(NextInputZeroPointValue)\n",
    "            # in_channels = 3\n",
    "            # out_channels = 64\n",
    "            # m = torch.nn.quantized.Conv2d(in_channels, out_channels, 3, stride=1, padding=1, dilation=1, groups=1, bias=False, padding_mode='zeros')\n",
    "            # bias = M * InputZeroPointValue * conv_result + M * myModel.state_dict()[f'{key[0:-6]}bias'] + NextInputZeroPointValue\n",
    "            # bias = M * myModel.state_dict()[key] * InputZeroPointValue + M * myModel.state_dict()[f'{key[0:-6]}bias'] + NextInputZeroPointValue\n",
    "            # print(f'M: {M} bias: {bias}')\n",
    "            input_counter = input_counter + 1\n",
    "        else:\n",
    "            print(key, 'pass')\n",
    "            pass\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "result = iter(getSandZ())\n",
    "f_N = open(\"quan_bin_pertensor/N_perlayer.bin\", 'wb+')\n",
    "f_Mo = open(\"quan_bin_pertensor/Mo_perlayer.bin\", 'wb+')\n",
    "f_bias = open(\"quan_bin_pertensor/bias_perlayer.bin\", 'wb+')\n",
    "for i in range(0, 13):\n",
    "    N = next(result)\n",
    "    Mo = next(result)\n",
    "    bias_quantified_list = next(result)\n",
    "    f_N.write(N.to_bytes(1, byteorder = 'little', signed = True))\n",
    "    f_Mo.write(Mo.to_bytes(1, byteorder = 'little', signed = True))\n",
    "    for item in bias_quantified_list:\n",
    "        f_bias.write(item.to_bytes(1, byteorder = 'little', signed = True))\n",
    "    print(N, '\\t', Mo, '\\t', bias_quantified_list)\n",
    "\n",
    "f_N.close()\n",
    "f_Mo.close()\n",
    "f_bias.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1_1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 196\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconv5_3\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    194\u001b[0m     \u001b[39mreturn\u001b[39;00m conv5_3_ot_data\n\u001b[0;32m--> 196\u001b[0m nn_test(np\u001b[39m.\u001b[39;49mones((\u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m, \u001b[39m224\u001b[39;49m, \u001b[39m224\u001b[39;49m)))\n",
      "Cell \u001b[0;32mIn[6], line 170\u001b[0m, in \u001b[0;36mnn_test\u001b[0;34m(input_data)\u001b[0m\n\u001b[1;32m    168\u001b[0m conv1_1_ot_data \u001b[39m=\u001b[39m conv(input_data,      \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, conv1_1_wt, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, conv1_1_Mo, conv1_1_n, conv1_1_bias)\n\u001b[1;32m    169\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconv1_1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 170\u001b[0m conv1_2_ot_data \u001b[39m=\u001b[39m conv(conv1_1_ot_data, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, conv1_2_wt, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m, conv1_2_Mo, conv1_2_n, conv1_2_bias)\n\u001b[1;32m    171\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mconv1_2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    172\u001b[0m conv2_1_ot_data \u001b[39m=\u001b[39m conv(conv1_2_ot_data, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, conv2_1_wt, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, conv2_1_Mo, conv2_1_n, conv2_1_bias)\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mconv\u001b[0;34m(im_data, im_scale, im_zeropoint, wt_data, wt_scale, wt_zeropoint, om_scale, om_zeropoint, Mo, n, bias_data)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconv\u001b[39m(im_data, im_scale, im_zeropoint, wt_data, wt_scale, wt_zeropoint, om_scale, om_zeropoint, Mo, n, bias_data):\n\u001b[0;32m---> 33\u001b[0m     part1 \u001b[39m=\u001b[39m (Mo \u001b[39m>>\u001b[39m (n)) \u001b[39m*\u001b[39m sy(im_data, wt_data)\n\u001b[1;32m     34\u001b[0m     part2 \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, bias_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], part1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m], part1\u001b[39m.\u001b[39mshape[\u001b[39m3\u001b[39m]))\n\u001b[1;32m     35\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, bias_data\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36msy\u001b[0;34m(im_data, wt_data)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[39mfor\u001b[39;00m width \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[1;32m     19\u001b[0m                     \u001b[39mfor\u001b[39;00m height \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39m3\u001b[39m):\n\u001b[1;32m     20\u001b[0m                         \u001b[39m# try:\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m                             om_data[\u001b[39m0\u001b[39m][kernel][width_base][height_base] \u001b[39m=\u001b[39m om_data[\u001b[39m0\u001b[39m][kernel][width_base][height_base] \u001b[39m+\u001b[39m im_data_padding[\u001b[39m0\u001b[39;49m][channel][width_base \u001b[39m+\u001b[39m width][height_base \u001b[39m+\u001b[39m height] \u001b[39m*\u001b[39m wt_data[kernel][channel][width][height]\n\u001b[1;32m     22\u001b[0m                         \u001b[39m# except:\u001b[39;00m\n\u001b[1;32m     23\u001b[0m                         \u001b[39m#     print(im_data_padding.shape)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m                         \u001b[39m#     print(wt_data.shape)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m                         \u001b[39m#     raise\u001b[39;00m\n\u001b[1;32m     29\u001b[0m             \u001b[39m# om_data[0][kernel][width_base][height_base] = total_im_data\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m om_data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def padding(im_data):\n",
    "    im_data_padding = np.zeros((im_data.shape[0], im_data.shape[1], im_data.shape[2] + 2, im_data.shape[3] + 2))\n",
    "    for batch in range(0, im_data.shape[0]):\n",
    "        for channel in range(0, im_data.shape[1]):\n",
    "            for width in range(0, im_data.shape[2]):\n",
    "                for height in range(0, im_data.shape[3]):\n",
    "                    im_data_padding[batch][channel][width + 1][height + 1] = im_data[batch][channel][width][height]\n",
    "    return im_data_padding\n",
    "\n",
    "def sy(im_data, wt_data):\n",
    "    om_data = np.zeros((1, wt_data.shape[0], im_data.shape[2], im_data.shape[3]))\n",
    "    im_data_padding = padding(im_data)\n",
    "    for kernel in range(0, wt_data.shape[0]):\n",
    "        for width_base in range(0, im_data_padding.shape[2] - 2):\n",
    "            for height_base in range(0, im_data_padding.shape[3] - 2):\n",
    "                # total_im_data = 0\n",
    "                for channel in range(0, im_data_padding.shape[1]):\n",
    "                    for width in range(0, 3):\n",
    "                        for height in range(0, 3):\n",
    "                            # try:\n",
    "                                om_data[0][kernel][width_base][height_base] = om_data[0][kernel][width_base][height_base] + im_data_padding[0][channel][width_base + width][height_base + height] * wt_data[kernel][channel][width][height]\n",
    "                            # except:\n",
    "                            #     print(im_data_padding.shape)\n",
    "                            #     print(wt_data.shape)\n",
    "                            #     print(kernel, '\\t', width_base, '\\t', height_base, '\\t', channel, '\\t', width, '\\t', height)\n",
    "                            #     print(im_data_padding[0][channel][width_base + width][height_base + height])\n",
    "                            #     print(wt_data[kernel][channel][width][height])\n",
    "                            #     raise\n",
    "                # om_data[0][kernel][width_base][height_base] = total_im_data\n",
    "    return om_data\n",
    "\n",
    "def conv(im_data, im_scale, im_zeropoint, wt_data, wt_scale, wt_zeropoint, om_scale, om_zeropoint, Mo, n, bias_data):\n",
    "    part1 = (Mo >> (n)) * sy(im_data, wt_data)\n",
    "    part2 = np.zeros((1, bias_data.shape[0], part1.shape[2], part1.shape[3]))\n",
    "    for i in range(0, bias_data.shape[0]):\n",
    "        for w in range(part1.shape[2]):\n",
    "            for h in range(part1.shape[3]):\n",
    "                part2[0][i][w][h] = bias_data[i]\n",
    "\n",
    "    # part2 = part2_tmp # reshape((1, (bias_data.shape[0], part1.shape[2], part1.shape[3])))\n",
    "    return part1 + part2\n",
    "    \n",
    "def readmodata(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        conv1_1_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)  \n",
    "        conv1_2_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv2_1_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv2_2_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_1_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_2_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_3_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_1_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_2_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_3_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_1_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_2_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_3_Mo = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        f.close()\n",
    "        return [conv1_1_Mo, conv1_2_Mo, conv2_1_Mo, conv2_2_Mo, conv3_1_Mo, conv3_2_Mo, conv3_3_Mo, conv4_1_Mo, conv4_2_Mo, conv4_3_Mo, conv5_1_Mo, conv5_2_Mo, conv5_3_Mo]\n",
    "    \n",
    "\n",
    "def readndata(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        conv1_1_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv1_2_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv2_1_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv2_2_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_1_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_2_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_3_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_1_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_2_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_3_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_1_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_2_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_3_n = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        f.close()\n",
    "        return [conv1_1_n, conv1_2_n, conv2_1_n, conv2_2_n, conv3_1_n, conv3_2_n, conv3_3_n, conv4_1_n, conv4_2_n, conv4_3_n, conv5_1_n, conv5_2_n, conv5_3_n]\n",
    "\n",
    "def readbiasdata(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        conv1_1_bias = np.zeros(64)\n",
    "        for i in range(0, 64):\n",
    "            conv1_1_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv1_2_bias = np.zeros(64)\n",
    "        for i in range(0, 64):\n",
    "            conv1_2_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv2_1_bias = np.zeros(128)\n",
    "        for i in range(0, 128):\n",
    "            conv2_1_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv2_2_bias = np.zeros(128)\n",
    "        for i in range(0, 128):\n",
    "            conv2_2_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_1_bias = np.zeros(256)\n",
    "        for i in range(0, 256):\n",
    "            conv3_1_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_2_bias = np.zeros(256)\n",
    "        for i in range(0, 256):\n",
    "            conv3_2_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv3_3_bias = np.zeros(256)\n",
    "        for i in range(0, 256):\n",
    "            conv3_3_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_1_bias = np.zeros(512)\n",
    "        for i in range(0, 512):\n",
    "            conv4_1_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_2_bias = np.zeros(512)\n",
    "        for i in range(0, 512):\n",
    "            conv4_2_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv4_3_bias = np.zeros(512)\n",
    "        for i in range(0, 512):\n",
    "            conv4_3_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_1_bias = np.zeros(512)\n",
    "        for i in range(0, 512):\n",
    "            conv5_1_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_2_bias = np.zeros(512)\n",
    "        for i in range(0, 512):\n",
    "            conv5_2_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        conv5_3_bias = np.zeros(512)\n",
    "        for i in range(0, 512):\n",
    "            conv5_3_bias[i] = int.from_bytes(f.read(1), byteorder = 'little', signed = True)\n",
    "        f.close()\n",
    "        return [conv1_1_bias, conv1_2_bias, conv2_1_bias, conv2_2_bias, conv3_1_bias, conv3_2_bias, conv3_3_bias, conv4_1_bias, conv4_2_bias, \\\n",
    "                conv4_3_bias, conv5_1_bias, conv5_2_bias, conv5_3_bias]\n",
    "\n",
    "def readwtdata():\n",
    "    conv1_1_wt = None\n",
    "    conv1_2_wt = None\n",
    "    conv2_1_wt = None\n",
    "    conv2_2_wt = None\n",
    "    conv3_1_wt = None\n",
    "    conv3_2_wt = None\n",
    "    conv3_3_wt = None\n",
    "    conv4_1_wt = None\n",
    "    conv4_2_wt = None\n",
    "    conv4_3_wt = None\n",
    "    conv5_1_wt = None\n",
    "    conv5_2_wt = None\n",
    "    conv5_3_wt = None\n",
    "    conv_counter = 0\n",
    "    conv_list = []\n",
    "    for key in myModel.state_dict().keys():\n",
    "        if 'features' in key and 'weight' in key:\n",
    "            conv_list.append(torch.int_repr(myModel.state_dict()[key]).numpy())\n",
    "            # conv_counter = conv_counter + 1\n",
    "    # print(conv1_1_wt)\n",
    "    # print(conv_list)\n",
    "    return conv_list # [conv1_1_wt, conv1_2_wt, conv2_1_wt, conv2_2_wt, conv3_1_wt, conv3_2_wt, conv3_3_wt, conv4_1_wt, conv4_2_wt, conv4_3_wt, conv5_1_wt, conv5_2_wt, conv5_3_wt]\n",
    "\n",
    "def nn_test(input_data):\n",
    "    assert(input_data.shape == (1, 3, 224, 224))\n",
    "    [conv1_1_Mo, conv1_2_Mo, conv2_1_Mo, conv2_2_Mo, conv3_1_Mo, \n",
    "    conv3_2_Mo, conv3_3_Mo, conv4_1_Mo, conv4_2_Mo, conv4_3_Mo, \n",
    "    conv5_1_Mo, conv5_2_Mo, conv5_3_Mo] = readmodata(\"quan_bin_pertensor/Mo_perlayer.bin\")\n",
    "\n",
    "    [conv1_1_n, conv1_2_n, conv2_1_n, conv2_2_n, conv3_1_n, \n",
    "    conv3_2_n, conv3_3_n, conv4_1_n, conv4_2_n, conv4_3_n, \n",
    "    conv5_1_n, conv5_2_n, conv5_3_n] = readndata(\"quan_bin_pertensor/N_perlayer.bin\")\n",
    "\n",
    "    [conv1_1_bias, conv1_2_bias, conv2_1_bias, conv2_2_bias, conv3_1_bias, \n",
    "    conv3_2_bias, conv3_3_bias, conv4_1_bias, conv4_2_bias, conv4_3_bias, \n",
    "    conv5_1_bias, conv5_2_bias, conv5_3_bias] = readbiasdata(\"quan_bin_pertensor/bias_perlayer.bin\")\n",
    "\n",
    "    [conv1_1_wt, conv1_2_wt, conv2_1_wt, conv2_2_wt, conv3_1_wt, \n",
    "     conv3_2_wt, conv3_3_wt, conv4_1_wt, conv4_2_wt, conv4_3_wt, \n",
    "     conv5_1_wt, conv5_2_wt, conv5_3_wt] = readwtdata()\n",
    "    # print(conv1_1_wt.shape)\n",
    "\n",
    "    conv1_1_ot_data = conv(input_data,      None, None, conv1_1_wt, None, None, None, None, conv1_1_Mo, conv1_1_n, conv1_1_bias)\n",
    "    print(\"conv1_1\")\n",
    "    conv1_2_ot_data = conv(conv1_1_ot_data, None, None, conv1_2_wt, None, None, None, None, conv1_2_Mo, conv1_2_n, conv1_2_bias)\n",
    "    print(\"conv1_2\")\n",
    "    conv2_1_ot_data = conv(conv1_2_ot_data, None, None, conv2_1_wt, None, None, None, None, conv2_1_Mo, conv2_1_n, conv2_1_bias)\n",
    "    print(\"conv2_1\")\n",
    "    conv2_2_ot_data = conv(conv2_1_ot_data, None, None, conv2_2_wt, None, None, None, None, conv2_2_Mo, conv2_2_n, conv2_2_bias)\n",
    "    print(\"conv2_2\")\n",
    "    conv3_1_ot_data = conv(conv2_2_ot_data, None, None, conv3_1_wt, None, None, None, None, conv3_1_Mo, conv3_1_n, conv3_1_bias)\n",
    "    print(\"conv3_1\")\n",
    "    conv3_2_ot_data = conv(conv3_1_ot_data, None, None, conv3_2_wt, None, None, None, None, conv3_2_Mo, conv3_2_n, conv3_2_bias)\n",
    "    print(\"conv3_2\")\n",
    "    conv3_3_ot_data = conv(conv3_2_ot_data, None, None, conv3_3_wt, None, None, None, None, conv3_3_Mo, conv3_3_n, conv3_3_bias)\n",
    "    print(\"conv3_3\")\n",
    "    conv4_1_ot_data = conv(conv3_3_ot_data, None, None, conv4_1_wt, None, None, None, None, conv4_1_Mo, conv4_1_n, conv4_1_bias)\n",
    "    print(\"conv4_1\")\n",
    "    conv4_2_ot_data = conv(conv4_1_ot_data, None, None, conv4_2_wt, None, None, None, None, conv4_2_Mo, conv4_2_n, conv4_2_bias)\n",
    "    print(\"conv4_2\")\n",
    "    conv4_3_ot_data = conv(conv4_2_ot_data, None, None, conv4_3_wt, None, None, None, None, conv4_3_Mo, conv4_3_n, conv4_3_bias)\n",
    "    print(\"conv4_3\")\n",
    "    conv5_1_ot_data = conv(conv4_3_ot_data, None, None, conv5_1_wt, None, None, None, None, conv5_1_Mo, conv5_1_n, conv5_1_bias)\n",
    "    print(\"conv5_1\")\n",
    "    conv5_2_ot_data = conv(conv5_1_ot_data, None, None, conv5_2_wt, None, None, None, None, conv5_2_Mo, conv5_2_n, conv5_2_bias)\n",
    "    print(\"conv5_2\")\n",
    "    conv5_3_ot_data = conv(conv5_2_ot_data, None, None, conv5_3_wt, None, None, None, None, conv5_3_Mo, conv5_3_n, conv5_3_bias)\n",
    "    print(\"conv5_3\")\n",
    "    return conv5_3_ot_data\n",
    "\n",
    "nn_test(np.ones((1, 3, 224, 224)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.0.weight dealing\n",
      "[69, 69, 67, 80, 68, 74, 67, 89, 69, 66, 47, 67, 33, 67, 67, 67, 62, 66, 65, 68, 67, 68, 77, 67, 69, 65, 68, 57, 67, 55, 68, 68, 68, 67, 68, 44, 67, 48, 64, 68, 67, 85, 68, 74, 42, 53, 67, 68, 66, 65, 71, 67, 92, 67, 87, 68, 68, 68, 67, 69, 55, 64, 62, 66]\n",
      "features.0.scale pass\n",
      "features.0.zero_point pass\n",
      "features.0.bias pass\n",
      "features.2.weight dealing\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 66\u001b[0m\n\u001b[1;32m     64\u001b[0m     current_layer_bias_quant_data_requant \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(current_layer_bias_quant_data \u001b[39m*\u001b[39m Mo) \u001b[39m>>\u001b[39m N\n\u001b[1;32m     65\u001b[0m     total_bias_quant_data \u001b[39m=\u001b[39m current_layer_bias_quant_data_requant \u001b[39m+\u001b[39m NextInputZeroPointValue\n\u001b[0;32m---> 66\u001b[0m     \u001b[39massert\u001b[39;00m(total_bias_quant_data \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m127\u001b[39m \u001b[39mand\u001b[39;00m total_bias_quant_data \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m128\u001b[39m)\n\u001b[1;32m     67\u001b[0m     weight_quant_data_channel_sum_list\u001b[39m.\u001b[39mappend(total_bias_quant_data)\n\u001b[1;32m     68\u001b[0m \u001b[39mprint\u001b[39m(weight_quant_data_channel_sum_list)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise\n",
    "    \n",
    "# def getSandZ():\n",
    "scale_chain = dict()\n",
    "prev_scale_layer = None\n",
    "zero_point_chain = dict()\n",
    "prev_zero_point_layer = None\n",
    "# fetch network order\n",
    "for key in myModel.state_dict().keys():\n",
    "    if 'scale' in key:\n",
    "        if prev_scale_layer is not None:\n",
    "            scale_chain[prev_scale_layer] = key\n",
    "        prev_scale_layer = key \n",
    "    elif 'zero_point' in key:\n",
    "        if prev_zero_point_layer is not None:\n",
    "            zero_point_chain[prev_zero_point_layer] = key\n",
    "        prev_zero_point_layer = key\n",
    "    else:\n",
    "        pass\n",
    "# print(scale_chain)\n",
    "# print(zero_point_chain)\n",
    "input_layer_size_list = [(1, 3, 224 + 2, 224 + 2), (1, 64, 224 + 2, 224 + 2), \n",
    "                         (1, 64, 112 + 2, 112 + 2), (1, 128, 112 + 2, 112 + 2), \n",
    "                         (1, 128, 56 + 2, 56 + 2), (1, 256, 56 + 2, 56 + 2), (1, 256, 56 + 2, 56 + 2), \n",
    "                         (1, 256, 28 + 2, 28 + 2), (1, 512, 28 + 2, 28 + 2), (1, 512, 28 + 2, 28 + 2), \n",
    "                         (1, 512, 14 + 2, 14 + 2), (1, 512, 14 + 2, 14 + 2), (1, 512, 14 + 2, 14 + 2)]\n",
    "input_counter = 0\n",
    "P = 7091\n",
    "for key in myModel.state_dict().keys():\n",
    "    if 'features' in key and 'weight' in key:\n",
    "        WeightScaleValue = myModel.state_dict()[key].q_scale()\n",
    "        WeightZeroPointValue = myModel.state_dict()[key].q_zero_point()\n",
    "        InputScaleValue = myModel.state_dict()[f'{key[0:-6]}scale'].item()\n",
    "        InputZeroPointValue = myModel.state_dict()[f'{key[0:-6]}zero_point'].item()\n",
    "        print(key, 'dealing') # , ' ', WeightScaleValue, ' ', WeightZeroPointValue, ' ', InputScaleValue, ' ', InputZeroPointValue)\n",
    "        assert(WeightZeroPointValue == 0)\n",
    "        NextInputScaleName = scale_chain[f'{key[0:-6]}scale']\n",
    "        NextInputScaleValue = myModel.state_dict()[NextInputScaleName].item()\n",
    "        NextInputZeroPointName = zero_point_chain[f'{key[0:-6]}zero_point']\n",
    "        NextInputZeroPointValue = myModel.state_dict()[NextInputZeroPointName].item()\n",
    "        M = WeightScaleValue * InputScaleValue / NextInputScaleValue\n",
    "        assert(M > 0 and M < 1)\n",
    "        NandMo = iter(get_mo(M, P))\n",
    "        N = next(NandMo) # N\n",
    "        Mo = next(NandMo) # Mo\n",
    "        # x = myModel.state_dict()[key].dequantize()\n",
    "        # bias = torch.ops.quantized.conv2d(myModel.state_dict()[key],myModel.state_dict()[f'{key[0:-6]}zero_point'], (3, 3), (1, 1), (1, 1), 0, 0, 1)\n",
    "        # sub_bias = M * myModel.state_dict()[f'{key[0:-6]}bias'] + NextInputZeroPointValue\n",
    "        # bias = torch.conv2d(torch.full(input_layer_size_list[input_counter], M * InputZeroPointValue), x, sub_bias, 1, 0, 1, 1) # no padding, padding zero has been quantified\n",
    "        # bias_quantified_list = []\n",
    "        # bias_numpy = bias.detach().numpy()\n",
    "        # for channel in range(0, bias_numpy.shape[1]):\n",
    "        #     bias_quantified_list.append(bias_numpy[0][channel][0][0]) # bias\n",
    "        # print(bias_quantified_list)\n",
    "\n",
    "\n",
    "\n",
    "        weight_quant_data = torch.int_repr(myModel.state_dict()[key]).numpy()\n",
    "        bias_quant_data = myModel.state_dict()[f'{key[0:-6]}bias'].detach().numpy()\n",
    "        weight_quant_data_channel_sum_list = list()\n",
    "        for kernel in range(0, weight_quant_data.shape[0]):\n",
    "            weight_quant_data_channel_sum = 0\n",
    "            for channel in range(0, weight_quant_data.shape[1]):\n",
    "                for width in range(3):\n",
    "                    for height in range(3):\n",
    "                        weight_quant_data_channel_sum = weight_quant_data_channel_sum + weight_quant_data[kernel][channel][width][height]\n",
    "            current_layer_bias_quant_data = InputZeroPointValue * weight_quant_data_channel_sum + bias_quant_data[kernel]\n",
    "           \n",
    "            current_layer_bias_quant_data_requant = int(current_layer_bias_quant_data * Mo) >> N\n",
    "            total_bias_quant_data = current_layer_bias_quant_data_requant + NextInputZeroPointValue\n",
    "            assert(total_bias_quant_data <= 127 and total_bias_quant_data >= -128)\n",
    "            weight_quant_data_channel_sum_list.append(total_bias_quant_data)\n",
    "        print(weight_quant_data_channel_sum_list)\n",
    "\n",
    "\n",
    "\n",
    "        # yield bias_quantified_list\n",
    "        # print(f'M: {M} bias: {bias}')\n",
    "        input_counter = input_counter + 1\n",
    "    else:\n",
    "        print(key, 'pass')\n",
    "        pass\n",
    "# return\n",
    "\n",
    "# result = iter(getSandZ())\n",
    "# next(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "model_int8 conv keys: odict_keys(['features.0.weight', 'features.0.bias', 'features.0.scale', 'features.0.zero_point', 'features.2.weight', 'features.2.bias', 'features.2.scale', 'features.2.zero_point', 'features.5.weight', 'features.5.bias', 'features.5.scale', 'features.5.zero_point', 'features.7.weight', 'features.7.bias', 'features.7.scale', 'features.7.zero_point', 'features.10.weight', 'features.10.bias', 'features.10.scale', 'features.10.zero_point', 'features.12.weight', 'features.12.bias', 'features.12.scale', 'features.12.zero_point', 'features.14.weight', 'features.14.bias', 'features.14.scale', 'features.14.zero_point', 'features.17.weight', 'features.17.bias', 'features.17.scale', 'features.17.zero_point', 'features.19.weight', 'features.19.bias', 'features.19.scale', 'features.19.zero_point', 'features.21.weight', 'features.21.bias', 'features.21.scale', 'features.21.zero_point', 'features.24.weight', 'features.24.bias', 'features.24.scale', 'features.24.zero_point', 'features.26.weight', 'features.26.bias', 'features.26.scale', 'features.26.zero_point', 'features.28.weight', 'features.28.bias', 'features.28.scale', 'features.28.zero_point', 'classifier.0.scale', 'classifier.0.zero_point', 'classifier.0._packed_params.dtype', 'classifier.0._packed_params._packed_params', 'classifier.3.scale', 'classifier.3.zero_point', 'classifier.3._packed_params.dtype', 'classifier.3._packed_params._packed_params', 'classifier.6.scale', 'classifier.6.zero_point', 'classifier.6._packed_params.dtype', 'classifier.6._packed_params._packed_params', 'quant.scale', 'quant.zero_point'])\n",
      "\n",
      "\n",
      "model_int8 features.0.weight: tensor([[[[-0.5489,  0.1397,  0.5290],\n",
      "          [-0.5789,  0.3593,  0.7685],\n",
      "          [-0.6887, -0.0499,  0.4891]],\n",
      "\n",
      "         [[ 0.1797,  0.0100, -0.0798],\n",
      "          [ 0.0399, -0.0699, -0.2595],\n",
      "          [ 0.1298, -0.1697, -0.1298]],\n",
      "\n",
      "         [[ 0.3094, -0.1697, -0.4292],\n",
      "          [ 0.4791, -0.0798, -0.4891],\n",
      "          [ 0.6288,  0.0200, -0.2795]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2296,  0.1298,  0.1896],\n",
      "          [-0.4292, -0.2395,  0.2495],\n",
      "          [-0.2495,  0.1397, -0.0100]],\n",
      "\n",
      "         [[-0.1397, -0.2196,  0.1497],\n",
      "          [-0.8384, -0.3493,  0.5689],\n",
      "          [-0.2395,  0.5190,  0.5390]],\n",
      "\n",
      "         [[-0.3094, -0.3693, -0.1298],\n",
      "          [-0.4691, -0.1597,  0.3493],\n",
      "          [ 0.0499,  0.5889,  0.4990]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1797,  0.5190,  0.0100],\n",
      "          [-0.2695, -0.7186,  0.3094],\n",
      "          [-0.0798, -0.2196,  0.3394]],\n",
      "\n",
      "         [[ 0.3094,  0.6687,  0.0200],\n",
      "          [-0.4691, -1.0680,  0.3394],\n",
      "          [-0.0798, -0.3094,  0.5489]],\n",
      "\n",
      "         [[ 0.3194,  0.4192, -0.3493],\n",
      "          [ 0.0898, -0.4691,  0.0100],\n",
      "          [ 0.1098, -0.1497, -0.0200]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.0798,  0.1298,  0.0299],\n",
      "          [ 0.2196,  0.2495, -0.0499],\n",
      "          [ 0.0499,  0.0299,  0.0200]],\n",
      "\n",
      "         [[-0.1797, -0.0699, -0.0100],\n",
      "          [-0.0499,  0.0100, -0.1298],\n",
      "          [-0.0599, -0.0599,  0.0399]],\n",
      "\n",
      "         [[-0.2296, -0.1198, -0.0200],\n",
      "          [-0.0998, -0.0200,  0.0000],\n",
      "          [-0.0299,  0.0000,  0.1397]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0200, -0.0299,  0.0000],\n",
      "          [-0.0699, -0.1896, -0.1397],\n",
      "          [-0.0699, -0.1797, -0.1697]],\n",
      "\n",
      "         [[ 0.0399, -0.0699, -0.0100],\n",
      "          [ 0.0100, -0.1497, -0.1198],\n",
      "          [ 0.0100, -0.0998, -0.1198]],\n",
      "\n",
      "         [[ 0.1298,  0.0898,  0.1298],\n",
      "          [ 0.1797,  0.1098,  0.1198],\n",
      "          [ 0.1497,  0.0998,  0.0998]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0299, -0.1098, -0.2595],\n",
      "          [ 0.2795, -0.0399, -0.2595],\n",
      "          [ 0.3493,  0.0299, -0.0599]],\n",
      "\n",
      "         [[ 0.2495,  0.1597, -0.1697],\n",
      "          [ 0.3893,  0.0299, -0.3493],\n",
      "          [ 0.1896, -0.1996, -0.2994]],\n",
      "\n",
      "         [[ 0.4591,  0.4292,  0.2795],\n",
      "          [ 0.1597, -0.0599, -0.1896],\n",
      "          [-0.1996, -0.4591, -0.4292]]]], size=(64, 3, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.009980898350477219, zero_point=0)\n",
      "\n",
      "\n",
      "model_int8 features.0.bias: Parameter containing:\n",
      "tensor([ 0.4034,  0.3778,  0.4644, -0.3228,  0.3940, -0.3953,  0.3951, -0.5496,\n",
      "         0.2693, -0.7602, -0.3508,  0.2334, -1.3239, -0.1694,  0.3938, -0.1026,\n",
      "         0.0460, -0.6995,  0.1549,  0.5628,  0.3011,  0.3425,  0.1073,  0.4651,\n",
      "         0.1295,  0.0788, -0.0492, -0.5638,  0.1465, -0.3890, -0.0715,  0.0649,\n",
      "         0.2768,  0.3279,  0.5682, -1.2640, -0.8368, -0.9485,  0.1358,  0.2727,\n",
      "         0.1841, -0.5325,  0.3507, -0.0827, -1.0248, -0.6912, -0.7711,  0.2612,\n",
      "         0.4033, -0.4802, -0.3066,  0.5807, -1.3325,  0.4844, -0.8160,  0.2386,\n",
      "         0.2300,  0.4979,  0.5553,  0.5230, -0.2182,  0.0117, -0.5516,  0.2108],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "model_int8 features.0.scale: tensor(0.3098)\n",
      "\n",
      "\n",
      "model_int8 features.0.zero_point: tensor(61)\n",
      "\n",
      "\n",
      "model_int8 features.2.weight: tensor([[[[-0.0296, -0.0973, -0.1311],\n",
      "          [ 0.0085, -0.0846, -0.1650],\n",
      "          [ 0.0296, -0.0677, -0.1311]],\n",
      "\n",
      "         [[ 0.0465, -0.0296, -0.0508],\n",
      "          [ 0.0719,  0.0085, -0.0169],\n",
      "          [ 0.0719,  0.0381,  0.0169]],\n",
      "\n",
      "         [[ 0.0719,  0.0042, -0.0465],\n",
      "          [ 0.0846,  0.0508,  0.0000],\n",
      "          [ 0.0085,  0.0212,  0.0042]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0296,  0.0212, -0.0085],\n",
      "          [ 0.0254,  0.0042, -0.0338],\n",
      "          [ 0.0212,  0.0381, -0.0127]],\n",
      "\n",
      "         [[ 0.0212,  0.0423,  0.0592],\n",
      "          [ 0.0254,  0.0381,  0.0338],\n",
      "          [-0.0085,  0.0212,  0.0508]],\n",
      "\n",
      "         [[ 0.0212, -0.0212, -0.0973],\n",
      "          [-0.0592, -0.0719, -0.0761],\n",
      "          [-0.0381, -0.0254, -0.0042]]],\n",
      "\n",
      "\n",
      "        [[[-0.0127, -0.0761, -0.1354],\n",
      "          [-0.0381, -0.0804, -0.1438],\n",
      "          [-0.0423, -0.1058, -0.1607]],\n",
      "\n",
      "         [[-0.0085,  0.0465,  0.0169],\n",
      "          [ 0.0169,  0.0085, -0.0169],\n",
      "          [ 0.0000, -0.0338, -0.0338]],\n",
      "\n",
      "         [[ 0.0127, -0.0127, -0.0465],\n",
      "          [-0.0169,  0.0169, -0.0804],\n",
      "          [ 0.0169,  0.0042,  0.0085]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0423,  0.0381,  0.0254],\n",
      "          [ 0.0169,  0.0042, -0.0127],\n",
      "          [ 0.0254,  0.0169, -0.0254]],\n",
      "\n",
      "         [[-0.0212,  0.0042, -0.0212],\n",
      "          [ 0.0000, -0.0085,  0.0169],\n",
      "          [-0.0042, -0.0212, -0.0212]],\n",
      "\n",
      "         [[-0.0296, -0.0254,  0.0254],\n",
      "          [-0.0592, -0.0550,  0.0719],\n",
      "          [-0.0635, -0.0381,  0.0423]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0127,  0.0254,  0.0550],\n",
      "          [-0.0254, -0.0169,  0.0296],\n",
      "          [ 0.0127,  0.0085,  0.0296]],\n",
      "\n",
      "         [[-0.0804, -0.0888, -0.0550],\n",
      "          [-0.0465, -0.1100, -0.0127],\n",
      "          [ 0.0338,  0.0254,  0.1100]],\n",
      "\n",
      "         [[-0.0423, -0.0169, -0.0085],\n",
      "          [ 0.0296,  0.0465,  0.0169],\n",
      "          [ 0.0888,  0.1269,  0.0719]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0169, -0.0169],\n",
      "          [-0.0085, -0.0127,  0.0000],\n",
      "          [ 0.0085,  0.0000, -0.0338]],\n",
      "\n",
      "         [[-0.0085, -0.0042,  0.0000],\n",
      "          [ 0.0000, -0.0042,  0.0085],\n",
      "          [ 0.0296,  0.0169,  0.0338]],\n",
      "\n",
      "         [[ 0.0592, -0.0085,  0.0381],\n",
      "          [-0.0212, -0.0508,  0.0423],\n",
      "          [ 0.0212,  0.0085,  0.0338]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-0.0465, -0.0296,  0.0085],\n",
      "          [-0.0465, -0.0508,  0.0169],\n",
      "          [-0.0423, -0.0254, -0.0254]],\n",
      "\n",
      "         [[-0.0212,  0.0169, -0.0635],\n",
      "          [ 0.0296,  0.0296, -0.0635],\n",
      "          [-0.0127,  0.0042, -0.0635]],\n",
      "\n",
      "         [[-0.0296, -0.0212, -0.0508],\n",
      "          [ 0.0127, -0.0508, -0.0550],\n",
      "          [-0.0381,  0.0000,  0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0000,  0.0000, -0.0085],\n",
      "          [ 0.0000,  0.0085,  0.0169],\n",
      "          [ 0.0127,  0.0085,  0.0338]],\n",
      "\n",
      "         [[ 0.0085,  0.0338,  0.0381],\n",
      "          [-0.0042,  0.0000,  0.0127],\n",
      "          [-0.0042,  0.0085,  0.0296]],\n",
      "\n",
      "         [[-0.0296, -0.0677, -0.0761],\n",
      "          [-0.0381, -0.0085,  0.0635],\n",
      "          [-0.0169,  0.0592,  0.1269]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0423,  0.0381,  0.0169],\n",
      "          [ 0.0423,  0.0296,  0.0169],\n",
      "          [-0.0042,  0.0212, -0.0042]],\n",
      "\n",
      "         [[ 0.0296, -0.0169,  0.0127],\n",
      "          [-0.0592, -0.0465, -0.0296],\n",
      "          [-0.0296,  0.0169, -0.0127]],\n",
      "\n",
      "         [[-0.0973, -0.0973, -0.1311],\n",
      "          [ 0.0719, -0.0085, -0.0761],\n",
      "          [ 0.1692,  0.1819,  0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0254,  0.0338,  0.0042],\n",
      "          [-0.0042,  0.0212,  0.0169],\n",
      "          [-0.0042, -0.0296,  0.0000]],\n",
      "\n",
      "         [[ 0.0085, -0.0042,  0.0042],\n",
      "          [ 0.0000,  0.0127,  0.0212],\n",
      "          [ 0.0042, -0.0127,  0.0042]],\n",
      "\n",
      "         [[ 0.0338,  0.0127,  0.0296],\n",
      "          [-0.0085,  0.0592,  0.0338],\n",
      "          [-0.0212,  0.0169, -0.0127]]],\n",
      "\n",
      "\n",
      "        [[[-0.0677,  0.0127,  0.0804],\n",
      "          [ 0.0085, -0.0338,  0.0508],\n",
      "          [-0.0381, -0.0296,  0.0635]],\n",
      "\n",
      "         [[ 0.0550, -0.1904,  0.1015],\n",
      "          [-0.1861,  0.0127,  0.2496],\n",
      "          [-0.0042,  0.0296, -0.0888]],\n",
      "\n",
      "         [[-0.1481, -0.1311,  0.1777],\n",
      "          [-0.1565,  0.1523,  0.0846],\n",
      "          [ 0.0931,  0.0846, -0.0338]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.0127,  0.0338, -0.0085],\n",
      "          [ 0.0508, -0.0085, -0.0338],\n",
      "          [ 0.0127, -0.0296,  0.0254]],\n",
      "\n",
      "         [[ 0.0296,  0.0296,  0.0042],\n",
      "          [ 0.0042, -0.0169, -0.0212],\n",
      "          [-0.0085, -0.0381, -0.0212]],\n",
      "\n",
      "         [[ 0.0254,  0.0465,  0.0677],\n",
      "          [ 0.0127,  0.0338, -0.0592],\n",
      "          [ 0.0254, -0.0931, -0.0423]]]], size=(64, 64, 3, 3),\n",
      "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
      "       scale=0.004230252001434565, zero_point=0)\n",
      "\n",
      "\n",
      "model_int8 features.2.bias: Parameter containing:\n",
      "tensor([ 0.0020, -0.0902,  0.6164, -0.0818,  0.2450, -0.0488,  0.1307, -0.0290,\n",
      "        -0.1429,  0.3068, -0.0399, -0.2524,  0.0999, -0.2326,  0.0353, -0.0904,\n",
      "         0.1138, -0.0307, -0.0108, -0.0215,  0.0554,  0.1382,  0.0362, -0.4511,\n",
      "         0.0056, -0.0246, -0.4296, -0.1458,  0.3813, -0.0359,  0.1184, -0.3527,\n",
      "        -0.0239, -0.0235,  0.6499, -0.0634, -0.0152, -0.2285,  0.0941, -0.5053,\n",
      "         0.1906,  0.0944,  0.3406, -0.0833,  0.1924, -0.1953, -0.0421, -0.1606,\n",
      "         0.3964,  0.2068,  0.1812, -0.1198, -0.0724, -0.1240,  0.1313,  0.1043,\n",
      "         0.5469,  0.5208,  0.0509, -0.8278,  0.4372, -0.3734, -0.3264, -0.1213],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "model_int8 features.2.scale: tensor(0.8419)\n",
      "\n",
      "\n",
      "model_int8 features.2.zero_point: tensor(70)\n",
      "\n",
      "\n",
      "model_int8 quant.scale: tensor([0.0375])\n",
      "\n",
      "\n",
      "model_int8 quant.zero_point: tensor([57])\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\nmodel_int8 conv keys:',myModel.state_dict().keys())\n",
    "print('\\n\\nmodel_int8 features.0.weight:',myModel.state_dict()['features.0.weight'])\n",
    "print('\\n\\nmodel_int8 features.0.bias:',myModel.state_dict()['features.0.bias'])\n",
    "print('\\n\\nmodel_int8 features.0.scale:',myModel.state_dict()['features.0.scale'])\n",
    "print('\\n\\nmodel_int8 features.0.zero_point:',myModel.state_dict()['features.0.zero_point'])\n",
    "\n",
    "print('\\n\\nmodel_int8 features.2.weight:',myModel.state_dict()['features.2.weight'])\n",
    "print('\\n\\nmodel_int8 features.2.bias:',myModel.state_dict()['features.2.bias'])\n",
    "print('\\n\\nmodel_int8 features.2.scale:',myModel.state_dict()['features.2.scale'])\n",
    "print('\\n\\nmodel_int8 features.2.zero_point:',myModel.state_dict()['features.2.zero_point'])\n",
    "\n",
    "print('\\n\\nmodel_int8 quant.scale:',myModel.state_dict()['quant.scale'])\n",
    "print('\\n\\nmodel_int8 quant.zero_point:',myModel.state_dict()['quant.zero_point'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "conv_weight0 tensor([[[[ -55,   14,   53],\n",
      "          [ -58,   36,   77],\n",
      "          [ -69,   -5,   49]],\n",
      "\n",
      "         [[  18,    1,   -8],\n",
      "          [   4,   -7,  -26],\n",
      "          [  13,  -17,  -13]],\n",
      "\n",
      "         [[  31,  -17,  -43],\n",
      "          [  48,   -8,  -49],\n",
      "          [  63,    2,  -28]]],\n",
      "\n",
      "\n",
      "        [[[  23,   13,   19],\n",
      "          [ -43,  -24,   25],\n",
      "          [ -25,   14,   -1]],\n",
      "\n",
      "         [[ -14,  -22,   15],\n",
      "          [ -84,  -35,   57],\n",
      "          [ -24,   52,   54]],\n",
      "\n",
      "         [[ -31,  -37,  -13],\n",
      "          [ -47,  -16,   35],\n",
      "          [   5,   59,   50]]],\n",
      "\n",
      "\n",
      "        [[[  18,   52,    1],\n",
      "          [ -27,  -72,   31],\n",
      "          [  -8,  -22,   34]],\n",
      "\n",
      "         [[  31,   67,    2],\n",
      "          [ -47, -107,   34],\n",
      "          [  -8,  -31,   55]],\n",
      "\n",
      "         [[  32,   42,  -35],\n",
      "          [   9,  -47,    1],\n",
      "          [  11,  -15,   -2]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[   8,   13,    3],\n",
      "          [  22,   25,   -5],\n",
      "          [   5,    3,    2]],\n",
      "\n",
      "         [[ -18,   -7,   -1],\n",
      "          [  -5,    1,  -13],\n",
      "          [  -6,   -6,    4]],\n",
      "\n",
      "         [[ -23,  -12,   -2],\n",
      "          [ -10,   -2,    0],\n",
      "          [  -3,    0,   14]]],\n",
      "\n",
      "\n",
      "        [[[   2,   -3,    0],\n",
      "          [  -7,  -19,  -14],\n",
      "          [  -7,  -18,  -17]],\n",
      "\n",
      "         [[   4,   -7,   -1],\n",
      "          [   1,  -15,  -12],\n",
      "          [   1,  -10,  -12]],\n",
      "\n",
      "         [[  13,    9,   13],\n",
      "          [  18,   11,   12],\n",
      "          [  15,   10,   10]]],\n",
      "\n",
      "\n",
      "        [[[   3,  -11,  -26],\n",
      "          [  28,   -4,  -26],\n",
      "          [  35,    3,   -6]],\n",
      "\n",
      "         [[  25,   16,  -17],\n",
      "          [  39,    3,  -35],\n",
      "          [  19,  -20,  -30]],\n",
      "\n",
      "         [[  46,   43,   28],\n",
      "          [  16,   -6,  -19],\n",
      "          [ -20,  -46,  -43]]]], dtype=torch.int8)\n",
      "\n",
      "\n",
      "conv_weight2 tensor([[[[ -7, -23, -31],\n",
      "          [  2, -20, -39],\n",
      "          [  7, -16, -31]],\n",
      "\n",
      "         [[ 11,  -7, -12],\n",
      "          [ 17,   2,  -4],\n",
      "          [ 17,   9,   4]],\n",
      "\n",
      "         [[ 17,   1, -11],\n",
      "          [ 20,  12,   0],\n",
      "          [  2,   5,   1]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  7,   5,  -2],\n",
      "          [  6,   1,  -8],\n",
      "          [  5,   9,  -3]],\n",
      "\n",
      "         [[  5,  10,  14],\n",
      "          [  6,   9,   8],\n",
      "          [ -2,   5,  12]],\n",
      "\n",
      "         [[  5,  -5, -23],\n",
      "          [-14, -17, -18],\n",
      "          [ -9,  -6,  -1]]],\n",
      "\n",
      "\n",
      "        [[[ -3, -18, -32],\n",
      "          [ -9, -19, -34],\n",
      "          [-10, -25, -38]],\n",
      "\n",
      "         [[ -2,  11,   4],\n",
      "          [  4,   2,  -4],\n",
      "          [  0,  -8,  -8]],\n",
      "\n",
      "         [[  3,  -3, -11],\n",
      "          [ -4,   4, -19],\n",
      "          [  4,   1,   2]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 10,   9,   6],\n",
      "          [  4,   1,  -3],\n",
      "          [  6,   4,  -6]],\n",
      "\n",
      "         [[ -5,   1,  -5],\n",
      "          [  0,  -2,   4],\n",
      "          [ -1,  -5,  -5]],\n",
      "\n",
      "         [[ -7,  -6,   6],\n",
      "          [-14, -13,  17],\n",
      "          [-15,  -9,  10]]],\n",
      "\n",
      "\n",
      "        [[[  3,   6,  13],\n",
      "          [ -6,  -4,   7],\n",
      "          [  3,   2,   7]],\n",
      "\n",
      "         [[-19, -21, -13],\n",
      "          [-11, -26,  -3],\n",
      "          [  8,   6,  26]],\n",
      "\n",
      "         [[-10,  -4,  -2],\n",
      "          [  7,  11,   4],\n",
      "          [ 21,  30,  17]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,   4,  -4],\n",
      "          [ -2,  -3,   0],\n",
      "          [  2,   0,  -8]],\n",
      "\n",
      "         [[ -2,  -1,   0],\n",
      "          [  0,  -1,   2],\n",
      "          [  7,   4,   8]],\n",
      "\n",
      "         [[ 14,  -2,   9],\n",
      "          [ -5, -12,  10],\n",
      "          [  5,   2,   8]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-11,  -7,   2],\n",
      "          [-11, -12,   4],\n",
      "          [-10,  -6,  -6]],\n",
      "\n",
      "         [[ -5,   4, -15],\n",
      "          [  7,   7, -15],\n",
      "          [ -3,   1, -15]],\n",
      "\n",
      "         [[ -7,  -5, -12],\n",
      "          [  3, -12, -13],\n",
      "          [ -9,   0,   8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  0,   0,  -2],\n",
      "          [  0,   2,   4],\n",
      "          [  3,   2,   8]],\n",
      "\n",
      "         [[  2,   8,   9],\n",
      "          [ -1,   0,   3],\n",
      "          [ -1,   2,   7]],\n",
      "\n",
      "         [[ -7, -16, -18],\n",
      "          [ -9,  -2,  15],\n",
      "          [ -4,  14,  30]]],\n",
      "\n",
      "\n",
      "        [[[ 10,   9,   4],\n",
      "          [ 10,   7,   4],\n",
      "          [ -1,   5,  -1]],\n",
      "\n",
      "         [[  7,  -4,   3],\n",
      "          [-14, -11,  -7],\n",
      "          [ -7,   4,  -3]],\n",
      "\n",
      "         [[-23, -23, -31],\n",
      "          [ 17,  -2, -18],\n",
      "          [ 40,  43,   8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  6,   8,   1],\n",
      "          [ -1,   5,   4],\n",
      "          [ -1,  -7,   0]],\n",
      "\n",
      "         [[  2,  -1,   1],\n",
      "          [  0,   3,   5],\n",
      "          [  1,  -3,   1]],\n",
      "\n",
      "         [[  8,   3,   7],\n",
      "          [ -2,  14,   8],\n",
      "          [ -5,   4,  -3]]],\n",
      "\n",
      "\n",
      "        [[[-16,   3,  19],\n",
      "          [  2,  -8,  12],\n",
      "          [ -9,  -7,  15]],\n",
      "\n",
      "         [[ 13, -45,  24],\n",
      "          [-44,   3,  59],\n",
      "          [ -1,   7, -21]],\n",
      "\n",
      "         [[-35, -31,  42],\n",
      "          [-37,  36,  20],\n",
      "          [ 22,  20,  -8]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[  3,   8,  -2],\n",
      "          [ 12,  -2,  -8],\n",
      "          [  3,  -7,   6]],\n",
      "\n",
      "         [[  7,   7,   1],\n",
      "          [  1,  -4,  -5],\n",
      "          [ -2,  -9,  -5]],\n",
      "\n",
      "         [[  6,  11,  16],\n",
      "          [  3,   8, -14],\n",
      "          [  6, -22, -10]]]], dtype=torch.int8)\n",
      "Size of model after quantization\n",
      "Size (MB): 138.416915\n",
      "..........Evaluation accuracy on 300 images, 78.00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "conv_weight0 = myModel.state_dict()['features.0.weight']\n",
    "conv_weight0.int_repr()\n",
    "print('\\n\\nconv_weight0',conv_weight0.int_repr())\n",
    "\n",
    "conv_weight2 = myModel.state_dict()['features.2.weight']\n",
    "conv_weight2.int_repr()\n",
    "print('\\n\\nconv_weight2',conv_weight2.int_repr())\n",
    "\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, data_loader_test, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(myModel), saved_model_dir + scripted_default_quantized_model_file) # save default_quantized model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    5. optimal\n",
    "    ·Quantizes weights on a per-channel basis\n",
    "    ·Uses a histogram observer that collects a histogram of activations and then picks quantization parameters\n",
    "    in an optimal manner.\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "per_channel_quantized_model = load_model(saved_model_dir + float_model_file)\n",
    "per_channel_quantized_model.eval()\n",
    "# per_channel_quantized_model.fuse_model() # VGG dont need fuse\n",
    "per_channel_quantized_model.qconfig = torch.quantization.get_default_qconfig('fbgemm') # set the quantize config\n",
    "print('\\n optimal quantize config: ')\n",
    "print(per_channel_quantized_model.qconfig)\n",
    "\n",
    "torch.quantization.prepare(per_channel_quantized_model, inplace=True) # execute the quantize config\n",
    "evaluate(per_channel_quantized_model,criterion, data_loader, num_calibration_batches) # calibrate\n",
    "print(\"Calibrate done\")\n",
    "\n",
    "torch.quantization.convert(per_channel_quantized_model, inplace=True) # convert to quantize model\n",
    "print('Post Training Optimal Quantization: Convert done')\n",
    "\n",
    "print(\"Size of model after optimal quantization\")\n",
    "print_size_of_model(per_channel_quantized_model)\n",
    "\n",
    "top1, top5 = evaluate(per_channel_quantized_model, criterion, data_loader_test, neval_batches=num_eval_batches) # test acc\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batch_size, top1.avg))\n",
    "torch.jit.save(torch.jit.script(per_channel_quantized_model), saved_model_dir + scripted_optimal_quantized_model_file) # save quantized model\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "------------------------------\n",
    "    6. compare performance\n",
    "------------------------------\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\nInference time compare: \")\n",
    "run_benchmark(saved_model_dir + scripted_float_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_default_quantized_model_file, data_loader_test)\n",
    "run_benchmark(saved_model_dir + scripted_optimal_quantized_model_file, data_loader_test)\n",
    "\n",
    "\"\"\" you can compare the model's size/accuracy/inference time.\n",
    "    ----------------------------------------------------------------------------------------\n",
    "                    | origin model | default quantized model | optimal quantized model\n",
    "    model size:     |    553 MB    |         138 MB          |        138 MB\n",
    "    test accuracy:  |    79.33     |         76.67           |        78.67\n",
    "    inference time: |    317 ms    |         254 ms          |        257 ms\n",
    "    ---------------------------------------------------------------------------------------\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
